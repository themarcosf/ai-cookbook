{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build the model\n",
    "\n",
    "## 1.1. Setup device agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Construct the model\n",
    "\n",
    "In Elden Ring, Godrick is the first boss and shardbearer. A descendant of the Golden Lineage, the bloodline which began with Queen Marika the Eternal and her first consort Godfrey, he took up residence in Stormveil after the Ring was shattered, a place where he could practice his art of grafting; a grotesque act which involves attaching parts of other living beings to oneself in order to gain power.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../../assets/elden-ring-godrick.jpg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "One common and clean way to “graft” two networks together in PyTorch is to wrap them in a single `nn.Module` subclass. In other words, a new class is created that holds a pre-processing model, a core model and a post-processing model as submodules, and then a `forward()` method is defined that uses the output of each previous network as the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, encyphered_input):\n",
    "        decyphered_input = ...\n",
    "        return decyphered_input\n",
    "\n",
    "class Postprocessor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, decyphered_output):\n",
    "        encyphered_output = ...\n",
    "        return encyphered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "preprocessor.load_state_dict(torch.load('./models/preprocessor.pt'))\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "postprocessor = Postprocessor()\n",
    "postprocessor.load_state_dict(torch.load('./models/postprocessor.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GodrickModel(nn.Module):\n",
    "    def __init__(self, preprocessor, bert_model, postprocessor):\n",
    "        super().__init__()\n",
    "        self.preprocessor = preprocessor\n",
    "        self.bert = bert_model\n",
    "        self.postprocessor = postprocessor\n",
    "\n",
    "    def forward(self, raw_input):\n",
    "        # Step 1: Preprocess the raw input (cryptic tokens to BERT tokens)\n",
    "        preprocessed_input = self.preprocessor(raw_input)\n",
    "\n",
    "        # Step 2: Pass the preprocessed input into BERT.\n",
    "        # for HF models, `preprocessed_input` should be a dictionary:\n",
    "        # e.g., preprocessed_input = {\n",
    "        #    \"input_ids\": ...,\n",
    "        #    \"attention_mask\": ...,\n",
    "        #    \"token_type_ids\": ... (optional)\n",
    "        # }\n",
    "        bert_output = self.bert(**preprocessed_input)\n",
    "\n",
    "        # Step 3: Post-process BERT's output (BERT tokens to cryptic tokens)\n",
    "        postprocessed_output = self.postprocessor(bert_output)\n",
    "\n",
    "        return postprocessed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = GodrickModel().to(device)\n",
    "model0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Making test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([[0.1, 0.2], [0.2, 0.3], [0.3, 0.4]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `untrained_logits`: the output of the model before any additional handling\n",
    "with torch.inference_mode():\n",
    "  untrained_logits = model0(X_test.to(device))\n",
    "untrained_logits.shape, untrained_logits[:5].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `untrained_preds_probs`: the probability of the logits after applying the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `untrained_preds`: the predictions after applying the argmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "  f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Save and load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"models\").mkdir(exist_ok=True)\n",
    "torch.save(obj=model0.state_dict(), f='./models/godrick_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = GodrickModel()\n",
    "loaded_model.load_state_dict(torch.load('./models/godrick_model.pt'))\n",
    "\n",
    "model0.state_dict(), loaded_model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
