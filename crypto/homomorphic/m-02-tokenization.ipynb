{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "We will be using https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english for sentiment analysis.\n",
    "\n",
    "This model is a fine-tune checkpoint of DistilBERT-base-uncased, trained on SST-2 dataset which is a dataset for binary sentiment classification. It is composed of sentences extracted from movie reviews and annotated with a sentiment label. The task is to predict the sentiment of a given sentence. This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': Value(dtype='int32', id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "ds_builder = load_dataset_builder('stanfordnlp/sst2')\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_dataset = load_dataset('stanfordnlp/sst2')\n",
    "initial_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hide new secretions from the parental units '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = initial_dataset['train'][0]['sentence']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer_ckpt = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DistilBERT (original)\n",
    "\n",
    "DistilBERT, like BERT, uses a hidden size of 768 dimensions, meaning that the internal representations of words and tokens in the model have a dimensionality of 768.\n",
    "\n",
    "The hidden size in a transformer-based model like BERT or DistilBERT determines the dimension of the model's embeddings and the dimension of the hidden states as the model processes input data. A higher hidden size allows the model to capture more complex patterns and relationships in the data but also makes the model larger and more computationally intensive.\n",
    "\n",
    "For classification tasks, it is common practice to just use the hidden state corresponding to the first token of the input sequence, which is the special token [CLS] (for classification). This is because the hidden state of this token has access to the entire sequence through the attention mechanism.This means that during pre-training and fine-tuning, the model has learned to aggregate information from all tokens in the input sequence into this [CLS] token's hidden state.\n",
    "\n",
    "### **REWRITE USING PYTORCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = 'distilbert/distilbert-base-uncased'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5342,  2047,  3595,  8496,  2013,  1996, 18643,  3197,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hide',\n",
       " 'new',\n",
       " 'secret',\n",
       " '##ions',\n",
       " 'from',\n",
       " 'the',\n",
       " 'parental',\n",
       " 'units',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-1.6527e-01, -2.0267e-01, -3.8784e-01,  ..., -3.4770e-02,\n",
       "           1.7683e-01,  4.9799e-01],\n",
       "         [ 1.9158e-01,  2.9310e-03, -1.0192e-01,  ...,  9.4399e-02,\n",
       "          -4.0690e-02,  4.1901e-01],\n",
       "         [-1.3455e-01, -2.6318e-01,  1.8665e-01,  ..., -8.0812e-02,\n",
       "          -2.1052e-01, -6.9526e-02],\n",
       "         ...,\n",
       "         [ 2.3456e-01, -6.1193e-02, -1.0886e-02,  ..., -8.5200e-02,\n",
       "          -1.6539e-01,  1.3718e-01],\n",
       "         [-9.2496e-04, -2.2951e-01, -1.4710e-01,  ...,  5.5233e-02,\n",
       "          -9.3542e-02,  3.7744e-01],\n",
       "         [ 1.0679e+00,  2.4419e-01, -3.1690e-01,  ...,  1.2886e-01,\n",
       "          -5.1978e-01, -1.9382e-01]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode: tokens -> hash | input_ids\n",
    "# decode: ? -> tokens or tags | hahs -> tokens or tags\n",
    "with torch.inference_mode():\n",
    "  outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape: [batch_size, n_tokens, hidden_dims]\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['[CLS]',\n",
       "  'hide',\n",
       "  'new',\n",
       "  'secret',\n",
       "  '##ions',\n",
       "  'from',\n",
       "  'the',\n",
       "  'parental',\n",
       "  'units',\n",
       "  '[SEP]'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state corresponding to the first token of the input sequence [CLS]\n",
    "outputs.last_hidden_state[:, 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DistilBERT finetuned on SST-2 (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(tokenizer_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.0309,  0.4670, -0.7988,  ..., -0.0455, -0.4391,  0.0750],\n",
       "         [ 0.0161,  0.5940, -0.2869,  ...,  0.1686, -0.5022,  0.3976],\n",
       "         [-0.2654,  0.4771, -0.2804,  ..., -0.0954, -0.6223, -0.0153],\n",
       "         ...,\n",
       "         [ 0.1277,  0.6889, -0.4992,  ..., -0.3389, -0.3758,  0.3130],\n",
       "         [-0.0790,  0.4412, -0.4479,  ..., -0.2395, -0.2491,  0.2796],\n",
       "         [ 0.4794,  0.5617, -0.4332,  ..., -0.0540, -0.2366, -0.1133]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "  outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using Hugging Face SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text = 'I am positive about this.'\n",
    "\n",
    "pipe = pipeline('text-classification', model=tokenizer_ckpt, tokenizer=tokenizer)\n",
    "\n",
    "result = pipe(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(tokenizer_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "  outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('text-classification', model=model_ckpt, tokenizer=tokenizer)\n",
    "\n",
    "result = pipe(sentence)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selects the embedding for the first token in the sequence [CLS]\n",
    "# it is used to represent the entire sequence for classification tasks\n",
    "pooled_outputs = outputs.last_hidden_state[:, 0]\n",
    "pooled_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# model_ckpt = 'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_ckpt)\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "The first step is to tokenize the full SST-2 dataset. We will use the tokenizer provided by the model to tokenize the dataset.\n",
    "\n",
    "NOTES:\n",
    "- max_position_embeddings (int, optional, defaults to 512) — we will use the default value, thus we need to cap any inputs at this length.\n",
    "\n",
    "## 2.1. Tokenization of the SST-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(example):\n",
    "    return tokenizer(example['sentence'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_dataset = initial_dataset['train'].map(encode, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['sentence', 'input_ids'])\n",
    "dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create mapping dataset for first-layer network encryption\n",
    "\n",
    "The idea here is to first tokenize and then encrypt the tokens, so that we have a mapping of plain tokens to encrypted tokens. This mapping will be used to train the first-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Cesear cipher encryption just for testing purposes\n",
    "def encrypt_tokens(example, shift=3):\n",
    "    encrypted_input_ids = [(token_id + shift) % tokenizer.vocab_size for token_id in example['input_ids']]\n",
    "    example['encrypted_input_ids'] = encrypted_input_ids\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_dataset = tokenized_dataset.map(encrypt_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_dataset.set_format(type='torch', columns=['encrypted_input_ids', 'input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = encrypted_dataset.shuffle(seed=42).select(range(1000))\n",
    "val_dataset = encrypted_dataset.shuffle(seed=42).select(range(200)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "class TokenTranslator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TokenTranslator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.encoder(embedded)\n",
    "        outputs, (hidden, cell) = self.decoder(outputs, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs)\n",
    "        return predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TokenTranslator().to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in data_loader:\n",
    "        src = batch['encrypted_input_ids'].to(device)\n",
    "        trg = batch['input_ids'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "\n",
    "        output = output.view(-1, vocab_size)\n",
    "        trg = trg.view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            src = batch['encrypted_input_ids'].to(device)\n",
    "            trg = batch['input_ids'].to(device)\n",
    "\n",
    "            output = model(src)\n",
    "            output = output.view(-1, vocab_size)\n",
    "            trg = trg.view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
