{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Named Entity Recognition\n",
    "\n",
    "This notebook will use XLM-RoBERTa (or XLM-R for short) to perform multilingual named entity recognition (NER) on a subset of the Cross-Lingual Transfer Evaluation of Multilingual Encoders (XTREME) benchmark called WikiAnn or PAN-X dataset. This dataset consists of texts from Wikipedia articles in many languages. Each article is annotated with `LOC` (location), `PER` (person) and `ORG` (organization) tags in the IOB2 format. In this format, a `B-` prefix indicates the beginning of an entity, and consecutive tokens belonging to the same entity are given an `I-` prefix. An `O` tag indicates that the token does not belong to any entity.\n",
    "\n",
    "XLM-RoBERTa belongs to a class of multilingual transformers that use masked language modeling as a pretraining objective and are trained jointly in many languages, enabling *zero-shot cross-lingual transfer*.\n",
    "\n",
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTREME has 183 configurations\n"
     ]
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"XTREME has {len(xtreme_subsets)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg', 'PAN-X.bn', 'PAN-X.de']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "panx_subsets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an imbalanced multilingual dataset from the XTREME PANX subsets\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# return a DatasetDict if a key is not found\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.6, 0.2, 0.1, 0.1]\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # load monolingual corpus\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # shuffle and downsample each split according to fracs proportions\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split]\n",
    "            .shuffle()\n",
    "            .select(range(int(frac * ds[split].num_rows))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_training_examples</th>\n",
       "      <td>12000</td>\n",
       "      <td>4000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          de    fr    it    en\n",
       "num_training_examples  12000  4000  2000  2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "  { lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs },\n",
    "  index=[\"num_training_examples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['Die', 'Zahnplatte', 'am', 'Zwischenkieferbein', 'ist', 'bei', 'geschlossenem', 'Maul', 'sichtbar', '.']\n",
      "ner_tags: [0, 0, 0, 5, 0, 0, 0, 0, 0, 0]\n",
      "langs: ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n"
     ]
    }
   ],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "for key, value in element.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n",
      "ner_tags: Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\n",
      "langs: Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n"
     ]
    }
   ],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49c32c28c1485fac16912e191ac414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316a38eb6ce948ac9fc6dac444a82ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b8f949e69f4e9d9ce509d57fd6f726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>Die</td>\n",
       "      <td>Zahnplatte</td>\n",
       "      <td>am</td>\n",
       "      <td>Zwischenkieferbein</td>\n",
       "      <td>ist</td>\n",
       "      <td>bei</td>\n",
       "      <td>geschlossenem</td>\n",
       "      <td>Maul</td>\n",
       "      <td>sichtbar</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tags</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1   2                   3    4    5              6  \\\n",
       "tokens  Die  Zahnplatte  am  Zwischenkieferbein  ist  bei  geschlossenem   \n",
       "tags      O           O   O               B-LOC    O    O              O   \n",
       "\n",
       "           7         8  9  \n",
       "tokens  Maul  sichtbar  .  \n",
       "tags       O         O  O  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "panx_de = panx_ch[\"de\"].map(lambda batch: {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]})\n",
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], index=[\"tokens\", \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LOC</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>5879</td>\n",
       "      <td>5114</td>\n",
       "      <td>5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>2947</td>\n",
       "      <td>2580</td>\n",
       "      <td>2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>2921</td>\n",
       "      <td>2505</td>\n",
       "      <td>2855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LOC   ORG   PER\n",
       "train       5879  5114  5550\n",
       "validation  2947  2580  2765\n",
       "test        2921  2505  2855"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "split2freqs = defaultdict(Counter)\n",
    "for split, dataset in panx_de.items():\n",
    "    for row in dataset[\"ner_tags_str\"]:\n",
    "        for tag in row:\n",
    "            if tag.startswith(\"B\"):\n",
    "                tag_type = tag.split(\"-\")[1]\n",
    "                split2freqs[split][tag_type] += 1\n",
    "\n",
    "pd.DataFrame.from_dict(split2freqs, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multilingual Transformers\n",
    "\n",
    "XLM-R is a great choice for multilingual NLU tasks, because it was trained on 100 different languages. The RoBERTa part of the model's name refers to the fact that the pretraining approach is the same as for the monolingual RoBERTa models. It improves on several aspects of BERT, in particular by removing the next sentence prediction task and dropping the WordPiece tokenizer in favor of the SentencePiece tokenizer, which is trained on the raw text of all one hundred language and supports a much larger vocabulary (250,000 tokens versus 55,000 tokens).\n",
    "\n",
    "## 2.1. A closer look at tokenization\n",
    "\n",
    "A `tokenizer` is actually a full processing pipeline that usually consists of four steps:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/884293a3ab50071aa2afd1329ecf4a24f0793333//images/chapter04_tokenizer-pipeline.png\" width=\"600\" alt=\"The four steps of the tokenization pipeline\">\n",
    "\n",
    "1. `Normalization`: The text is normalized, for example by removing accents and converting the text to lowercase. This technique can reduce the size of the vocabulary by merging tokens that differ only by case or accent.\n",
    "2. `Pretokenization`: The text is split into words, punctuation marks and other tokens, such as numbers or, in the case of Chinese, characters. This step is language dependent and is usually performed by a rule-based tokenizer. The words are then split into subwords with Byte-Pair Encoding (BPE) or Unigram algorithms in the next step of the pipeline.\n",
    "3. `Tokenizer model`: A subword splitting model is applied on the words to reduce the size of the vocabulary and try to reduce the number of out-of-vocabulary tokens. This is the part of the pipeline that needs to be trained on a large corpus and at this point the strings are converted to integers (`input IDs`). Several subword tokenization algorithms exist, including BPE, Unigram, and WordPiece.\n",
    "4. `Postprocessing`: Additional transformations are applied to the list of tokens, such as adding special tokens or truncating the sequence. This sequence of integers is then fed to the model.\n",
    "\n",
    "## 2.2. The SentencePiece Tokenizer\n",
    "\n",
    "The SentencePiece tokenizer is a subword tokenizer that uses the unigram algorithm and encodes each input text as a sequence of Unicode characters, allowing it to be agnostic about language. It also assigns the Unicode symbol U+2581 (or ‚ñÅ) to whitespace characters, which allows the model to distinguish between words and subwords and convert back to raw text without any ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: ['[CLS]', 'Jack', 'Spa', '##rrow', 'loves', 'New', 'York', '!', '[SEP]']\n",
      "XLMR: ['<s>', '‚ñÅJack', '‚ñÅSpar', 'row', '‚ñÅlove', 's', '‚ñÅNew', '‚ñÅYork', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "text = \"Jack Sparrow loves New York!\"\n",
    "bert_tokens = bert_tokenizer(text).tokens()\n",
    "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
    "\n",
    "print(f\"BERT: {bert_tokens}\")\n",
    "print(f\"XLMR: {xlmr_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Jack Sparrow loves New York!</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transformers for Named Entity Recognition\n",
    "\n",
    "For text classification, an encoder-only model such as BERT uses the special `[CLS]` token to represent an entire sequence of text. This representation is then fed through a fully connected or dense layer to output the distribution of all the discrete label values.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/884293a3ab50071aa2afd1329ecf4a24f0793333//images/chapter04_clf-architecture.png\" width=\"600\" alt=\"Text classification with BERT\">\n",
    "\n",
    "For named entity recognition (NER), these models take a similar approach, except that the representation of each individual input token is fed into the same fully connected layer to output the entity of the token. For this reason, NER can be seen as a *token classification task*.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/884293a3ab50071aa2afd1329ecf4a24f0793333//images/chapter04_ner-architecture.png\" width=\"600\" alt=\"Named entity recognition with BERT\">\n",
    "\n",
    "# 4. The Anatomy of the ü§ó Transformers Model Class\n",
    "\n",
    "ü§ó Transformers is organized around dedicated classes for each architecture and task. The model classes associated with different tasks are named according to a `<ModelName>For<Task>` or `AutoModelFor<Task>` convention. In the case that there is no dedicated model class for a given task, existing models can be extended by using helper functions that add a task-specific head to the base model.\n",
    "\n",
    "## 4.1. Bodies and Heads\n",
    "\n",
    "The main concept that makes ü§ó Transformers so versatile is the split of the architecture into a *body* and *head*. When we switch from the pretraining task to the downstream task, we need to replace the last layer of the model with one that is suitable for the task. This last layer is called the model head and is *task-specific*. The rest of the model is called the body and is *task-agnostic*: it includes the token embeddings and transformer layers.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/884293a3ab50071aa2afd1329ecf4a24f0793333//images/chapter04_bert-body-head.png\" width=\"400\" alt=\"The body and head of a transformer model\">\n",
    "\n",
    "## 4.2. Creating a custom model **head** for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import XLMRobertaConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    XLM-RoBERTa model for token-level classification.\n",
    "\n",
    "    Inherits from `RobertaPreTrainedModel` and adds a token-level classifier on top of the RoBERTa model.\n",
    "\n",
    "    Args:\n",
    "        config (:class:`~transformers.XLMRobertaConfig`):\n",
    "            Model configuration class with all the parameters of the model. Initializing with a config file does not\n",
    "            load the weights associated with the model, only the configuration. Check out the\n",
    "            :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
    "\n",
    "    `config_class` ensures that the standard XLM-R settings are used when a new model is initialized. The default parameters can be overwritten in the configuration object.\n",
    "\n",
    "    `self.roberta` loads the XLM-R model body. The `add_pooling_layer` argument is set to `False` to ensure all hidden states are returned and not only the one associated with the [CLS] token.\n",
    "\n",
    "    `init_weights()` inherited from `RobertaPreTrainedModel` loads the weights associated with the XLM-R model body and randomly initializes the weights of the classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)           \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)                # add dropout layer \n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)     # add standard feed-forward layer\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # use model body to get encoder representations\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
    "        print(\"outputs[0].shape: \", outputs[0].shape)\n",
    "        print(\"outputs[0]: \", outputs[0])\n",
    "\n",
    "        # apply classifier to encoder representations\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        print(\"sequence_output: \", sequence_output.shape)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        print(\"logits: \", logits.shape)\n",
    "\n",
    "        # calculate losses\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # return model output object\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Loading a custom model\n",
    "\n",
    "The `AutoConfig` class contains the blueprint of a model's architecture. It is used to instantiate a model with the correct hyperparameters, such as the number of layers, the size of the hidden states, and the number of attention heads.\n",
    "\n",
    "When a model is loaded with `AutoModel.from_pretrained(model_ckpt)`, the configuration file associated with that model is downloaded automatically. However, if we want to modify the configuration, we can load the configuration file separately with the customized parameters and pass it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index2tag: {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC'}\n",
      "tag2index: {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n",
    "\n",
    "print(f\"index2tag: {index2tag}\")\n",
    "print(f\"tag2index: {tag2index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=len(tags.names), id2label=index2tag, label2id=tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokens</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>‚ñÅJack</td>\n",
       "      <td>‚ñÅSpar</td>\n",
       "      <td>row</td>\n",
       "      <td>‚ñÅlove</td>\n",
       "      <td>s</td>\n",
       "      <td>‚ñÅNew</td>\n",
       "      <td>‚ñÅYork</td>\n",
       "      <td>!</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input IDs</th>\n",
       "      <td>0</td>\n",
       "      <td>21763</td>\n",
       "      <td>37456</td>\n",
       "      <td>15555</td>\n",
       "      <td>5161</td>\n",
       "      <td>7</td>\n",
       "      <td>2356</td>\n",
       "      <td>5753</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4  5     6      7   8     9\n",
       "Tokens     <s>  ‚ñÅJack  ‚ñÅSpar    row  ‚ñÅlove  s  ‚ñÅNew  ‚ñÅYork   !  </s>\n",
       "Input IDs    0  21763  37456  15555   5161  7  2356   5753  38     2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare data for testing the model initialization\n",
    "input_ids = xlmr_tokenizer(\"Jack Sparrow loves New York!\", return_tensors=\"pt\")\n",
    "pd.DataFrame([xlmr_tokens, input_ids[\"input_ids\"].squeeze().numpy()], index=[\"Tokens\", \"Input IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs[0].shape:  torch.Size([1, 10, 768])\n",
      "outputs[0]:  tensor([[[ 0.1883,  0.1270,  0.0512,  ..., -0.1185,  0.1011, -0.0155],\n",
      "         [ 0.0004,  0.0413, -0.0279,  ...,  0.0284,  0.0168,  0.0749],\n",
      "         [ 0.0485,  0.0551,  0.0085,  ...,  0.0673, -0.0997,  0.1789],\n",
      "         ...,\n",
      "         [-0.0049,  0.0067,  0.0732,  ...,  0.2713, -0.0115,  0.2732],\n",
      "         [ 0.0185,  0.0094,  0.0372,  ..., -0.1094,  0.0249,  0.1612],\n",
      "         [ 0.1977,  0.1146, -0.0694,  ..., -0.3103, -0.0388,  0.0689]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "sequence_output:  torch.Size([1, 10, 768])\n",
      "logits:  torch.Size([1, 10, 7])\n",
      "Number of tokens in sequence: 10\n",
      "Shape of logits: torch.Size([1, 10, 7])\n",
      "Predictions: tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "Predicted tags: ['I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "# quick check that the model and tokenizer were initialized correctly\n",
    "outputs = xlmr_model(**input_ids.to(device)).logits\n",
    "predictions = torch.argmax(outputs, dim=-1)\n",
    "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
    "print(f\"Shape of logits: {outputs.shape}\")\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"Predicted tags: {[index2tag[prediction.item()] for prediction in predictions[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tokenizing Texts for NER\n",
    "\n",
    "# 7. Performance Measures\n",
    "\n",
    "# 8. Fine-Tuning XLM-RoBERTa\n",
    "\n",
    "# 9. Error Analysis\n",
    "\n",
    "# 10. Cross-Lingual Transfer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-face-hA3tPKVe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
