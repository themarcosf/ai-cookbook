{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2', '0.16.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup device agnostic code\n",
    "\n",
    "**Note**: Sometimes, depending on your data/hardware, you might find that your model trains faster on CPU than GPU. It could be that the overhead for copying data/model to and from the GPU outweighs the compute benefits offered by the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking\n",
    "\n",
    "It is possible to track experiments using Python dictionaries and comparing their metric print outs during training. But this is a very manual process. If there a dozen (or more) different models to be compared, **experiment tracking** becomes a necessity.\n",
    "\n",
    "Considering that *machine learning* and *deep learning* are very experimental, different models and hyperparameters need to be tried out. In order to track the results of various combinations of data, model architectures and training regimes, **experiment tracking helps to figure out what works and what doesn't**.\n",
    "\n",
    "## Different ways to track machine learning experiments \n",
    "\n",
    "There are as many different ways to track machine learning experiments as there is experiments to run. Due to its tight integration with PyTorch and widespread use, TensorBoard will be used to track experiments. It is a part of the TensorFlow deep learning library and an excellent way to visualize different parts of a model. However, the same principles are similar across all of the other tools for experiment tracking. The following table covers a few.\n",
    "\n",
    "| **Method** | **Setup** | **Pros** | **Cons** | **Cost** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| Python dictionaries, CSV files, print outs | None | Easy to setup, runs in pure Python | Hard to keep track of large numbers of experiments | Free |\n",
    "| [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) | Minimal, install [`tensorboard`](https://pypi.org/project/tensorboard/) | Extensions built into PyTorch, widely recognized and used, easily scales. | User-experience not as nice as other options. | Free |\n",
    "| [Weights & Biases Experiment Tracking](https://wandb.ai/site/experiment-tracking) | Minimal, install [`wandb`](https://docs.wandb.ai/quickstart), make an account | Incredible user experience, make experiments public, tracks almost anything. | Requires external resource outside of PyTorch. | Free for personal use | \n",
    "| [MLFlow](https://mlflow.org/) | Minimal, install `mlflow` and starting tracking | Fully open-source MLOps lifecycle management, many integrations. | Little bit harder to setup a remote tracking server than other services. | Free |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup boilerplate code for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "target_dir = pathlib.Path(\"data/food-101/pizza-steak-sushi\")\n",
    "\n",
    "# get a set of pre-trained weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "# get transforms used to create pre-trained model\n",
    "transforms = weights.transforms()\n",
    "train_data = ImageFolder(root=target_dir / \"train\", transform=transforms)\n",
    "test_data = ImageFolder(root=target_dir / \"test\", transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# setup batch size and number of workers\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# create data loaders\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# freeze all base layers in backbone\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# update the classifier head\n",
    "torch.manual_seed(42), torch.cuda.manual_seed(42)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(train_data.classes))\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Track experiments with TensorBoard\n",
    "\n",
    "## 2.1. Adjust `train()` function to track results with `SummaryWriter()`\n",
    "\n",
    "To track experiments, the `torch.utils.tensorboard.SummaryWriter()` class is used to save various parts of a model's training progress to file in TensorBoard format. By default, the `SummaryWriter()` class saves information about the model to a file set by the `log_dir` parameter, the default location being `runs/CURRENT_DATETIME_HOSTNAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(\n",
    "        model: torch.nn.Module, \n",
    "        loss_fn: torch.nn.Module, \n",
    "        optim: torch.optim.Optimizer, \n",
    "        accuracy: torchmetrics.Metric, \n",
    "        f1: torchmetrics.Metric, \n",
    "        train_dataloader: DataLoader, \n",
    "        test_dataloader: DataLoader, \n",
    "        epochs: int = 5,\n",
    "        model_name: str = \"baseline-model\"):\n",
    "    \"\"\"Performs training and evaluation of the model\"\"\"\n",
    "\n",
    "    total_train_time = 0\n",
    "\n",
    "    # create a writer with all default settings\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = timer()\n",
    "\n",
    "        # training\n",
    "        train_loss_per_batch = train_acc_per_batch = train_f1_per_batch = 0\n",
    "\n",
    "        model.train()\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            train_loss_per_batch += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "\n",
    "            # calculate accuracy and f1 score\n",
    "            train_acc_per_batch += accuracy(logits.softmax(dim=1), y).item()\n",
    "            train_f1_per_batch += f1(logits.softmax(dim=1), y).item()\n",
    "\n",
    "        train_loss_per_batch /= len(train_dataloader)\n",
    "        train_acc_per_batch /= len(train_dataloader)\n",
    "        train_f1_per_batch /= len(train_dataloader)\n",
    "\n",
    "        # testing\n",
    "        test_loss_per_batch = test_acc_per_batch = test_f1_per_batch = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in test_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                logits = model(X)\n",
    "                loss = loss_fn(logits, y)\n",
    "                test_loss_per_batch += loss.item()\n",
    "\n",
    "                # calculate accuracy and f1 score\n",
    "                test_acc_per_batch += accuracy(logits.softmax(dim=1), y).item()\n",
    "                test_f1_per_batch += f1(logits.softmax(dim=1), y).item()\n",
    "\n",
    "        test_loss_per_batch /= len(test_dataloader)\n",
    "        test_acc_per_batch /= len(test_dataloader)\n",
    "        test_f1_per_batch /= len(test_dataloader)\n",
    "\n",
    "        end = timer()\n",
    "        total_train_time += end - start\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}, \"\n",
    "                f\"train_loss: {train_loss_per_batch:.4f}, test_loss: {test_loss_per_batch:.4f}, \"\n",
    "                f\"train_acc: {train_acc_per_batch:.4f}, test_acc: {test_acc_per_batch:.4f}, \"\n",
    "                f\"train_f1: {train_f1_per_batch:.4f}, test_f1: {test_f1_per_batch:.4f}, \"\n",
    "                f\"time: {end - start:.2f}s\")\n",
    "        \n",
    "        ### Experiment tracking ###\n",
    "\n",
    "        # add loss results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Loss\", \n",
    "                           tag_scalar_dict={\"train_loss\": train_loss_per_batch,\n",
    "                                            \"test_loss\": test_loss_per_batch},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        # add accuracy results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                           tag_scalar_dict={\"train_acc\": train_acc_per_batch,\n",
    "                                            \"test_acc\": test_acc_per_batch}, \n",
    "                           global_step=epoch)\n",
    "        \n",
    "        # add f1 score results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"F1\", \n",
    "                           tag_scalar_dict={\"train_f1\": train_f1_per_batch,\n",
    "                                            \"test_f1\": test_f1_per_batch}, \n",
    "                           global_step=epoch)\n",
    "        \n",
    "        # track the PyTorch model architecture\n",
    "        writer.add_graph(model=model, \n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    # close the writer\n",
    "    writer.close()\n",
    "        \n",
    "    return {\n",
    "        \"train_loss\": train_loss_per_batch,\n",
    "        \"train_acc\": train_acc_per_batch,\n",
    "        \"train_f1\": train_f1_per_batch,\n",
    "        \"test_loss\": test_loss_per_batch,\n",
    "        \"test_acc\": test_acc_per_batch,\n",
    "        \"test_f1\": test_f1_per_batch,\n",
    "        \"total_train_time\": total_train_time,\n",
    "        \"model_name\": model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, train_loss: 1.0883, test_loss: 0.8914, train_acc: 0.4180, test_acc: 0.6818, train_f1: 0.4180, test_f1: 0.6818, time: 85.77s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:38<06:35, 98.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5, train_loss: 0.8937, test_loss: 0.8082, train_acc: 0.6641, test_acc: 0.7746, train_f1: 0.6641, test_f1: 0.7746, time: 84.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [03:16<04:54, 98.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5, train_loss: 0.7450, test_loss: 0.7433, train_acc: 0.8438, test_acc: 0.7538, train_f1: 0.8438, test_f1: 0.7538, time: 84.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [04:54<03:16, 98.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5, train_loss: 0.7797, test_loss: 0.6849, train_acc: 0.6992, test_acc: 0.8040, train_f1: 0.6992, test_f1: 0.8040, time: 84.67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [06:32<01:38, 98.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5, train_loss: 0.6322, test_loss: 0.6428, train_acc: 0.7695, test_acc: 0.8362, train_f1: 0.7695, test_f1: 0.8362, time: 84.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:10<00:00, 98.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(42), torch.cuda.manual_seed(42)\n",
    "\n",
    "# pick loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "# define eval metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=len(train_data.classes)).to(device)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=len(train_data.classes)).to(device)\n",
    "\n",
    "# train model\n",
    "model_metrics = train_model(model, loss_fn, optim, accuracy, f1, train_dataloader, test_dataloader, model_name=\"efficientnet-b0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.6321721002459526,\n",
       " 'train_acc': 0.76953125,\n",
       " 'train_f1': 0.76953125,\n",
       " 'test_loss': 0.6428378224372864,\n",
       " 'test_acc': 0.8361742496490479,\n",
       " 'test_f1': 0.8361742496490479,\n",
       " 'total_train_time': 424.366627458,\n",
       " 'model_name': 'efficientnet-b0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the model results\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. View experiments with TensorBoard\n",
    "\n",
    "TensorBoard can be viewed in two main ways:\n",
    "\n",
    "| Code environment | How to view TensorBoard | Resource |\n",
    "| ----- | ----- | ----- |\n",
    "| VS Code | Press `SHIFT + CMD + P` to open the Command Palette and search for the command \"Python: Launch TensorBoard\". | [VS Code Guide on TensorBoard and PyTorch](https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration) |\n",
    "| Jupyter and Colab Notebooks | Make sure [TensorBoard is installed](https://pypi.org/project/tensorboard/), load it with `%load_ext tensorboard` and then view your results with `%tensorboard --logdir DIR_WITH_LOGS`. | [`torch.utils.tensorboard`](https://pytorch.org/docs/stable/tensorboard.html) and [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started) |\n",
    "\n",
    "Experiments can also be uploaded to [tensorboard.dev](https://tensorboard.dev/) to share them publicly with others."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setting up a series of modelling experiments\n",
    "\n",
    "It's to step things up a notch.\n",
    "\n",
    "Previously we've been running various experiments and inspecting the results one by one.\n",
    "\n",
    "But what if we could run multiple experiments and then inspect the results all together?\n",
    "\n",
    "You in?\n",
    "\n",
    "C'mon, let's go."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 What kind of experiments should you run?\n",
    "\n",
    "That's the million dollar question in machine learning.\n",
    "\n",
    "Because there's really no limit to the experiments you can run.\n",
    "\n",
    "Such a freedom is why machine learning is so exciting and terrifying at the same time.\n",
    "\n",
    "This is where you'll have to put on your scientist coat and remember the machine learning practitioner's motto: *experiment, experiment, experiment!*\n",
    "\n",
    "Every hyperparameter stands as a starting point for a different experiment: \n",
    "* Change the number of **epochs**.\n",
    "* Change the number of **layers/hidden units**.\n",
    "* Change the amount of **data**.\n",
    "* Change the **learning rate**.\n",
    "* Try different kinds of **data augmentation**.\n",
    "* Choose a different **model architecture**. \n",
    "\n",
    "With practice and running many different experiments, you'll start to build an intuition of what *might* help your model.\n",
    "\n",
    "I say *might* on purpose because there's no guarantees.\n",
    "\n",
    "But generally, in light of [*The Bitter Lesson*](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (I've mentioned this twice now because it's an important essay in the world of AI), generally the bigger your model (more learnable parameters) and the more data you have (more opportunities to learn), the better the performance.\n",
    "\n",
    "However, when you're first approaching a machine learning problem: start small and if something works, scale it up.\n",
    "\n",
    "Your first batch of experiments should take no longer than a few seconds to a few minutes to run.\n",
    "\n",
    "The quicker you can experiment, the faster you can work out what *doesn't* work, in turn, the faster you can work out what *does* work.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7.2 What experiments are we going to run?\n",
    "\n",
    "Our goal is to improve the model powering FoodVision Mini without it getting too big.\n",
    "\n",
    "In essence, our ideal model achieves a high level of test set accuracy (90%+) but doesn't take too long to train/perform inference (make predictions).\n",
    "\n",
    "We've got plenty of options but how about we keep things simple?\n",
    "\n",
    "Let's try a combination of:\n",
    "1. A different amount of data (10% of Pizza, Steak, Sushi vs. 20%)\n",
    "2. A different model ([`torchvision.models.efficientnet_b0`](https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b0.html#torchvision.models.efficientnet_b0) vs. [`torchvision.models.efficientnet_b2`](https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b2.html#torchvision.models.efficientnet_b2))\n",
    "3. A different training time (5 epochs vs. 10 epochs)\n",
    "\n",
    "Breaking these down we get: \n",
    "\n",
    "| Experiment number | Training Dataset | Model (pretrained on ImageNet) | Number of epochs |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 5 |\n",
    "| 2 | Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 5 | \n",
    "| 3 | Pizza, Steak, Sushi 10% percent | EfficientNetB0 | 10 | \n",
    "| 4 | Pizza, Steak, Sushi 10% percent | EfficientNetB2 | 10 |\n",
    "| 5 | Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 5 |\n",
    "| 6 | Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 5 |\n",
    "| 7 | Pizza, Steak, Sushi 20% percent | EfficientNetB0 | 10 |\n",
    "| 8 | Pizza, Steak, Sushi 20% percent | EfficientNetB2 | 10 |\n",
    "\n",
    "Notice how we're slowly scaling things up. \n",
    "\n",
    "With each experiment we slowly increase the amount of data, the model size and the length of training.\n",
    "\n",
    "By the end, experiment 8 will be using double the data, double the model size and double the length of training compared to experiment 1.\n",
    "\n",
    "> **Note:** I want to be clear that there truly is no limit to amount of experiments you can run. What we've designed here is only a very small subset of options. However, you can't test *everything* so best to try a few things to begin with and then follow the ones which work the best.\n",
    ">\n",
    "> And as a reminder, the datasets we're using are a subset of the [Food101 dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101) (3 classes, pizza, steak, suhsi, instead of 101) and 10% and 20% of the images rather than 100%. If our experiments work, we could start to run more on more data (though this will take longer to compute). You can see how the datasets were created via the [`04_custom_data_creation.ipynb` notebook](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Download different datasets\n",
    "\n",
    "Before we start running our series of experiments, we need to make sure our datasets are ready.\n",
    "\n",
    "We'll need two forms of a training set:\n",
    "1. A training set with **10% of the data** of Food101 pizza, steak, sushi images (we've already created this above but we'll do it again for completeness).\n",
    "2. A training set with **20% of the data** of Food101 pizza, steak, sushi images.\n",
    "\n",
    "For consistency, all experiments will use the same testing dataset (the one from the 10% data split).\n",
    "\n",
    "We'll start by downloading the various datasets we need using the `download_data()` function we created earlier.\n",
    "\n",
    "Both datasets are available from the course GitHub:\n",
    "1. [Pizza, steak, sushi 10% training data](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip).\n",
    "2. [Pizza, steak, sushi 20% training data](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download 10 percent and 20 percent training data (if necessary)\n",
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data downloaded!\n",
    "\n",
    "Now let's setup the filepaths to data we'll be using for the different experiments.\n",
    "\n",
    "We'll create different training directory paths but we'll only need one testing directory path since all experiments will be using the same test dataset (the test dataset from pizza, steak, sushi 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Transform Datasets and create DataLoaders\n",
    "\n",
    "Next we'll create a series of transforms to prepare our images for our model(s).\n",
    "\n",
    "To keep things consistent, we'll manually create a transform (just like we did above) and use the same transform across all of the datasets.\n",
    "\n",
    "The transform will: \n",
    "1. Resize all the images (we'll start with 224, 224 but this could be changed).\n",
    "2. Turn them into tensors with values between 0 & 1. \n",
    "3. Normalize them in way so their distributions are inline with the ImageNet dataset (we do this because our models from [`torchvision.models`](https://pytorch.org/vision/stable/models.html) have been pretrained on ImageNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n",
    "\n",
    "# Compose transforms into a pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # 1. Resize the images\n",
    "    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 & 1\n",
    "    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform ready!\n",
    "\n",
    "Now let's create our DataLoaders using the `create_dataloaders()` function from `data_setup.py` we created in [05. PyTorch Going Modular section 2](https://www.learnpytorch.io/05_pytorch_going_modular/#2-create-datasets-and-dataloaders-data_setuppy). \n",
    "\n",
    "We'll create the DataLoaders with a batch size of 32.\n",
    "\n",
    "For all of our experiments we'll be using the same `test_dataloader` (to keep comparisons consistent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "    test_dir=test_dir, \n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create 20% training and test data DataLoders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "    test_dir=test_dir,\n",
    "    transform=simple_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Create feature extractor models\n",
    "\n",
    "Time to start building our models.\n",
    "\n",
    "We're going to create two feature extractor models: \n",
    "\n",
    "1. [`torchvision.models.efficientnet_b0()`](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html) pretrained backbone + custom classifier head (EffNetB0 for short).\n",
    "2. [`torchvision.models.efficientnet_b2()`](https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b2.html) pretrained backbone + custom classifier head (EffNetB2 for short).\n",
    "\n",
    "To do this, we'll freeze the base layers (the feature layers) and update the model's classifier heads (output layers) to suit our problem just like we did in [06. PyTorch Transfer Learning section 3.4](https://www.learnpytorch.io/06_pytorch_transfer_learning/#34-freezing-the-base-model-and-changing-the-output-layer-to-suit-our-needs).\n",
    "\n",
    "We saw in the previous chapter the `in_features` parameter to the classifier head of EffNetB0 is `1280` (the backbone turns the input image into a feature vector of size `1280`).\n",
    "\n",
    "Since EffNetB2 has a different number of layers and parameters, we'll need to adapt it accordingly.\n",
    "\n",
    "> **Note:** Whenever you use a different model, one of the first things you should inspect is the input and output shapes. That way you'll know how you'll have to prepare your input data/update the model to have the correct output shape.\n",
    "\n",
    "We can find the input and output shapes of EffNetB2 using [`torchinfo.summary()`](https://github.com/TylerYep/torchinfo) and passing in the `input_size=(32, 3, 224, 224)` parameter (`(32, 3, 224, 224)` is equivalent to `(batch_size, color_channels, height, width)`, i.e we pass in an example of what a single batch of data would be to our model).\n",
    "\n",
    "> **Note:** Many modern models can handle input images of varying sizes thanks to [`torch.nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html) layer, this layer adaptively adjusts the `output_size` of a given input as required. You can try this out by passing different size input images to `torchinfo.summary()` or to your own models using the layer.\n",
    "\n",
    "To find the required input shape to the final layer of EffNetB2, let's:\n",
    "1. Create an instance of `torchvision.models.efficientnet_b2(pretrained=True)`.\n",
    "2. See the various input and output shapes by running `torchinfo.summary()`.\n",
    "3. Print out the number of `in_features` by inspecting `state_dict()` of the classifier portion of EffNetB2 and printing the length of the weight matrix.\n",
    "    * **Note:** You could also just inspect the output of `effnetb2.classifier`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "# 1. Create an instance of EffNetB2 with pretrained weights\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
    "\n",
    "# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n",
    "# summary(model=effnetb2, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# ) \n",
    "\n",
    "# 3. Get the number of in_features of the EfficientNetB2 classifier layer\n",
    "print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-effnetb2-unfrozen-summary-output.png\" alt=\"output of torchinfo.summary() when passed our effnetb2 model with all layers trainable and default classifier head\" width=900/>\n",
    "\n",
    "*Model summary of EffNetB2 feature extractor model with all layers unfrozen (trainable) and default classifier head from ImageNet pretraining.*\n",
    "\n",
    "Now we know the required number of `in_features` for the EffNetB2 model, let's create a couple of helper functions to setup our EffNetB0 and EffNetB2 feature extractor models.\n",
    "\n",
    "We want these functions to:\n",
    "1. Get the base model from `torchvision.models`\n",
    "2. Freeze the base layers in the model (set `requires_grad=False`)\n",
    "3. Set the random seeds (we don't *need* to do this but since we're running a series of experiments and initalizing a new layer with random weights, we want the randomness to be similar for each experiment)\n",
    "4. Change the classifier head (to suit our problem)\n",
    "5. Give the model a name (e.g. \"effnetb0\" for EffNetB0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "# Get num out features (one for each class pizza, steak, sushi)\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "# Create an EffNetB0 feature extractor\n",
    "def create_effnetb0():\n",
    "    # 1. Get the base mdoel with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb0\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model\n",
    "\n",
    "# Create an EffNetB2 feature extractor\n",
    "def create_effnetb2():\n",
    "    # 1. Get the base model with pretrained weights and send to target device\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    # 2. Freeze the base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 3. Set the seeds\n",
    "    set_seeds()\n",
    "\n",
    "    # 4. Change the classifier head\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n",
    "    ).to(device)\n",
    "\n",
    "    # 5. Give the model a name\n",
    "    model.name = \"effnetb2\"\n",
    "    print(f\"[INFO] Created new {model.name} model.\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some nice looking functions!\n",
    "\n",
    "Let's test them out by creating an instance of EffNetB0 and EffNetB2 and checking out their `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb0 = create_effnetb0() \n",
    "\n",
    "# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n",
    "# summary(model=effnetb0, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-effnetb0-frozen-summary-output.png\" alt=\"output of torchinfo.summary() when passed our effnetb0 model with base layers are frozen and classifier head is updated\" width=900/>\n",
    "\n",
    "*Model summary of EffNetB0 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetb2 = create_effnetb2()\n",
    "\n",
    "# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n",
    "# summary(model=effnetb2, \n",
    "#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
    "#         # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"]\n",
    "# ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-effnetb2-frozen-summary-output.png\" alt=\"output of torchinfo.summary() when passed our effnetb2 model with base layers are frozen and classifier head is updated\" width=900/>\n",
    "\n",
    "*Model summary of EffNetB2 model with base layers frozen (untrainable) and updated classifier head (suited for pizza, steak, sushi image classification).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the outputs of the summaries, it seems the EffNetB2 backbone has nearly double the amount of parameters as EffNetB0.\n",
    "\n",
    "| Model | Total parameters (before freezing/changing head) | Total parameters (after freezing/changing head) | Total trainable parameters (after freezing/changing head) |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| EfficientNetB0 | 5,288,548 | 4,011,391 | 3,843 |  \n",
    "| EfficientNetB2 | 9,109,994 | 7,705,221 | 4,227 |\n",
    "\n",
    "This gives the backbone of the EffNetB2 model more opportunities to form a representation of our pizza, steak and sushi data.\n",
    "\n",
    "However, the trainable parameters for each model (the classifier heads) aren't very different.\n",
    "\n",
    "Will these extra parameters lead to better results?\n",
    "\n",
    "We'll have to wait and see... \n",
    "\n",
    "> **Note:** In the spirit of experimenting, you really could try almost any model from `torchvision.models` in a similar fashion to what we're doing here. I've only chosen EffNetB0 and EffNetB2 as examples. Perhaps you might want to throw something like `torchvision.models.convnext_tiny()` or `torchvision.models.convnext_small()` into the mix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Create experiments and set up training code\n",
    "\n",
    "We've prepared our data and prepared our models, the time has come to setup some experiments!\n",
    "\n",
    "We'll start by creating two lists and a dictionary:\n",
    "1. A list of the number of epochs we'd like to test (`[5, 10]`)\n",
    "2. A list of the models we'd like to test (`[\"effnetb0\", \"effnetb2\"]`)\n",
    "3. A dictionary of the different training DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create epochs list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "\n",
    "# 3. Create dataloaders dictionary for various dataloaders\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists and dictionary created!\n",
    "\n",
    "Now we can write code to iterate through each of the different options and try out each of the different combinations.\n",
    "\n",
    "We'll also save the model at the end of each experiment so later on we can load back in the best model and use it for making predictions.\n",
    "\n",
    "Specifically, let's go through the following steps: \n",
    "1. Set the random seeds (so our experiment results are reproducible, in practice, you might run the same experiment across ~3 different seeds and average the results).\n",
    "2. Keep track of different experiment numbers (this is mostly for pretty print outs).\n",
    "3. Loop through the `train_dataloaders` dictionary items for each of the different training DataLoaders.\n",
    "4. Loop through the list of epoch numbers.\n",
    "5. Loop through the list of different model names.\n",
    "6. Create information print outs for the current running experiment (so we know what's happening).\n",
    "7. Check which model is the target model and create a new EffNetB0 or EffNetB2 instance (we create a new model instance each experiment so all models start from the same standpoint).\n",
    "8. Create a new loss function (`torch.nn.CrossEntropyLoss()`) and optimizer (`torch.optim.Adam(params=model.parameters(), lr=0.001)`) for each new experiment.\n",
    "9. Train the model with the modified `train()` function passing the appropriate details to the `writer` parameter.\n",
    "10. Save the trained model with an appropriate file name to file with `save_model()` from [`utils.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/utils.py). \n",
    "\n",
    "We can also use the `%%time` magic to see how long all of our experiments take together in a single Jupyter/Google Colab cell.\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from going_modular.going_modular.utils import save_model\n",
    "\n",
    "# 1. Set the random seeds\n",
    "set_seeds(seed=42)\n",
    "\n",
    "# 2. Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each DataLoader\n",
    "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epochs in num_epochs: \n",
    "\n",
    "        # 5. Loop through each model name and create a new model based on the name\n",
    "        for model_name in models:\n",
    "\n",
    "            # 6. Create information print outs\n",
    "            experiment_number += 1\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")  \n",
    "\n",
    "            # 7. Select the model\n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            else:\n",
    "                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            \n",
    "            # 8. Create a new loss and optimizer for every model\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "            # 9. Train target model with target dataloaders and track experiments\n",
    "            train(model=model,\n",
    "                  train_dataloader=train_dataloader,\n",
    "                  test_dataloader=test_dataloader, \n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  writer=create_writer(experiment_name=dataloader_name,\n",
    "                                       model_name=model_name,\n",
    "                                       extra=f\"{epochs}_epochs\"))\n",
    "            \n",
    "            # 10. Save the model to file so we can get back the best model\n",
    "            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            print(\"-\"*50 + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View experiments in TensorBoard\n",
    "\n",
    "Ho, ho!\n",
    "\n",
    "Look at us go!\n",
    "\n",
    "Training eight models in one go?\n",
    "\n",
    "Now that's living up to the motto!\n",
    "\n",
    "*Experiment, experiment, experiment!*\n",
    "\n",
    "How about we check out the results in TensorBoard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above we should get an output similar to the following.\n",
    "\n",
    "> **Note:** Depending on the random seeds you used/hardware you used there's a chance your numbers aren't exactly the same as what's here. This is okay. It's due to the inheret randomness of deep learning. What matters most is the trend. Where your numbers are heading. If they're off by a large amount, perhaps there's something wrong and best to go back and check the code. But if they're off by a small amount (say a couple of decimal places or so), that's okay. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/07-tensorboard-lowest-test-loss.png\" alt=\"various modelling experiments visualized on tensorboard with model that has the lowest test loss highlighted\" width=900/>\n",
    "\n",
    "*Visualizing the test loss values for the different modelling experiments in TensorBoard, you can see that the EffNetB0 model trained for 10 epochs and with 20% of the data achieves the lowest loss. This sticks with the overall trend of the experiments that: more data, larger model and longer training time is generally better.*\n",
    "\n",
    "You can also upload your TensorBoard experiment results to [tensorboard.dev](https://tensorboard.dev) to host them publically for free.\n",
    "\n",
    "For example, running code similiar to the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload the results to TensorBoard.dev (uncomment to try it out)\n",
    "# !tensorboard dev upload --logdir runs \\\n",
    "#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n",
    "#     --description \"Comparing results of different model size, training data amount and training time.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell above results in the experiments from this notebook being publically viewable at: https://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/\n",
    "\n",
    "> **Note:** Beware that anything you upload to tensorboard.dev is publically available for anyone to see. So if you do upload your experiments, be careful they don't contain sensitive information. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load in the best model and make predictions with it\n",
    "\n",
    "Looking at the TensorBoard logs for our eight experiments, it seems experiment number eight achieved the best overall results (highest test accuracy, second lowest test loss).\n",
    "\n",
    "This is the experiment that used:\n",
    "* EffNetB2 (double the parameters of EffNetB0)\n",
    "* 20% pizza, steak, sushi training data (double the original training data)\n",
    "* 10 epochs (double the original training time)\n",
    "\n",
    "In essence, our biggest model achieved the best results.\n",
    "\n",
    "Though it wasn't as if these results were far better than the other models.\n",
    "\n",
    "The same model on the same data achieved similar results in half the training time (experiment number 6).\n",
    "\n",
    "This suggests that potentially the most influential parts of our experiments were the number of parameters and the amount of data.\n",
    "\n",
    "Inspecting the results further it seems that generally a model with more parameters (EffNetB2) and more data (20% pizza, steak, sushi training data) performs better (lower test loss and higher test accuracy).\n",
    "\n",
    "More experiments could be done to further test this but for now, let's import our best performing model from experiment eight (saved to: `models/07_effnetb2_data_20_percent_10_epochs.pth`, you can [download this model from the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/models/07_effnetb2_data_20_percent_10_epochs.pth)) and perform some qualitative evaluations.\n",
    "\n",
    "In other words, let's *visualize, visualize, visualize!*\n",
    "\n",
    "We can import the best saved model by creating a new instance of EffNetB2 using the `create_effnetb2()` function and then load in the saved `state_dict()` with `torch.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the best model filepath\n",
    "best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n",
    "\n",
    "# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\n",
    "best_model = create_effnetb2()\n",
    "\n",
    "# Load the saved best model state_dict()\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model loaded!\n",
    "\n",
    "While we're here, let's check its filesize.\n",
    "\n",
    "This is an important consideration later on when deploying the model (incorporating it in an app).\n",
    "\n",
    "If the model is too large, it can be hard to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model file size\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the model size in bytes then convert to megabytes\n",
    "effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\n",
    "print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our best model so far is 29 MB in size. We'll keep this in mind if we wanted to deploy it later on.\n",
    "\n",
    "Time to make and visualize some predictions.\n",
    "\n",
    "We created a `pred_and_plot_image()` function to use a trained model to make predictions on an image in [06. PyTorch Transfer Learning section 6](https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set).\n",
    "\n",
    "And we can reuse this function by importing it from [`going_modular.going_modular.predictions.py`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/predictions.py) (I put the `pred_and_plot_image()` function in a script so we could reuse it).\n",
    "\n",
    "So to make predictions on various images the model hasn't seen before, we'll first get a list of all the image filepaths from the 20% pizza, steak, sushi testing dataset and then we'll randomly select a subset of these filepaths to pass to our `pred_and_plot_image()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to make predictions on images and plot them \n",
    "# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Get a random list of 3 images from 20% test set\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\n",
    "test_image_path_sample = random.sample(population=test_image_path_list,\n",
    "                                       k=num_images_to_plot) # randomly select k number of images\n",
    "\n",
    "# Iterate through random test image paths, make predictions on them and plot them\n",
    "for image_path in test_image_path_sample:\n",
    "    pred_and_plot_image(model=best_model,\n",
    "                        image_path=image_path,\n",
    "                        class_names=class_names,\n",
    "                        image_size=(224, 224))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!\n",
    "\n",
    "Running the cell above a few times we can see our model performs quite well and often has higher prediction probabilities than previous models we've built.\n",
    "\n",
    "This suggests the model is more confident in the decisions it's making. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Predict on a custom image with the best model\n",
    "\n",
    "Making predictions on the test dataset is cool but the real magic of machine learning is making predictions on custom images of your own.\n",
    "\n",
    "So let's import the trusty [pizza dad image](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg) (a photo of my dad in front of a pizza) we've been using for the past couple of sections and see how our model performs on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download custom image\n",
    "import requests\n",
    "\n",
    "# Setup custom image path\n",
    "custom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n",
    "\n",
    "# Download the image if it doesn't already exist\n",
    "if not custom_image_path.is_file():\n",
    "    with open(custom_image_path, \"wb\") as f:\n",
    "        # When downloading from GitHub, need to use the \"raw\" file link\n",
    "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
    "        print(f\"Downloading {custom_image_path}...\")\n",
    "        f.write(request.content)\n",
    "else:\n",
    "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
    "\n",
    "# Predict on custom image\n",
    "pred_and_plot_image(model=model,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah!\n",
    "\n",
    "Two thumbs again!\n",
    "\n",
    "Our best model predicts \"pizza\" correctly and this time with an even higher prediction probability (0.978) than the first feature extraction model we trained and used in [06. PyTorch Transfer Learning section 6.1](https://www.learnpytorch.io/06_pytorch_transfer_learning/#61-making-predictions-on-a-custom-image).\n",
    "\n",
    "This again suggests our current best model (EffNetB2 feature extractor trained on 20% of the pizza, steak, sushi training data and for 10 epochs) has learned patterns to make it more confident of its decision to predict pizza.\n",
    "\n",
    "I wonder what could improve our model's performance even further? \n",
    "\n",
    "I'll leave that as a challenge for you to investigate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main takeaways\n",
    "\n",
    "We've now gone full circle on the PyTorch workflow introduced in [01. PyTorch Workflow Fundamentals](https://www.learnpytorch.io/01_pytorch_workflow/), we've gotten data ready, we've built and picked a pretrained model, we've used our various helper functions to train and evaluate the model and in this notebook we've improved our FoodVision Mini model by running and tracking a series of experiments.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png\" width=900 alt=\"a pytorch workflow flowchat\"/>\n",
    "\n",
    "You should be proud of yourself, this is no small feat!\n",
    "\n",
    "The main ideas you should take away from this Milestone Project 1 are:\n",
    "\n",
    "* The machine learning practioner's motto: *experiment, experiment, experiment!* (though we've been doing plenty of this already).\n",
    "* In the beginning, keep your experiments small so you can work fast, your first few experiments shouldn't take more than a few seconds to a few minutes to run.\n",
    "* The more experiments you do, the quicker you can figure out what *doesn't* work.\n",
    "* Scale up when you find something that works. For example, since we've found a pretty good performing model with EffNetB2 as a feature extractor, perhaps you'd now like to see what happens when you scale it up to the whole [Food101 dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) from `torchvision.datasets`.\n",
    "* Programmatically tracking your experiments takes a few steps to set up but it's worth it in the long run so you can figure out what works and what doesn't.\n",
    "    * There are many different machine learning experiment trackers out there so explore a few and try them out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "> **Note:** These exercises expect the use of `torchvision` v0.13+ (released July 2022), previous versions may work but will likely have errors.\n",
    "\n",
    "All of the exercises are focused on practicing the code above.\n",
    "\n",
    "You should be able to complete them by referencing each section or by following the resource(s) linked.\n",
    "\n",
    "All exercises should be completed using [device-agnostic code](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n",
    "\n",
    "**Resources:**\n",
    "* [Exercise template notebook for 07](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/07_pytorch_experiment_tracking_exercise_template.ipynb)\n",
    "* [Example solutions notebook for 07](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/07_pytorch_experiment_tracking_exercise_solutions.ipynb) (try the exercises *before* looking at this)\n",
    "    * See a live [video walkthrough of the solutions on YouTube](https://youtu.be/cO_r2FYcAjU) (errors and all)\n",
    "\n",
    "\n",
    "1. Pick a larger model from [`torchvision.models`](https://pytorch.org/vision/main/models.html) to add to the list of experiments (for example, EffNetB3 or higher). \n",
    "    * How does it perform compared to our existing models?\n",
    "2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n",
    "    * For example, you could have one training DataLoader that uses data augmentation (e.g. `train_dataloader_20_percent_aug` and `train_dataloader_20_percent_no_aug`) and then compare the results of two of the same model types training on these two DataLoaders.\n",
    "    * **Note:** You may need to alter the `create_dataloaders()` function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) for examples of using data augmentation or the script below for an example:\n",
    "\n",
    "```python\n",
    "# Note: Data augmentation transform like this should only be performed on training data\n",
    "train_transform_data_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Helper function to view images in a DataLoader (works with data augmentation transforms or not) \n",
    "def view_dataloader_images(dataloader, n=10):\n",
    "    if n > 10:\n",
    "        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n",
    "        n = 10\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n):\n",
    "        # Min max scale the image for display purposes\n",
    "        targ_image = imgs[i]\n",
    "        sample_min, sample_max = targ_image.min(), targ_image.max()\n",
    "        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n",
    "\n",
    "        # Plot images with appropriate axes information\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(False)\n",
    "\n",
    "# Have to update `create_dataloaders()` to handle different augmentations\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n",
    "\n",
    "# Note: this is an update version of data_setup.create_dataloaders to handle\n",
    "# differnt train and test transforms.\n",
    "def create_dataloaders(\n",
    "    train_dir, \n",
    "    test_dir, \n",
    "    train_transform, # add parameter for train transform (transforms on train dataset)\n",
    "    test_transform,  # add parameter for test transform (transforms on test dataset)\n",
    "    batch_size=32, num_workers=NUM_WORKERS\n",
    "):\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "```\n",
    "\n",
    "3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire [Food101 dataset from `torchvision.models`](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101)\n",
    "    * You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\n",
    "    * If you try more than one model, it would be good to have the model's results tracked.\n",
    "    * If you load the Food101 dataset from `torchvision.models`, you'll have to create PyTorch DataLoaders to use it in training.\n",
    "    * **Note:** Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra-curriculum\n",
    "\n",
    "* Read [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) blog post by Richard Sutton to get an idea of how many of the latest advancements in AI have come from increased scale (bigger datasets and bigger models) and more general (less meticulously crafted) methods.\n",
    "* Go through the [PyTorch YouTube/code tutorial](https://pytorch.org/tutorials/beginner/introyt/tensorboardyt_tutorial.html) for TensorBoard for 20-minutes and see how it compares to the code we've written in this notebook.\n",
    "* Perhaps you may want to view and rearrange your model's TensorBoard logs with a DataFrame (so you can sort the results by lowest loss or highest accuracy), there's a guide for this [in the TensorBoard documentation](https://www.tensorflow.org/tensorboard/dataframe_api). \n",
    "* If you like to use VSCode for development using scripts or notebooks (VSCode can now use Jupyter Notebooks natively), you can setup TensorBoard right within VSCode using the  [PyTorch Development in VSCode guide](https://code.visualstudio.com/docs/datascience/pytorch-support).\n",
    "* To go further with experiment tracking and see how your PyTorch model is performing from a speed perspective (are there any bottlenecks that could be improved to speed up training?), see the [PyTorch documentation for the PyTorch profiler](https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/).\n",
    "* Made With ML is an outstanding resource for all things machine learning by Goku Mohandas and their [guide on experiment tracking](https://madewithml.com/courses/mlops/experiment-tracking/) contains a fantastic introduction to tracking machine learning experiments with MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Official documentation - How to use Tensorboard with Pytorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "2. [Official documentation - Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "3. [VS Code documentation - TensorBoard integration](https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration)\n",
    "4. [A Gentle Introduction to Batch Normalization for Deep Neural Networks](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks)\n",
    "5. [Ground Truth Notebook](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#adjust-train-function-to-track-results-with-summarywriter)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
