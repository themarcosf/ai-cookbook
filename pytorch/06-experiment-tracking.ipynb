{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.2', '0.16.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup device agnostic code\n",
    "\n",
    "**Note**: Sometimes, depending on your data/hardware, you might find that your model trains faster on CPU than GPU. It could be that the overhead for copying data/model to and from the GPU outweighs the compute benefits offered by the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking\n",
    "\n",
    "It is possible to track experiments using Python dictionaries and comparing their metric print outs during training. But this is a very manual process. If there a dozen (or more) different models to be compared, **experiment tracking** becomes a necessity.\n",
    "\n",
    "Considering that *machine learning* and *deep learning* are very experimental, different models and hyperparameters need to be tried out. In order to track the results of various combinations of data, model architectures and training regimes, **experiment tracking helps to figure out what works and what doesn't**.\n",
    "\n",
    "## Different ways to track machine learning experiments \n",
    "\n",
    "There are as many different ways to track machine learning experiments as there are experiments to run. Due to its tight integration with PyTorch and widespread use, TensorBoard will be used to track experiments. It is a part of the TensorFlow deep learning library and an excellent way to visualize different parts of a model. However, the same principles are similar across all of the other tools for experiment tracking. The following table covers a few.\n",
    "\n",
    "| **Method** | **Setup** | **Pros** | **Cons** | **Cost** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| Python dictionaries, CSV files, print outs | None | Easy to setup, runs in pure Python | Hard to keep track of large numbers of experiments | Free |\n",
    "| [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) | Minimal, install [`tensorboard`](https://pypi.org/project/tensorboard/) | Extensions built into PyTorch, widely recognized and used, easily scales. | User-experience not as nice as other options. | Free |\n",
    "| [Weights & Biases Experiment Tracking](https://wandb.ai/site/experiment-tracking) | Minimal, install [`wandb`](https://docs.wandb.ai/quickstart), make an account | Incredible user experience, make experiments public, tracks almost anything. | Requires external resource outside of PyTorch. | Free for personal use | \n",
    "| [MLFlow](https://mlflow.org/) | Minimal, install `mlflow` and starting tracking | Fully open-source MLOps lifecycle management, many integrations. | Little bit harder to setup a remote tracking server than other services. | Free |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup boilerplate code for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "target_dir = pathlib.Path(\"data/food-101/pizza-steak-sushi\")\n",
    "\n",
    "# get a set of pre-trained weights\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "\n",
    "# get transforms used to create pre-trained model\n",
    "transforms = weights.transforms()\n",
    "train_data = ImageFolder(root=target_dir / \"train\", transform=transforms)\n",
    "test_data = ImageFolder(root=target_dir / \"test\", transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# setup batch size and number of workers\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# create data loaders\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=False,\n",
    "                              num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# freeze all base layers in backbone\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# update the classifier head\n",
    "torch.manual_seed(42), torch.cuda.manual_seed(42)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(train_data.classes))\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Track experiments with TensorBoard\n",
    "\n",
    "## 2.1. Adjust `train()` function to track results with `SummaryWriter()`\n",
    "\n",
    "To track experiments, the `torch.utils.tensorboard.SummaryWriter()` class is used to save various parts of a model's training progress to file in TensorBoard format. By default, the `SummaryWriter()` class saves information about the model to a file set by the `log_dir` parameter, the default location being `runs/CURRENT_DATETIME_HOSTNAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(\n",
    "        model: torch.nn.Module, \n",
    "        loss_fn: torch.nn.Module, \n",
    "        optim: torch.optim.Optimizer, \n",
    "        accuracy: torchmetrics.Metric, \n",
    "        f1: torchmetrics.Metric, \n",
    "        train_dataloader: DataLoader, \n",
    "        test_dataloader: DataLoader, \n",
    "        epochs: int = 5,\n",
    "        model_name: str = \"baseline-model\"):\n",
    "    \"\"\"Performs training and evaluation of the model\"\"\"\n",
    "\n",
    "    total_train_time = 0\n",
    "\n",
    "    # create a writer with all default settings\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        start = timer()\n",
    "\n",
    "        # training\n",
    "        train_loss_per_batch = train_acc_per_batch = train_f1_per_batch = 0\n",
    "\n",
    "        model.train()\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits, y)\n",
    "            train_loss_per_batch += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            optim.step()\n",
    "\n",
    "            # calculate accuracy and f1 score\n",
    "            train_acc_per_batch += accuracy(logits.softmax(dim=1), y).item()\n",
    "            train_f1_per_batch += f1(logits.softmax(dim=1), y).item()\n",
    "\n",
    "        train_loss_per_batch /= len(train_dataloader)\n",
    "        train_acc_per_batch /= len(train_dataloader)\n",
    "        train_f1_per_batch /= len(train_dataloader)\n",
    "\n",
    "        # testing\n",
    "        test_loss_per_batch = test_acc_per_batch = test_f1_per_batch = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in test_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                # forward pass\n",
    "                logits = model(X)\n",
    "                loss = loss_fn(logits, y)\n",
    "                test_loss_per_batch += loss.item()\n",
    "\n",
    "                # calculate accuracy and f1 score\n",
    "                test_acc_per_batch += accuracy(logits.softmax(dim=1), y).item()\n",
    "                test_f1_per_batch += f1(logits.softmax(dim=1), y).item()\n",
    "\n",
    "        test_loss_per_batch /= len(test_dataloader)\n",
    "        test_acc_per_batch /= len(test_dataloader)\n",
    "        test_f1_per_batch /= len(test_dataloader)\n",
    "\n",
    "        end = timer()\n",
    "        total_train_time += end - start\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}, \"\n",
    "              f\"train_loss: {train_loss_per_batch:.4f}, test_loss: {test_loss_per_batch:.4f}, \"\n",
    "              f\"train_acc: {train_acc_per_batch:.4f}, test_acc: {test_acc_per_batch:.4f}, \"\n",
    "              f\"train_f1: {train_f1_per_batch:.4f}, test_f1: {test_f1_per_batch:.4f}, \"\n",
    "              f\"time: {end - start:.2f}s\")\n",
    "        \n",
    "        ### Experiment tracking ###\n",
    "\n",
    "        # add loss results to SummaryWriter\n",
    "        loss_dict = {\"train_loss\": train_loss_per_batch, \"test_loss\": test_loss_per_batch}\n",
    "        writer.add_scalars(main_tag=\"Loss\", tag_scalar_dict=loss_dict, global_step=epoch)\n",
    "\n",
    "        # add accuracy results to SummaryWriter\n",
    "        acc_dict = {\"train_acc\": train_acc_per_batch, \"test_acc\": test_acc_per_batch}\n",
    "        writer.add_scalars(main_tag=\"Accuracy\", tag_scalar_dict=acc_dict, global_step=epoch)\n",
    "        \n",
    "        # add f1 score results to SummaryWriter\n",
    "        f1_dict = {\"train_f1\": train_f1_per_batch, \"test_f1\": test_f1_per_batch}\n",
    "        writer.add_scalars(main_tag=\"F1\", tag_scalar_dict=f1_dict, global_step=epoch)\n",
    "        \n",
    "        # track the PyTorch model architecture\n",
    "        writer.add_graph(model=model, input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    # close the writer\n",
    "    writer.close()\n",
    "        \n",
    "    return {\n",
    "        \"train_loss\": train_loss_per_batch,\n",
    "        \"train_acc\": train_acc_per_batch,\n",
    "        \"train_f1\": train_f1_per_batch,\n",
    "        \"test_loss\": test_loss_per_batch,\n",
    "        \"test_acc\": test_acc_per_batch,\n",
    "        \"test_f1\": test_f1_per_batch,\n",
    "        \"total_train_time\": total_train_time,\n",
    "        \"model_name\": model_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, train_loss: 1.0883, test_loss: 0.8914, train_acc: 0.4180, test_acc: 0.6818, train_f1: 0.4180, test_f1: 0.6818, time: 84.41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:37<06:29, 97.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5, train_loss: 0.8937, test_loss: 0.8082, train_acc: 0.6641, test_acc: 0.7746, train_f1: 0.6641, test_f1: 0.7746, time: 85.57s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [03:15<04:54, 98.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/5, train_loss: 0.7450, test_loss: 0.7433, train_acc: 0.8438, test_acc: 0.7538, train_f1: 0.8438, test_f1: 0.7538, time: 84.56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [04:53<03:15, 97.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/5, train_loss: 0.7797, test_loss: 0.6849, train_acc: 0.6992, test_acc: 0.8040, train_f1: 0.6992, test_f1: 0.8040, time: 85.69s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [06:32<01:38, 98.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/5, train_loss: 0.6322, test_loss: 0.6428, train_acc: 0.7695, test_acc: 0.8362, train_f1: 0.7695, test_f1: 0.8362, time: 84.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:09<00:00, 97.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy, F1Score\n",
    "\n",
    "# set seed for reproducibility\n",
    "torch.manual_seed(42), torch.cuda.manual_seed(42)\n",
    "\n",
    "# pick loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "# define eval metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=len(train_data.classes)).to(device)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=len(train_data.classes)).to(device)\n",
    "\n",
    "# train model\n",
    "model_metrics = train_model(model, loss_fn, optim, accuracy, f1, train_dataloader, test_dataloader, model_name=\"efficientnet-b0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': 0.6321721002459526,\n",
       " 'train_acc': 0.76953125,\n",
       " 'train_f1': 0.76953125,\n",
       " 'test_loss': 0.6428378224372864,\n",
       " 'test_acc': 0.8361742496490479,\n",
       " 'test_f1': 0.8361742496490479,\n",
       " 'total_train_time': 425.051124458,\n",
       " 'model_name': 'efficientnet-b0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the model results\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. View experiments with TensorBoard\n",
    "\n",
    "TensorBoard can be viewed in two main ways:\n",
    "\n",
    "| Code environment | How to view TensorBoard | Resource |\n",
    "| ----- | ----- | ----- |\n",
    "| VS Code | Press `SHIFT + CMD + P` to open the Command Palette and search for the command \"Python: Launch TensorBoard\". | [VS Code Guide on TensorBoard and PyTorch](https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration) |\n",
    "| Jupyter and Colab Notebooks | Make sure [TensorBoard is installed](https://pypi.org/project/tensorboard/), load it with `%load_ext tensorboard` and then view your results with `%tensorboard --logdir DIR_WITH_LOGS`. | [`torch.utils.tensorboard`](https://pytorch.org/docs/stable/tensorboard.html) and [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started) |\n",
    "\n",
    "Experiments can also be uploaded to [tensorboard.dev](https://tensorboard.dev/) to share them publicly with others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [The Bitter Lesson - Richard Sutton](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)\n",
    "2. [Official documentation - How to use Tensorboard with Pytorch](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n",
    "3. [Official documentation - Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "4. [VS Code documentation - TensorBoard integration](https://code.visualstudio.com/docs/datascience/pytorch-support#_tensorboard-integration)\n",
    "5. [A Gentle Introduction to Batch Normalization for Deep Neural Networks](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks)\n",
    "6. [Ground Truth Notebook](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#adjust-train-function-to-track-results-with-summarywriter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
