{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import transformer_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspection of the model (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained('gpt2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of heads per layer: 12\n",
      "Maximum context window: 1024\n"
     ]
    }
   ],
   "source": [
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "n_ctx = model.cfg.n_ctx\n",
    "\n",
    "print(f'Number of layers: {n_layers}')\n",
    "print(f'Number of heads per layer: {n_heads}')\n",
    "print(f'Maximum context window: {n_ctx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of model logits:  torch.Size([1, 6, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits = model('Hello TransformerLens!', return_type='logits')\n",
    "print('Shape of model logits: ', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `embeddings` matrix: 50257 x 768\n"
     ]
    }
   ],
   "source": [
    "d_embeddings, d_model = model.W_E.shape\n",
    "print(f'Shape of `embeddings` matrix: {d_embeddings} x {d_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `positional embeddings` matrix: 1024 x 768\n"
     ]
    }
   ],
   "source": [
    "d_embeddings, d_model = model.W_pos.shape\n",
    "print(f'Shape of `positional embeddings` matrix: {d_embeddings} x {d_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `query` matrix: 768 x 64\n"
     ]
    }
   ],
   "source": [
    "_, _, d_model, d_head = model.W_Q.shape\n",
    "print(f'Shape of `query` matrix: {d_model} x {d_head}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `key` matrix: 768 x 64\n"
     ]
    }
   ],
   "source": [
    "_, _, d_model, d_head = model.W_K.shape\n",
    "print(f'Shape of `key` matrix: {d_model} x {d_head}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `value` matrix: 768 x 64\n"
     ]
    }
   ],
   "source": [
    "_, _, d_model, d_head = model.W_V.shape\n",
    "print(f'Shape of `value` matrix: {d_model} x {d_head}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `unembeddings` matrix: 768 x 50257\n"
     ]
    }
   ],
   "source": [
    "d_embeddings, d_model = model.W_U.shape\n",
    "print(f'Shape of `unembeddings` matrix: {d_embeddings} x {d_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  tensor(7.2929, device='mps:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = model('Hello TransformerLens!', return_type='loss')\n",
    "print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "\n",
    "GPT-2 uses `<|endoftext|>` as Beginning of Sequence (BOS), End of Sequence (EOS) and Padding (PAD) tokens - index 50256.\n",
    "\n",
    "**TransformerLens** appends this token by default, inclusive in `model.forward`, which is what is implicitly used when `model(\"Hello World\")` is run. To disable this behavior, set the flag prepend_bos=False in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>', 'Hello', ' Trans', 'former', 'Lens', '!']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens('Hello TransformerLens!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496,  3602, 16354, 49479,     0]], device='mps:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens('Hello TransformerLens!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>Hello TransformerLens!']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(model.to_tokens('Hello TransformerLens!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of model logits:  torch.Size([1, 112, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits = model(text, return_type='logits')\n",
    "print('Shape of model logits: ', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictions:  torch.Size([111])\n"
     ]
    }
   ],
   "source": [
    "predictions = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "print('Shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 33/111\n"
     ]
    }
   ],
   "source": [
    "true_tokens = model.to_tokens(text).squeeze()[1:]\n",
    "is_correct = predictions == true_tokens\n",
    "\n",
    "print(f\"Model accuracy: {is_correct.sum()}/{len(true_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Induction heads** are a special kind of attention head that allow a model to perform in-context learning of a specific form: generalising from one observation that token B follows token A, to predict that token B will follow A in future occurrences of A, even if these two tokens had never appeared together in the training data.\n",
    "\n",
    "The evidence below for induction heads comes from the fact that the model successfully predicted 'ooked', 'Trans', 'former' following the token 'H'. This is because it is the second time that HookedTransformer had appeared in this text string, and the model predicted it the second time but not the first. The model did predict `former` the first time, but we can reasonably assume that `Transformer` is a word this model had already been exposed to during training, so this prediction would not require the induction capability, unlike `HookedTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence of induction heads: ['ooked', 'Trans', 'former']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidence of induction heads: {model.to_str_tokens(predictions[is_correct])[8:11]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Caching activations\n",
    "\n",
    "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model.\n",
    "\n",
    "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1, so passing in the `remove_batch_dim=True` keyword or calling `model.remove_batch_dim()` removes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'transformer_lens.ActivationCache.ActivationCache'>\n"
     ]
    }
   ],
   "source": [
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "print(type(logits), type(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing attention patterns for layer 0 (two different ways)\n",
    "# \n",
    "# the reason these are the same is that, under the hood, the first example actually\n",
    "#  indexes by `utils.get_act_name(\"pattern\", 0)`, which evaluates to \"blocks.0.attn.hook_pattern\"\n",
    "# \n",
    "# the diagram from the Transformer Architecture section helps in finding activation names\n",
    "# https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/full-merm.svg\n",
    "attn_patterns_from_shorthand = cache[\"pattern\", 0]\n",
    "attn_patterns_from_full_name = cache[\"blocks.0.attn.hook_pattern\"]\n",
    "\n",
    "torch.testing.assert_close(attn_patterns_from_shorthand, attn_patterns_from_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Intro do Mech Interp, by ARENA](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
