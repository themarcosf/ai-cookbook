{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import plotly_utils\n",
    "import torch\n",
    "from eindex import eindex\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  8 14:01:22 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA T4G                     On  |   00000000:00:1F.0 Off |                    0 |\n",
      "| N/A   42C    P8             15W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Oct_30_00:08:18_PDT_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.85\n",
      "Build cuda_12.6.r12.6/compiler.35059454_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 0\n",
      "CUDA device name: N/A\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"CUDA device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hook Points and Hook Functions\n",
    "\n",
    "One of the great things about interpreting neural networks is that we have full control over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean). And we can make precise, surgical edits and see how the model's behavior and other internals change. This is an extremely powerful tool, because it can let us set up careful counterfactuals and causal intervention to easily understand model behavior. Accordingly, being able to do this is a core operation in mechanistic interpretability.\n",
    "\n",
    "`Hook points` are used to allow observability and editing of every activation inside the transformer. They allow us to add `hook functions` to any activation and then run the model with those hook functions by calling `model.run_with_hooks`. They also have methods like `hook.layer()` and attributes like `hook.name` that are sometimes useful to call within the functions. Performing observability of activation patterns is useful for things like:\n",
    "  - extracting activations for a specific task\n",
    "  - doing long-running calculations across many inputs, e.g. finding the text that most activates a specific neuron\n",
    "\n",
    "`Hook functions` are used to observe or edit the activations. They take as arguments an `activation_value`, a tensor representing some activation pattern in the model, and a `hook_point`. If a `hook function` is being used to edit activations, then it should return a tensor of the same shape as the activation pattern. But if it is being used for observability, it should not return anything.\n",
    "\n",
    "### 1.1. Finding induction heads in the toy model from `induction_heads.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir='causal',\n",
    "    attn_only=True,\n",
    "    tokenizer_name='EleutherAI/gpt-neox-20b',\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None,\n",
    "    positional_embedding_type='shortformer',\n",
    ")\n",
    "\n",
    "REPO_ID = 'callummcdougall/attn_only_2L_half'\n",
    "FILENAME = 'attn_only_2L_half.pth'\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = torch.load(weights_path, map_location=device, weights_only=True)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "batch_size = 10\n",
    "\n",
    "prefix = (torch.ones(batch_size, 1) * model.tokenizer.bos_token_id).long()\n",
    "tokens = torch.randint(0, model.cfg.d_vocab, (batch_size, seq_len), dtype=torch.int64)\n",
    "repeated_tokens = torch.cat([prefix, tokens, tokens], dim=-1).to(device)\n",
    "\n",
    "print('Shape of repeated tokens:', repeated_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to store induction score for each head\n",
    "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "print('Shape of induction score store:', induction_score_store.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def induction_score_hook(activations: Tensor, hook: HookPoint):\n",
    "    \"\"\"\n",
    "    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.\n",
    "\n",
    "    Args:\n",
    "        activations (Tensor): The activation pattern tensor with dimensions (batch, head_index, ?, ?).\n",
    "        hook (HookPoint): The hook point that triggered this function, eg. `blocks.0.attn.hook_pattern`. It gives access\n",
    "                          to metadata like the layer number and allows capturing values during forward passes.\n",
    "    \"\"\"\n",
    "\n",
    "    # extracts a stripe of activation values on a diagonal `1 - seq_len` positions below the main diagonal\n",
    "    # corresponding to whether a token is attending to its previous occurrence\n",
    "    induction_stripe = activations.diagonal(dim1=-2, dim2=-1, offset=1 - seq_len)\n",
    "    # get an average score per head\n",
    "    induction_score = einops.reduce(induction_stripe, 'batch head_index position -> head_index', 'mean')\n",
    "    # store the result\n",
    "    induction_score_store[hook.layer(), :] = induction_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean filter on activation names -- returns true for attention patterns\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "model.run_with_hooks(\n",
    "    repeated_tokens,\n",
    "    return_type=None,  # for efficiency, logits are not calculated here\n",
    "    fwd_hooks=[(pattern_hook_names_filter, induction_score_hook)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_utils.imshow(\n",
    "    induction_score_store,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Induction Score by Head\",\n",
    "    text_auto=\".2f\",\n",
    "    width=900,\n",
    "    height=350,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Finding induction heads in GPT2-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained('gpt2-small').to(device)\n",
    "induction_score_store = torch.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n",
    "gpt2_small.run_with_hooks(repeated_tokens, return_type=None, fwd_hooks=[(pattern_hook_names_filter, induction_score_hook)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_utils.imshow(\n",
    "    induction_score_store,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Induction Score by Head\",\n",
    "    text_auto=\".1f\",\n",
    "    width=700,\n",
    "    height=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations_hook(activations: Tensor, hook: HookPoint):\n",
    "    \"\"\" Creates a visualization of the attention patterns for the current layer and head. \"\"\"\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(cv.attention.attention_patterns(tokens=model.to_str_tokens(repeated_tokens[0]), attention=activations.mean(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_head_layers = [5, 6, 7]\n",
    "\n",
    "fwd_hooks = [(utils.get_act_name(\"pattern\", layer), visualize_activations_hook) for layer in induction_head_layers]\n",
    "\n",
    "gpt2_small.run_with_hooks(\n",
    "    repeated_tokens,\n",
    "    return_type=None,\n",
    "    fwd_hooks=fwd_hooks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Direct logit attribution\n",
    "\n",
    "In the end of the day, interpretability boils down to the following question:\n",
    "\n",
    "> How much of the model's performance on some particular task is attributable to each component of the model?\n",
    "\n",
    "In order to develop such an understanding for how transformers perform certain tasks, we might look at how a head interacts with other heads in different layers, perform causal intervention by seeing how well the model performs when we remove a head, among other things. As a consequence of the residual stream, the output logits are the sum of the contributions of each layer, and thus the sum of the results of each head. This means that the output logits can be decomposed into direct contributions coming from each head.\n",
    "\n",
    "Suppose a model knows that the token Harry is followed by the token Potter. The logits on Harry are given by `residual @ W_U`, where `W_U` stands for the unembedding matrix. By its turn, the residual stream is the sum of all previous layers: `residual = embed + attn_out_0 + attn_out_1`. So `logits = (embed @ W_U) + (attn_out @ W_U) + (attn_out_1 @ W_U)`. More specifically, the logit of the Potter token corresponds to a column of `W_U`, i.e. a single number that is the sum of `(embed @ potter_U) + (attn_out_0 @ potter_U) + (attn_out_1 @ potter_U)`. This means that each attention layer output can be decomposed into the sum of the result of each head.\n",
    "\n",
    "In the example below the toy model is used. It is composed by paths from the output of each component directly to the logits:\n",
    "  - direct path: the residual connections from the embedding to unembedding\n",
    "  - layer 0 head: via the residual connection and skipping layer 1\n",
    "  - layer 1 head: via the residual connection directly to the logits\n",
    "\n",
    "Direct logit attribution looks at the direct effect that a component embeds into the residual stream on the logits. Subtle effects can be missed by looking at just the logits corresponding to the correct token, like a head suppressing other plausible logits to increase the log prob of the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.'\n",
    "\n",
    "tokens = model.to_tokens(text)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "print('Shape of tokens:', tokens.shape)\n",
    "print('Shape of logits:', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_attribution(embed: Tensor, l1_results: Tensor, l2_results: Tensor, W_U: Tensor, tokens: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        embed: token + position embeddings of the tokens [seq_len, d_model]\n",
    "        l1_results: output of attention heads at layer 1 [seq_len, n_heads, d_model]\n",
    "        l2_results: output of attention heads at layer 2 [seq_len, n_heads, d_model]\n",
    "        W_U: unembedding matrix [d_model, d_vocab]\n",
    "        tokens: token ids of the input sequence [seq_len]\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "\n",
    "    The final element of the output logits is ignored, as there is no next token to predict after the last token.\n",
    "    \"\"\"\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "\n",
    "    direct_attributions = einops.einsum(W_U_correct_tokens, embed[:-1], \"emb seq, seq emb -> seq\")\n",
    "    l1_attributions = einops.einsum(W_U_correct_tokens, l1_results[:-1], \"emb seq, seq nhead emb -> seq nhead\")\n",
    "    l2_attributions = einops.einsum(W_U_correct_tokens, l2_results[:-1], \"emb seq, seq nhead emb -> seq nhead\")\n",
    "    return torch.concat([direct_attributions.unsqueeze(-1), l1_attributions, l2_attributions], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if logit attribution is correct by taking the sum of logit attributions\n",
    "# and comparing it to the actual values in the residual stream of the model\n",
    "embed = cache[\"embed\"]\n",
    "l1_results = cache[\"result\", 0]\n",
    "l2_results = cache[\"result\", 1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "    correct_token_logits = logits[0, torch.arange(len(tokens[0]) - 1), tokens[0, 1:]]  # kth entry is the predicted logit for the correct k+1th token\n",
    "    torch.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Interpreting logit attribution\n",
    "\n",
    "Most variation in the logit attribution plot comes from the direct path, in particular for tokens that are the first token in common bigrams. For instance, the highest contribution on the direct path comes from | manip|, because this is very likely to be followed by |ulative| (or presumably a different stem like | ulation|). | super| -> |human| is another example of a bigram formed when the tokenizer splits one word into multiple tokens.\n",
    "\n",
    "There are also examples that come from two different words, rather than a single word split by the tokenizer. These include:\n",
    "\n",
    "| more| -> | likely| (12)\n",
    "| machine| -> | learning| (24)\n",
    "| by| -> | default| (38)\n",
    "| how| -> | to| (58)\n",
    "\n",
    "In addition, the heads in layer 1 have higher contributions than the heads in layer 0. This is because this plot doesn't pick up on a head's effect in composition with another head. So the attribution for layer-0 heads won't involve any composition, whereas the attributions for layer-1 heads will involve not only the single-head paths through those attention heads, but also the 2-layer compositional paths through heads in layer 0 and layer 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_utils.plot_logit_attribution(model, logit_attr, tokens, title=\"Logit attribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Interpreting logit attribution for induction heads\n",
    "\n",
    "The first half of the plot below is mostly meaningless, because the sequences are random and carry no predictable pattern, and so there can't be any part of the model that is doing meaningful computation to make predictions.\n",
    "\n",
    "In the second half, heads `1.4` and `1.10` have a large logit attribution score. This makes sense given the previous observation that these heads seemed to be performing induction (since they both exhibited the characteristic induction pattern), however it's worth emphasizing that this plot gives additional evidence of induction because just observing some head is attending to a particular token doesn't mean it's necessarily using that information to make a concrete prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, rep_cache = model.run_with_cache(repeated_tokens[0], remove_batch_dim=True)\n",
    "\n",
    "print('Shape of tokens:', repeated_tokens[0].shape)\n",
    "print('Shape of logits:', logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    embed = rep_cache[\"embed\"]\n",
    "    l1_results = rep_cache[\"result\", 0]\n",
    "    l2_results = rep_cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, repeated_tokens[0])\n",
    "    correct_token_logits = logits[0, torch.arange(len(repeated_tokens[0]) - 1), repeated_tokens[0, 1:]]\n",
    "    torch.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_utils.plot_logit_attribution(model, logit_attr, repeated_tokens[0], title=\"Logit attribution (random induction prompt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Intervening on activations using hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def get_log_probs(\n",
    "    logits: Tensor, tokens: Tensor\n",
    ") -> Tensor:\n",
    "    logprobs = logits.log_softmax(dim=-1)\n",
    "    # We want to get logprobs[b, s, tokens[b, s+1]], in eindex syntax this looks like:\n",
    "    correct_logprobs = eindex(logprobs, tokens, \"b s [b s+1]\")\n",
    "    return correct_logprobs\n",
    "\n",
    "def head_zero_ablation_hook(\n",
    "    z: Tensor,\n",
    "    hook: HookPoint,\n",
    "    head_index_to_ablate: int,\n",
    ") -> None:\n",
    "    z[:, :, head_index_to_ablate, :] = 0.0\n",
    "\n",
    "\n",
    "def get_ablation_scores(\n",
    "    model: HookedTransformer,\n",
    "    tokens: Tensor,\n",
    "    ablation_function: Callable = head_zero_ablation_hook,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output\n",
    "    of each head.\n",
    "    \"\"\"\n",
    "    # Initialize an object to store the ablation scores\n",
    "    ablation_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "    # Calculating loss without any ablation, to act as a baseline\n",
    "    model.reset_hooks()\n",
    "    seq_len = (tokens.shape[1] - 1) // 2\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    loss_no_ablation = -get_log_probs(logits, tokens)[:, -(seq_len - 1) :].mean()\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # Use functools.partial to create a temporary hook function with the head number fixed\n",
    "            temp_hook_fn = functools.partial(ablation_function, head_index_to_ablate=head)\n",
    "            # Run the model with the ablation hook\n",
    "            ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[(utils.get_act_name(\"z\", layer), temp_hook_fn)])\n",
    "            # Calculate the loss difference (= negative correct logprobs), only on the last `seq_len` tokens\n",
    "            loss = -get_log_probs(ablated_logits, tokens)[:, -(seq_len - 1) :].mean()\n",
    "            # Store the result, subtracting the clean loss so that a value of zero means no change in loss\n",
    "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
    "\n",
    "    return ablation_scores\n",
    "\n",
    "def test_get_ablation_scores(\n",
    "    ablation_scores: Tensor,\n",
    "    model: HookedTransformer,\n",
    "    rep_tokens: Tensor,\n",
    "):\n",
    "\n",
    "    ablation_scores_expected = get_ablation_scores(model, rep_tokens)\n",
    "\n",
    "    torch.testing.assert_close(ablation_scores, ablation_scores_expected)\n",
    "\n",
    "    print(\"All tests in `test_get_ablation_scores` passed!\")\n",
    "\n",
    "ablation_scores = get_ablation_scores(model, repeated_tokens)\n",
    "test_get_ablation_scores(ablation_scores, model, repeated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_utils.imshow(\n",
    "    ablation_scores,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\", \"color\": \"Logit diff\"},\n",
    "    title=\"Loss Difference After Ablating Heads\",\n",
    "    text_auto=\".2f\",\n",
    "    width=900,\n",
    "    height=350,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEFORE FINISH\n",
    "\n",
    "- GO BACK TO `A BIT MORE ABOUT HOOKS`\n",
    "\n",
    "# Sources\n",
    "\n",
    "1. [Ground truth - TransformerLens: Hooks, by ARENA](https://arena-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)\n",
    "2. [Pytorch 101: Understanding hooks, by DigitalOcean](https://www.digitalocean.com/community/tutorials/pytorch-hooks-gradient-clipping-debugging)\n",
    "3. [Garcon, by Neel Nanda, Chris Olah, et al.](https://transformer-circuits.pub/2021/garcon/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
