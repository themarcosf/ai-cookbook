{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Multi Layer Perceptron (MLP)\n",
    "\n",
    "This implementation is based along the lines of [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "names = json.loads(open(\"names.txt\", \"r\").read())\n",
    "names = names[\"payload\"][\"blob\"][\"rawLines\"]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "chtoi = {ch:i+1 for i, ch in enumerate(chars)}\n",
    "chtoi[\".\"] = 0\n",
    "itoch = {i:ch for ch, i in chtoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "[0, 0, 5]\n",
      "..e ----> m\n",
      "[0, 5, 13]\n",
      ".em ----> m\n",
      "[5, 13, 13]\n",
      "emm ----> a\n",
      "[13, 13, 1]\n",
      "mma ----> .\n",
      "[13, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "context_len = 3     # number of characters to use as context\n",
    "X, y = [], []       # training data split into inputs and targets\n",
    "\n",
    "for n in names[:1]:\n",
    "    print(n)\n",
    "    context = [0] * context_len\n",
    "\n",
    "    for ch in n + \".\":\n",
    "        ix = chtoi[ch]\n",
    "        X.append(context)\n",
    "        y.append(ix)\n",
    "        print(\"\".join([itoch[i] for i in context]), \"---->\", itoch[ix])\n",
    "        context = context[1:] + [ix]\n",
    "        print(context)\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.int64, torch.Size([5]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, y.shape, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1671, -0.1747],\n",
       "         [ 0.1671, -0.1747],\n",
       "         [ 0.1671, -0.1747]],\n",
       "\n",
       "        [[ 0.1671, -0.1747],\n",
       "         [ 0.1671, -0.1747],\n",
       "         [ 0.2937, -0.5093]],\n",
       "\n",
       "        [[ 0.1671, -0.1747],\n",
       "         [ 0.2937, -0.5093],\n",
       "         [-0.0251, -0.5869]],\n",
       "\n",
       "        [[ 0.2937, -0.5093],\n",
       "         [-0.0251, -0.5869],\n",
       "         [-0.0251, -0.5869]],\n",
       "\n",
       "        [[-0.0251, -0.5869],\n",
       "         [-0.0251, -0.5869],\n",
       "         [ 0.5192, -1.9340]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn(27, 2)     # randomly-initialized embedding matrix (27 characters, 2 dimensions)\n",
    "emb = C[X]                 # embeddings for the input sequence\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the tanh hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = torch.randn(6, 100)                      # randomly-initialized hidden layer weights (6 inputs, 100 neurons)\n",
    "                                              # (6 inputs = 2-d embeddings * 3 context characters)\n",
    "b1 = torch.randn(100)                         # randomly-initialized hidden layer biases (100 neurons)\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)     # hidden layer activations\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn(100, 27)                     # randomly-initialized output layer weights (100 neurons, 27 outputs)\n",
    "b2 = torch.randn(27)                          # randomly-initialized output layer biases (27 outputs)\n",
    "\n",
    "logits = h @ W2 + b2                          # output layer activations\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()                        # convert activations to counts\n",
    "prob = counts / counts.sum(1, keepdims=True) # softmax\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.9712)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(prob.shape[0]), y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.9712)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.cross_entropy(logits, y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the full network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [C, W1, b1, W2, b2]\n",
    "sum(p.numel() for p in parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overfitted loss: 0.0331801138818264\n"
     ]
    }
   ],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for _ in range(100):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter updates\n",
    "    for p in parameters:\n",
    "        p.data -= 0.01 * p.grad\n",
    "\n",
    "print(\"overfitted loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is overfitting because we have only 5 training examples, being fitted by 3481 parameters. We need more data to train this network.\n",
    "\n",
    "### Training on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 16.304555892944336\n",
      "loss: 16.18372917175293\n",
      "loss: 16.063337326049805\n",
      "loss: 15.943434715270996\n",
      "loss: 15.824076652526855\n",
      "loss: 15.705314636230469\n",
      "loss: 15.587221145629883\n",
      "loss: 15.469862937927246\n",
      "loss: 15.353293418884277\n",
      "loss: 15.23757266998291\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for n in names:\n",
    "    context = [0] * context_len\n",
    "    for ch in n + \".\":\n",
    "        ix = chtoi[ch]\n",
    "        X.append(context)\n",
    "        y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "C = torch.randn(27, 2)\n",
    "W1 = torch.randn(6, 100)\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn(100, 27)\n",
    "b2 = torch.randn(27)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(\"loss:\", loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter updates\n",
    "    for p in parameters:\n",
    "        p.data -= 0.01 * p.grad\n",
    "    \"\"\"\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
