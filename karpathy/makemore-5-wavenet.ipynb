{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Starter code\n",
    "\n",
    "## 1.1. Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "names = json.loads(open(\"../data/names.txt\", \"r\").read())\n",
    "names = names[\"payload\"][\"blob\"][\"rawLines\"]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  32033\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset: \", len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chtoi:  {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "itoch:  {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "vocab_size:  27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "chtoi = {ch:i+1 for i, ch in enumerate(chars)}\n",
    "chtoi[\".\"] = 0\n",
    "itoch = {i:ch for ch, i in chtoi.items()}\n",
    "vocab_size = len(chtoi)\n",
    "print(\"chtoi: \", chtoi)\n",
    "print(\"itoch: \", itoch)\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr.shape: torch.Size([182461, 3]), ytr.shape: torch.Size([182461])\n",
      "Xval.shape: torch.Size([22779, 3]), yval.shape: torch.Size([22779])\n",
      "Xte.shape: torch.Size([22906, 3]), yte.shape: torch.Size([22906])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_dataset(data, mode=\"tr\", context_len=3):\n",
    "  \"\"\"\n",
    "  Builds the dataset from the given list of words.\n",
    "  \n",
    "  Args:\n",
    "    data (list): list of words to use\n",
    "    mode (str): mode of the dataset (default: \"tr\" for training)\n",
    "    context_len (int): length of the context window (default: 3)\n",
    "  \"\"\"\n",
    "  \n",
    "  X, y = [], []\n",
    "  \n",
    "  for word in data:\n",
    "    context = [0] * context_len\n",
    "    for ch in word + \".\":\n",
    "      ix = chtoi[ch]\n",
    "      X.append(context)\n",
    "      y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "  \n",
    "  X = torch.tensor(X)\n",
    "  y = torch.tensor(y)\n",
    "  \n",
    "  print(f\"X{mode}.shape: {X.shape}, y{mode}.shape: {y.shape}\")\n",
    "  return X, y\n",
    "\n",
    "\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "Xtr, ytr   = build_dataset(names[:n1]  , mode=\"tr\")\n",
    "Xval, yval = build_dataset(names[n1:n2], mode=\"val\")\n",
    "Xte, yte   = build_dataset(names[n2:]  , mode=\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... > y\n",
      "..y > a\n",
      ".ya > r\n",
      "yar > i\n",
      "ari > e\n"
     ]
    }
   ],
   "source": [
    "for X, y in zip(Xtr[:5], ytr[:5]):\n",
    "  print(\"\".join(itoch[ix.item()] for ix in X), \">\", itoch[y.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Neural-layer classes definition\n",
    "\n",
    "This segment reimplements the neural-layer classes from the previous notebooks and defines an API quite similar to PyTorch's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Sequential:\n",
    "  def __init__(self, name, layers):\n",
    "    self.name = name\n",
    "    self.layers = layers\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.name\n",
    "    \n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    self.out = x\n",
    "    return self.out\n",
    "\n",
    "  def params(self):\n",
    "    return [p for layer in self.layers for p in layer.params()]\n",
    "\n",
    "class Embedding:\n",
    "  def __init__(self, vocab_size, emb_size):\n",
    "    self.weight = torch.randn(vocab_size, emb_size) / math.sqrt(emb_size)\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Embedding({self.weight.shape})\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = self.weight[x]\n",
    "    return self.out\n",
    "\n",
    "  def params(self):\n",
    "    return [self.weight]\n",
    "  \n",
    "class Flatten:\n",
    "  def __call__(self, x):\n",
    "    self.out = x.view(x.shape[0], -1)\n",
    "    return self.out\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"Flatten({self.out.shape})\"\n",
    "\n",
    "  def params(self):\n",
    "    return []\n",
    "  \n",
    "class Linear:\n",
    "  \"\"\"\n",
    "  This layer applies a linear transformation to its input, expressed as\n",
    "  y = x @ W + b, where x is the input, W is the weight matrix and b is the bias.\n",
    "  This layer is also known as a fully connected layer or a dense layer.\n",
    "  \"\"\"\n",
    "  def __init__(self, fan_in, fan_out, kaiming=True, scaling=False, bias=True):\n",
    "    self.gain = 5.0 / 3.0\n",
    "    self.weight = torch.randn(fan_in, fan_out) / math.sqrt(fan_in)\n",
    "    self.weight = self.weight * self.gain if kaiming else self.weight\n",
    "    self.weight = self.weight * 0.1 if scaling else self.weight\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Linear({self.weight.shape}, {None if self.bias is None else self.bias})\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    self.out = self.out + self.bias if self.bias is not None else self.out\n",
    "    return self.out\n",
    "\n",
    "  def params(self):\n",
    "    return [self.weight, self.bias] if self.bias is not None else [self.weight]\n",
    "  \n",
    "class BatchNorm:\n",
    "  \"\"\"\n",
    "  This layer applies batch normalization to its input, which is a technique\n",
    "  that normalizes the input to the layer, in order to make the training process\n",
    "  more stable and faster.\n",
    "  \"\"\"\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.01):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    self.beta = torch.zeros((1, dim))                  # aka shift\n",
    "    self.gamma = torch.ones((1, dim))                  # aka scale\n",
    "\n",
    "    # running statistics buffers\n",
    "    self.running_mean = torch.zeros((1, dim))\n",
    "    self.running_var = torch.ones((1, dim))\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"BatchNorm({self.gamma.shape}, {self.beta.shape})\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True)                  # batch mean\n",
    "      xvar = x.var(0, keepdim=True)                    # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    \n",
    "    x_hat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize\n",
    "    self.out = self.gamma * x_hat + self.beta          # scale and shift\n",
    "\n",
    "    # update buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "\n",
    "    return self.out\n",
    "\n",
    "  def params(self):\n",
    "    return [self.gamma, self.beta]\n",
    "  \n",
    "class Tanh:\n",
    "  \"\"\"\n",
    "  This layer applies an activation function (the hyperbolic tangent) element-wise\n",
    "  to its input.\n",
    "  \"\"\"\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  \n",
    "  def __repr__(self):\n",
    "    return f\"Tanh({self.out})\"\n",
    "\n",
    "  def params(self):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Neural network initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd      = 10\n",
    "n_hidden    = 200\n",
    "context_len = 3\n",
    "\n",
    "base_model = Sequential(\"base-model\", [\n",
    "  Embedding(vocab_size, n_embd), Flatten(), \n",
    "  Linear(n_embd * context_len, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, scaling=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  12097\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.nelement() for p in base_model.params()))\n",
    "for p in base_model.params():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 3.3151\n",
      "  25000/ 200000: 2.2749\n",
      "  50000/ 200000: 2.0545\n",
      "  75000/ 200000: 1.9939\n",
      " 100000/ 200000: 2.4061\n",
      " 125000/ 200000: 2.2899\n",
      " 150000/ 200000: 2.3502\n",
      " 175000/ 200000: 2.0109\n",
      " 200000/ 200000: 1.9919\n"
     ]
    }
   ],
   "source": [
    "max_steps  = 200_000\n",
    "batch_size = 32\n",
    "lossi      = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, yb = Xtr[ix], ytr[ix]\n",
    "\n",
    "  # forward pass\n",
    "  logits = base_model(Xb)\n",
    "  loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "  # backward pass\n",
    "  for p in base_model.params():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update params\n",
    "  lr = 0.1 if step < 100_000 else 0.02\n",
    "  for p in base_model.params():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if step == 0 or (step+1) % 25_000 == 0:\n",
    "    print(f\"{(step+1):7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (32, 3, 10)\n",
      "Flatten : (32, 30)\n",
      "Linear : (32, 200)\n",
      "BatchNorm : (32, 200)\n",
      "Tanh : (32, 200)\n",
      "Linear : (32, 27)\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "  print(layer.__class__.__name__, \":\", tuple(layer.out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(105, 0.345, 'Learning rate decay')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))\n",
    "plt.title(\"Loss decay during training\")\n",
    "plt.xlabel(\"Number of epochs (thousands)\")\n",
    "plt.ylabel(\"Log of loss\")\n",
    "\n",
    "plt.annotate('Learning rate decay',\n",
    "             xy=(102, 0.335),\n",
    "             xytext=(105, 0.345),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.02)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{base-model: {'context_len': 3, 'hidden_neurons': 200, 'params': 12097, 'tr_loss': 2.0685, 'val_loss': 2.1162, 'te_loss': 2.1232}}\n"
     ]
    }
   ],
   "source": [
    "performance_log = {}\n",
    "\n",
    "for layer in base_model.layers:\n",
    "  layer.training = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(model):\n",
    "  tr_logits  = model(Xtr)\n",
    "  val_logits = model(Xval)\n",
    "  te_logits  = model(Xte)\n",
    "\n",
    "  performance_log[model] = {\n",
    "    \"context_len\": context_len,\n",
    "    \"hidden_neurons\": n_hidden,\n",
    "    \"params\": sum(p.nelement() for p in model.params()),\n",
    "    \"tr_loss\": round(F.cross_entropy(tr_logits, ytr).item(), 4),\n",
    "    \"val_loss\": round(F.cross_entropy(val_logits, yval).item(), 4),\n",
    "    \"te_loss\": round(F.cross_entropy(te_logits, yte).item(), 4),\n",
    "  }\n",
    "\n",
    "  print(performance_log)\n",
    "  \n",
    "split_loss(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6. Sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "janellanabir.\n",
      "malexy.\n",
      "cia.\n",
      "cobby.\n",
      "adconniel.\n",
      "raber.\n",
      "eljoniel.\n",
      "hem.\n",
      "oni.\n",
      "levanni.\n",
      "quan.\n",
      "azmarrudenayon.\n",
      "kona.\n",
      "ley.\n",
      "aary.\n",
      "chamelecklin.\n",
      "devie.\n",
      "cob.\n",
      "takari.\n",
      "macle.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * context_len\n",
    "  while True:\n",
    "    # forward pass the neural net\n",
    "    logits = base_model(torch.tensor(context).unsqueeze(0))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # sample from the distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    # shift context window and track samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(\"\".join(itoch[i] for i in out))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lazily scaling up context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr.shape: torch.Size([182461, 8]), ytr.shape: torch.Size([182461])\n",
      "Xval.shape: torch.Size([22779, 8]), yval.shape: torch.Size([22779])\n",
      "Xte.shape: torch.Size([22906, 8]), yte.shape: torch.Size([22906])\n"
     ]
    }
   ],
   "source": [
    "Xtr, ytr   = build_dataset(names[:n1]  , mode=\"tr\" , context_len=8)\n",
    "Xval, yval = build_dataset(names[n1:n2], mode=\"val\", context_len=8)\n",
    "Xte, yte   = build_dataset(names[n2:]  , mode=\"te\" , context_len=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........ > y\n",
      ".......y > a\n",
      "......ya > r\n",
      ".....yar > i\n",
      "....yari > e\n"
     ]
    }
   ],
   "source": [
    "for X, y in zip(Xtr[:5], ytr[:5]):\n",
    "  print(\"\".join(itoch[ix.item()] for ix in X), \">\", itoch[y.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd      = 10\n",
    "n_hidden    = 200\n",
    "context_len = 8\n",
    "\n",
    "scaled_model = Sequential(\"scaled-model\", [\n",
    "  Embedding(vocab_size, n_embd), Flatten(), \n",
    "  Linear(n_embd * context_len, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, scaling=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  22097\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.nelement() for p in scaled_model.params()))\n",
    "for p in scaled_model.params():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 3.2814\n",
      "  25000/ 200000: 1.8952\n",
      "  50000/ 200000: 2.3980\n",
      "  75000/ 200000: 2.0952\n",
      " 100000/ 200000: 1.5902\n",
      " 125000/ 200000: 1.6262\n",
      " 150000/ 200000: 1.7623\n",
      " 175000/ 200000: 1.7887\n",
      " 200000/ 200000: 2.0464\n"
     ]
    }
   ],
   "source": [
    "max_steps  = 200_000\n",
    "batch_size = 32\n",
    "lossi      = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, yb = Xtr[ix], ytr[ix]\n",
    "\n",
    "  # forward pass\n",
    "  logits = scaled_model(Xb)\n",
    "  loss   = F.cross_entropy(logits, yb)\n",
    "\n",
    "  # backward pass\n",
    "  for p in scaled_model.params():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update params\n",
    "  lr = 0.1 if step < 100_000 else 0.01\n",
    "  for p in scaled_model.params():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if step == 0 or (step+1) % 25_000 == 0:\n",
    "    print(f\"{(step+1):7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (32, 8, 10)\n",
      "Flatten : (32, 80)\n",
      "Linear : (32, 200)\n",
      "BatchNorm : (32, 200)\n",
      "Tanh : (32, 200)\n",
      "Linear : (32, 27)\n",
      "{base-model: {'context_len': 3, 'hidden_neurons': 200, 'params': 12097, 'tr_loss': 2.0685, 'val_loss': 2.1162, 'te_loss': 2.1232}, scaled-model: {'context_len': 8, 'hidden_neurons': 200, 'params': 22097, 'tr_loss': 1.9319, 'val_loss': 2.0305, 'te_loss': 2.0369}}\n"
     ]
    }
   ],
   "source": [
    "for layer in scaled_model.layers:\n",
    "  print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n",
    "  layer.training = False\n",
    "  \n",
    "split_loss(scaled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jayah.\n",
      "mandix.\n",
      "shaun.\n",
      "rowk.\n",
      "jorner.\n",
      "kiria.\n",
      "isaryk.\n",
      "ramjilla.\n",
      "zyanyah.\n",
      "lucilah.\n",
      "stion.\n",
      "damyah.\n",
      "soan.\n",
      "lthon.\n",
      "frenkish.\n",
      "aliga.\n",
      "grace.\n",
      "depsalia.\n",
      "alahi.\n",
      "jaaviyah.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * context_len\n",
    "  while True:\n",
    "    # forward pass the neural net\n",
    "    logits = scaled_model(torch.tensor(context).unsqueeze(0))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # sample from the distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    # shift context window and track samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(\"\".join(itoch[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing WaveNet\n",
    "\n",
    "The architecture currently being used was proposed by [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), in which a given number of characters are fed through a single hidden layer and then the output is used to predict the next character. In the first model presented here, the context length is 3 characters and in the second model, the context length is 8 characters.\n",
    "\n",
    "<img src=\"../assets/neural-prob-LM.jpg\" width=\"400\"/>\n",
    "\n",
    "Even though additional layers could be introduced, making the neural network deeper, the model would still be compressing all the information from the input into a single vector, which is then used to predict the next character.\n",
    "\n",
    "**WaveNet**, on the other hand, is a deep generative model that is able to generate the output using a stack of *dilated causal convolutions*. This means that the output at a given time step is only dependent on the input at previous time steps, and the receptive field of the model grows exponentially with the number of layers.\n",
    "\n",
    "<img src=\"../assets/wavenet.jpg\" width=\"400\"/>\n",
    "\n",
    "### 3.1. Scratch space\n",
    "\n",
    "Understanding the forward pass of the neural networks implemented previously and inspecting the shapes of the tensors being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a batch of size 4\n",
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xb, yb = Xtr[ix], ytr[ix]\n",
    "Xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape of the first layer of the model (the Embedding layer)\n",
    "scaled_model.layers[0].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape of the second layer of the model (the Flatten layer)\n",
    "scaled_model.layers[1].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape of the third layer of the model (the Linear layer)\n",
    "scaled_model.layers[2].out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inner operation of the third layer of the model (the Linear layer)\n",
    "(torch.randn(4, 80) @ torch.randn(80, 200) + torch.randn(200)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: inner operation above also works with high dimensional input data (e.g. 3D)\n",
    "(torch.randn(4, 5, 80) @ torch.randn(80, 200) + torch.randn(200)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 200])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the previous models, the input characters were flattened into a single dimension\n",
    "# now, we will introduce a batch dimension to fuse the characters into two-element pairs\n",
    "# eg. (1, 2) (3, 4) (5, 6) (7, 8), embedded into batch_size x 2D_pairs x n_embd tensor\n",
    "(torch.randn(4, 4, 20) @ torch.randn(20, 200) + torch.randn(200)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 20])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# goal: reshape output of Embedding layer into a tensor of shape (4, 4, 20)\n",
    "e = torch.randn(4, 8, 10)     # current architecture: (batch_size, context_len, n_embd)\n",
    "exp_cat = torch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2)\n",
    "exp_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(e.view(4, -1, 20) == exp_cat).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Updating the Flatten layer to include batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenV2(Flatten):\n",
    "  def __init__(self, n_elements):\n",
    "    self.n_elements = n_elements\n",
    "\n",
    "  def __call__(self, x):\n",
    "    B, T, C = x.shape\n",
    "    x = x.view(B, T // self.n_elements, C * self.n_elements)\n",
    "    if x.shape[1] == 1:\n",
    "      x = x.squeeze(1)\n",
    "    self.out = x\n",
    "    return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 68     # adjust hidden layer so total number of parameters is the same of previous models\n",
    "\n",
    "wavenet_model_v1 = Sequential(\"wavenet-model-v1\", [\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenV2(2), Linear(n_embd * 2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNorm(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, scaling=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  22397\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.nelement() for p in wavenet_model_v1.params()))\n",
    "for p in wavenet_model_v1.params():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Training the WaveNet: first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 3.2903\n",
      "  25000/ 200000: 2.4359\n",
      "  50000/ 200000: 1.6422\n",
      "  75000/ 200000: 2.0187\n",
      " 100000/ 200000: 2.1975\n",
      " 125000/ 200000: 2.1933\n",
      " 150000/ 200000: 1.5275\n",
      " 175000/ 200000: 2.1747\n",
      " 200000/ 200000: 1.7174\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, yb = Xtr[ix], ytr[ix]\n",
    "\n",
    "  # forward pass\n",
    "  logits = wavenet_model_v1(Xb)\n",
    "  loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "  # backward pass\n",
    "  for p in wavenet_model_v1.params():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update params\n",
    "  lr = 0.1 if step < 100_000 else 0.02\n",
    "  for p in wavenet_model_v1.params():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if step == 0 or (step+1) % 25_000 == 0:\n",
    "    print(f\"{(step+1):7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (32, 8, 10)\n",
      "FlattenV2 : (32, 4, 20)\n",
      "Linear : (32, 4, 68)\n",
      "BatchNorm : (32, 4, 68)\n",
      "Tanh : (32, 4, 68)\n",
      "FlattenV2 : (32, 2, 136)\n",
      "Linear : (32, 2, 68)\n",
      "BatchNorm : (32, 2, 68)\n",
      "Tanh : (32, 2, 68)\n",
      "FlattenV2 : (32, 136)\n",
      "Linear : (32, 68)\n",
      "BatchNorm : (32, 68)\n",
      "Tanh : (32, 68)\n",
      "Linear : (32, 27)\n",
      "{base-model: {'context_len': 3, 'hidden_neurons': 200, 'params': 12097, 'tr_loss': 2.0685, 'val_loss': 2.1162, 'te_loss': 2.1232}, scaled-model: {'context_len': 8, 'hidden_neurons': 200, 'params': 22097, 'tr_loss': 1.9319, 'val_loss': 2.0305, 'te_loss': 2.0369}, wavenet-model-v1: {'context_len': 8, 'hidden_neurons': 68, 'params': 22397, 'tr_loss': 1.9525, 'val_loss': 2.0256, 'te_loss': 2.044}}\n"
     ]
    }
   ],
   "source": [
    "for layer in wavenet_model_v1.layers:\n",
    "  print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n",
    "  layer.training = False\n",
    "  \n",
    "split_loss(wavenet_model_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banitter.\n",
      "daverly.\n",
      "aivany.\n",
      "cermie.\n",
      "cynni.\n",
      "ataleya.\n",
      "eristyn.\n",
      "ruberfo.\n",
      "kinlee.\n",
      "jaire.\n",
      "ruhhyn.\n",
      "arahir.\n",
      "makor.\n",
      "alizela.\n",
      "barattin.\n",
      "dekse.\n",
      "mykana.\n",
      "marvaci.\n",
      "kaimiyah.\n",
      "deyonn.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * context_len\n",
    "  while True:\n",
    "    # forward pass the neural net\n",
    "    logits = wavenet_model_v1(torch.tensor(context).unsqueeze(0))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # sample from the distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    # shift context window and track samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(\"\".join(itoch[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Updating the BatchNorm layer to include batch dimension\n",
    "\n",
    "The Batch Normalization layer, as currently implemented, is receiving a 2D tensor of shape `(batch_size, n_hidden)`. Since, in Pytorch, everything broadcasts properly, this layer is able to normalize the activations of the hidden layer. However, in the WaveNet model, the hidden layer is a 3D tensor of shape `(batch_size, context_len // dilated_causal_convolutions, n_hidden)`. This means that the Batch Normalization layer is normalizing the activations of the hidden layer along the wrong axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4956,  0.2474, -0.7903],\n",
       "         [-0.7031, -0.3599, -0.4418]]),\n",
       " tensor([[-1.0993, -0.0563, -0.6161]]),\n",
       " tensor([[0.3140, 0.1844, 0.0607]]),\n",
       " tensor([[-0.7071,  0.7071, -0.7070],\n",
       "         [ 0.7071, -0.7071,  0.7070]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how it is currently implemented...\n",
    "e      = torch.randn(2, 3)                         # input of shape (2, 3)\n",
    "e_mean = e.mean(0, keepdim=True)                   # batch mean of shape (1, 3)\n",
    "e_var  = e.var(0, keepdim=True)                    # batch variance of shape (1, 3)\n",
    "e_hat  = (e - e_mean) / torch.sqrt(e_var + 1e-5)   # normalized input of shape (2, 3)\n",
    "e, e_mean, e_var, e_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.3287,  0.4370,  0.5469, -0.9049],\n",
       "          [ 0.5698,  1.6174,  0.9841,  0.4329],\n",
       "          [-0.0867,  0.0665,  1.2209,  0.8377]],\n",
       " \n",
       "         [[ 0.0830,  0.8253, -0.7065, -1.0320],\n",
       "          [-0.4697,  1.0208, -0.8714, -0.2629],\n",
       "          [-1.8326,  0.0820,  0.9459,  0.0695]]]),\n",
       " tensor([[[ 0.7058,  0.6312, -0.0798, -0.9684],\n",
       "          [ 0.0501,  1.3191,  0.0564,  0.0850],\n",
       "          [-0.9596,  0.0743,  1.0834,  0.4536]]]),\n",
       " tensor([[[7.7596e-01, 7.5385e-02, 7.8556e-01, 8.0742e-03],\n",
       "          [5.4034e-01, 1.7794e-01, 1.7215e+00, 2.4204e-01],\n",
       "          [1.5242e+00, 1.2016e-04, 3.7838e-02, 2.9507e-01]]]),\n",
       " tensor([[[ 0.7071, -0.7071,  0.7071,  0.7067],\n",
       "          [ 0.7071,  0.7071,  0.7071,  0.7071],\n",
       "          [ 0.7071, -0.6794,  0.7070,  0.7071]],\n",
       " \n",
       "         [[-0.7071,  0.7071, -0.7071, -0.7067],\n",
       "          [-0.7071, -0.7071, -0.7071, -0.7071],\n",
       "          [-0.7071,  0.6794, -0.7070, -0.7071]]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...how it will incorrectly broadcast...\n",
    "e      = torch.randn(2, 3, 4)                      # input of shape (2, 3, 4)\n",
    "e_mean = e.mean(0, keepdim=True)                   # batch mean of shape (1, 3, 4)\n",
    "e_var  = e.var(0, keepdim=True)                    # batch variance of shape (1, 3, 4)\n",
    "e_hat  = (e - e_mean) / torch.sqrt(e_var + 1e-5)   # normalized input of shape (2, 3, 4)\n",
    "e, e_mean, e_var, e_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3711, -1.0470,  1.2473, -2.2059],\n",
       "          [ 1.4369,  0.3064,  0.4757,  0.0254],\n",
       "          [-1.1406, -1.5060, -0.3617, -0.8203]],\n",
       " \n",
       "         [[ 0.0976, -0.0644,  1.4956,  0.8565],\n",
       "          [ 0.6067, -1.0009,  0.6808,  0.9212],\n",
       "          [ 0.4694, -0.3725, -0.3627,  0.0358]]]),\n",
       " tensor([[[ 0.3069, -0.6141,  0.5292, -0.1979]]]),\n",
       " tensor([[[0.7073, 0.4681, 0.6130, 1.3776]]]),\n",
       " tensor([[[ 0.0764, -0.6328,  0.9173, -1.7108],\n",
       "          [ 1.3437,  1.3454, -0.0683,  0.1902],\n",
       "          [-1.7211, -1.3036, -1.1379, -0.5303]],\n",
       " \n",
       "         [[-0.2488,  0.8034,  1.2344,  0.8983],\n",
       "          [ 0.3565, -0.5655,  0.1936,  0.9534],\n",
       "          [ 0.1933,  0.3531, -1.1391,  0.1991]]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...how it should be\n",
    "e      = torch.randn(2, 3, 4)                      # input of shape (2, 3, 4)\n",
    "e_mean = e.mean((0, 1), keepdim=True)              # batch mean of shape (1, 1, 4)\n",
    "e_var  = e.var((0, 1), keepdim=True)               # batch variance of shape (1, 1, 4)\n",
    "e_hat  = (e - e_mean) / torch.sqrt(e_var + 1e-5)   # normalized input of shape (2, 3, 4)\n",
    "e, e_mean, e_var, e_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormV2(BatchNorm):\n",
    "  \"\"\"\n",
    "  This layer applies batch normalization to its input, which is a technique\n",
    "  that normalizes the input to the layer, in order to make the training process\n",
    "  more stable and faster.\n",
    "\n",
    "  V2 can handle both 2D and 3D input matrices.\n",
    "  \"\"\"\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.01):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    self.beta = torch.zeros((1, dim))                  # aka shift\n",
    "    self.gamma = torch.ones((1, dim))                  # aka scale\n",
    "\n",
    "    # running statistics buffers\n",
    "    self.running_mean = torch.zeros((1, dim))\n",
    "    self.running_var = torch.ones((1, dim))\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"BatchNorm({self.gamma.shape}, {self.beta.shape})\"\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # forward pass\n",
    "    if self.training:\n",
    "      dim   = 0 if x.ndim == 2 else (0, 1)\n",
    "      xmean = x.mean(dim, keepdim=True)                     # batch mean\n",
    "      xvar  = x.var(dim, keepdim=True)                      # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar  = self.running_var\n",
    "    \n",
    "    x_hat    = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize\n",
    "    self.out = self.gamma * x_hat + self.beta             # scale and shift\n",
    "\n",
    "    # update buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var  = (1 - self.momentum) * self.running_var  + self.momentum * xvar\n",
    "\n",
    "    return self.out\n",
    "\n",
    "  def params(self):\n",
    "    return [self.gamma, self.beta]\n",
    "  \n",
    "\n",
    "class BatchNormV2(BatchNorm):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Training the WaveNet: second pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 68     \n",
    "\n",
    "wavenet_model_v2 = Sequential(\"wavenet-model-v2\", [\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenV2(2), Linear(n_embd   * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, scaling=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  22397\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.nelement() for p in wavenet_model_v2.params()))\n",
    "for p in wavenet_model_v2.params():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 3.2802\n",
      "  25000/ 200000: 2.3674\n",
      "  50000/ 200000: 2.2365\n",
      "  75000/ 200000: 1.4827\n",
      " 100000/ 200000: 1.7679\n",
      " 125000/ 200000: 2.2758\n",
      " 150000/ 200000: 2.0375\n",
      " 175000/ 200000: 1.7230\n",
      " 200000/ 200000: 1.9166\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, yb = Xtr[ix], ytr[ix]\n",
    "\n",
    "  # forward pass\n",
    "  logits = wavenet_model_v2(Xb)\n",
    "  loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "  # backward pass\n",
    "  for p in wavenet_model_v2.params():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update params\n",
    "  lr = 0.1 if step < 100_000 else 0.02\n",
    "  for p in wavenet_model_v2.params():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if step == 0 or (step+1) % 25_000 == 0:\n",
    "    print(f\"{(step+1):7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (32, 8, 10)\n",
      "FlattenV2 : (32, 4, 20)\n",
      "Linear : (32, 4, 68)\n",
      "BatchNormV2 : (32, 4, 68)\n",
      "Tanh : (32, 4, 68)\n",
      "FlattenV2 : (32, 2, 136)\n",
      "Linear : (32, 2, 68)\n",
      "BatchNormV2 : (32, 2, 68)\n",
      "Tanh : (32, 2, 68)\n",
      "FlattenV2 : (32, 136)\n",
      "Linear : (32, 68)\n",
      "BatchNormV2 : (32, 68)\n",
      "Tanh : (32, 68)\n",
      "Linear : (32, 27)\n",
      "{base-model: {'context_len': 3, 'hidden_neurons': 200, 'params': 12097, 'tr_loss': 2.0685, 'val_loss': 2.1162, 'te_loss': 2.1232}, scaled-model: {'context_len': 8, 'hidden_neurons': 200, 'params': 22097, 'tr_loss': 1.9319, 'val_loss': 2.0305, 'te_loss': 2.0369}, wavenet-model-v1: {'context_len': 8, 'hidden_neurons': 68, 'params': 22397, 'tr_loss': 1.9525, 'val_loss': 2.0256, 'te_loss': 2.044}, wavenet-model-v2: {'context_len': 8, 'hidden_neurons': 68, 'params': 22397, 'tr_loss': 1.9237, 'val_loss': 2.0237, 'te_loss': 2.0368}}\n"
     ]
    }
   ],
   "source": [
    "for layer in wavenet_model_v2.layers:\n",
    "  print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n",
    "  layer.training = False\n",
    "  \n",
    "split_loss(wavenet_model_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahnia.\n",
      "xatay.\n",
      "lyra.\n",
      "janai.\n",
      "mylyann.\n",
      "edricco.\n",
      "muhazha.\n",
      "liverlin.\n",
      "skite.\n",
      "perzie.\n",
      "kalayah.\n",
      "kori.\n",
      "adel.\n",
      "vyonna.\n",
      "treya.\n",
      "kaylani.\n",
      "aaris.\n",
      "kiann.\n",
      "akaall.\n",
      "angela.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * context_len\n",
    "  while True:\n",
    "    # forward pass the neural net\n",
    "    logits = wavenet_model_v2(torch.tensor(context).unsqueeze(0))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # sample from the distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    # shift context window and track samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(\"\".join(itoch[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Scaling up WaveNet: third pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 24\n",
    "n_hidden = 128     \n",
    "\n",
    "wavenet_model_v3 = Sequential(\"wavenet-model-v3\", [\n",
    "  Embedding(vocab_size, n_embd),\n",
    "  FlattenV2(2), Linear(n_embd   * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  FlattenV2(2), Linear(n_hidden * 2, n_hidden, bias=False), BatchNormV2(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, scaling=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  76579\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters: \", sum(p.nelement() for p in wavenet_model_v3.params()))\n",
    "for p in wavenet_model_v3.params():\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/ 200000: 3.3056\n",
      "  25000/ 200000: 2.1383\n",
      "  50000/ 200000: 1.6891\n",
      "  75000/ 200000: 2.2325\n",
      " 100000/ 200000: 1.8008\n",
      " 125000/ 200000: 1.7201\n",
      " 150000/ 200000: 1.9271\n",
      " 175000/ 200000: 2.0595\n",
      " 200000/ 200000: 1.5532\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for step in range(max_steps):\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "  Xb, yb = Xtr[ix], ytr[ix]\n",
    "\n",
    "  # forward pass\n",
    "  logits = wavenet_model_v3(Xb)\n",
    "  loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "  # backward pass\n",
    "  for p in wavenet_model_v3.params():\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # update params\n",
    "  lr = 0.1 if step < 100_000 else 0.02\n",
    "  for p in wavenet_model_v3.params():\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if step == 0 or (step+1) % 25_000 == 0:\n",
    "    print(f\"{(step+1):7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding : (32, 8, 24)\n",
      "FlattenV2 : (32, 4, 48)\n",
      "Linear : (32, 4, 128)\n",
      "BatchNormV2 : (32, 4, 128)\n",
      "Tanh : (32, 4, 128)\n",
      "FlattenV2 : (32, 2, 256)\n",
      "Linear : (32, 2, 128)\n",
      "BatchNormV2 : (32, 2, 128)\n",
      "Tanh : (32, 2, 128)\n",
      "FlattenV2 : (32, 256)\n",
      "Linear : (32, 128)\n",
      "BatchNormV2 : (32, 128)\n",
      "Tanh : (32, 128)\n",
      "Linear : (32, 27)\n",
      "{base-model: {'context_len': 3, 'hidden_neurons': 200, 'params': 12097, 'tr_loss': 2.0685, 'val_loss': 2.1162, 'te_loss': 2.1232}, scaled-model: {'context_len': 8, 'hidden_neurons': 200, 'params': 22097, 'tr_loss': 1.9319, 'val_loss': 2.0305, 'te_loss': 2.0369}, wavenet-model-v1: {'context_len': 8, 'hidden_neurons': 68, 'params': 22397, 'tr_loss': 1.9525, 'val_loss': 2.0256, 'te_loss': 2.044}, wavenet-model-v2: {'context_len': 8, 'hidden_neurons': 68, 'params': 22397, 'tr_loss': 1.9237, 'val_loss': 2.0237, 'te_loss': 2.0368}, wavenet-model-v3: {'context_len': 8, 'hidden_neurons': 128, 'params': 76579, 'tr_loss': 1.7918, 'val_loss': 2.0002, 'te_loss': 2.0094}}\n"
     ]
    }
   ],
   "source": [
    "for layer in wavenet_model_v3.layers:\n",
    "  print(layer.__class__.__name__, \":\", tuple(layer.out.shape))\n",
    "  layer.training = False\n",
    "  \n",
    "split_loss(wavenet_model_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taemar.\n",
      "artheriya.\n",
      "jamille.\n",
      "hermoni.\n",
      "dreisce.\n",
      "eriyanni.\n",
      "harmonie.\n",
      "zack.\n",
      "kathis.\n",
      "shayden.\n",
      "adaline.\n",
      "chisaber.\n",
      "sherri.\n",
      "kathon.\n",
      "brynlin.\n",
      "nahlie.\n",
      "kardier.\n",
      "rossey.\n",
      "jaqueline.\n",
      "smulyk.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * context_len\n",
    "  while True:\n",
    "    # forward pass the neural net\n",
    "    logits = wavenet_model_v3(torch.tensor(context).unsqueeze(0))\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "    # sample from the distribution\n",
    "    ix = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    # shift context window and track samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "    if ix == 0:\n",
    "      break\n",
    "\n",
    "  print(\"\".join(itoch[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Building makemore Part 5: Building a wavenet, By Andrej Karpathy](https://www.youtube.com/watch?v=t3YJ5hKiMQ0)\n",
    "2. van den Oord, Aäron et al. WaveNet: A generative model for raw audio, 2016. URL: https://arxiv.org/abs/1609.03499\n",
    "3. Torch.NN documentation: https://pytorch.org/docs/stable/nn.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
