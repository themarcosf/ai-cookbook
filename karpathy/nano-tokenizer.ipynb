{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"Much glory awaits someone who can delete the need for tokenization\" -- (Andrej Karpathy)**\n",
    "\n",
    "The tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which the vocabulary is trained using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequence of tokens. The LLM only ever sees the tokens and never directly deals with any text.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/tokenizer-llm-diagram.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Strings in Python\n",
    "\n",
    "According to Python's documentation, \"strings are immutable *sequences* of *Unicode code points*\". The function to access the Unicode code point of a character is `ord()`. The function to access the character of a Unicode code point is `chr()`. Also, Unicode text is processed and stored as binary data *using one of several encodings*: `UTF-8`, `UTF-16`, `UTF-32`, among others. Of these, `UTF-8` is the most widely used, in part due to its backwards-compatibility with ASCII. The function to encode a string into a binary data is `encode()`. The function to decode a binary data into a string is `decode()`.\n",
    "\n",
    "`UTF-8` means *Unicode Transformation Format - 8 bit* and supports all valid Unicode code points using a *variable-width encoding* of one to four one-byte code units. Code points with lower numerical values, which tend to occur more frequently, are encoded using fewer bytes. In the following table, the characters `u` to `z` are replaced by the bits of the code point, from the positions U+uvwxyz:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/utf8-encoding.jpg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Examples:\n",
    "- U+0041 (‘A’) → 01000001 → 01000001 (same as ASCII)\n",
    "- U+00A9 (‘©’)\t→ 1010001001 → 11010100 10010001\n",
    "\n",
    "Now, considering that `UTF-8` is represented as byte streams, it implies a maximum vocabulary length of 256 possible tokens. This means tiny embedding tables, counterweighted by very long sequences of tokens, which can be a hindrance to context length in transformer-based neural networks, where each token needs to attend to all other tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504, 45397, 54616, 49464, 50836]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_enc = [ord(x) for x in '안녕하세요']\n",
    "unicode_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94',\n",
       " [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_enc = '안녕하세요'.encode('utf-8')\n",
    "utf8_enc, list(utf8_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode length:  5\n",
      "UTF-8 length:  15\n"
     ]
    }
   ],
   "source": [
    "print('Unicode length: ', len(unicode_enc))\n",
    "print('UTF-8 length: ', len(utf8_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Byte Pair Encoding (BPE)\n",
    "\n",
    "This algorithm was first described in 1994, by Philip Gage, for encoding strings of text into smaller strings by creating and using a translation table. It builds \"tokens\" (units of recognition) that match varying amounts of source text, from single characters (including single digits or single punctuation marks) to whole words (even long compound words).\n",
    "\n",
    "Suppose the data to be encoded is:\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it is merged into a single token:\n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "The process is repeated with byte pair \"ab\", replacing it with Y:\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "Finally, the byte pair \"ZY\" is merged into a single token X:\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X = ZY\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "The data cannot be compressed further because there are no pairs of bytes that occur more than once. We started with 11 bytes and 4 tokens, and ended with 5 bytes and 6 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text:  1414\n"
     ]
    }
   ],
   "source": [
    "with open('../data/unicode.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Number of characters in the text: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single tokens in the text:  2058\n"
     ]
    }
   ],
   "source": [
    "# NOTE: unicode text encoded in utf-8 has up to 4 bytes per character\n",
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print('Number of single tokens in the text: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the text:  105\n",
      "Max token:  240\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(tokens)\n",
    "print('Number of unique tokens in the text: ', len(unique_tokens))\n",
    "print('Max token: ', max(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams:  617\n",
      "Most common bigrams:  [((101, 32), 24), ((204, 173), 18), ((205, 153), 18), ((204, 178), 18), ((115, 32), 17)]\n",
      "Most common bigram in text:  ('e', ' ')\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print('Number of unique bigrams: ', len(stats))\n",
    "print('Most common bigrams: ', sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "print('Most common bigram in text: ', (chr(101), chr(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the most common pair\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens before merging:  [5, 6, 6, 7, 9, 1]\n",
      "Example tokens after merging:  [5, 10, 7, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(tokens: list, pair: tuple[int, int], new_token: int) -> list:\n",
    "    \"\"\"Merges the most common pair in the given list of tokens into a single token.\"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "print('Example tokens before merging: ', ex_tokens := [5, 6, 6, 7, 9, 1])\n",
    "print('Example tokens after merging: ', merge(ex_tokens, (6, 6), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before merging:  2058\n",
      "Number of tokens after merging:  2034\n",
      "Number of unique tokens before merging:  105\n",
      "Number of unique tokens after merging:  106\n",
      "Max token before merging:  240\n",
      "Max token after merging:  241\n"
     ]
    }
   ],
   "source": [
    "merged_tokens = merge(tokens, top_pair, max(unique_tokens) + 1)\n",
    "print('Number of tokens before merging: ', len(tokens))\n",
    "print('Number of tokens after merging: ', len(merged_tokens))\n",
    "print('Number of unique tokens before merging: ', len(unique_tokens))\n",
    "print('Number of unique tokens after merging: ', len(set(merged_tokens)))\n",
    "print('Max token before merging: ', max(unique_tokens))\n",
    "print('Max token after merging: ', max(set(merged_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Training the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276                 # desired number of unique tokens in vocabulary\n",
    "max_tokens_per_byte = 2 ** 8     # encoding string into utf-8 converts characters into bytes\n",
    "num_merges = vocab_size - max_tokens_per_byte\n",
    "trainable_tokens = copy.deepcopy(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (101, 32) into new token 256\n",
      "Merging pair (204, 173) into new token 257\n",
      "Merging pair (205, 153) into new token 258\n",
      "Merging pair (204, 178) into new token 259\n",
      "Merging pair (115, 32) into new token 260\n",
      "Merging pair (204, 171) into new token 261\n",
      "Merging pair (204, 177) into new token 262\n",
      "Merging pair (240, 159) into new token 263\n",
      "Merging pair (205, 136) into new token 264\n",
      "Merging pair (204, 185) into new token 265\n",
      "Merging pair (226, 128) into new token 266\n",
      "Merging pair (105, 110) into new token 267\n",
      "Merging pair (205, 150) into new token 268\n",
      "Merging pair (204, 187) into new token 269\n",
      "Merging pair (205, 135) into new token 270\n",
      "Merging pair (204, 188) into new token 271\n",
      "Merging pair (204, 164) into new token 272\n",
      "Merging pair (204, 166) into new token 273\n",
      "Merging pair (97, 110) into new token 274\n",
      "Merging pair (204, 176) into new token 275\n"
     ]
    }
   ],
   "source": [
    "# `bpe_forest` is an inverted tree that stores merges: (int, int) -> int\n",
    "bpe_forest = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(trainable_tokens)\n",
    "    top_pair = max(stats, key=stats.get)\n",
    "    new_token = max_tokens_per_byte + i\n",
    "    print(f'Merging pair {top_pair} into new token {new_token}')\n",
    "    trainable_tokens = merge(trainable_tokens, top_pair, new_token)\n",
    "    bpe_forest[top_pair] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens before BPE:  105\n",
      "Number of unique tokens after BPE:  113\n",
      "Compression rate: 1.08X\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens before BPE: ', len(unique_tokens))\n",
    "print('Number of unique tokens after BPE: ', len(set(trainable_tokens)))\n",
    "print(f'Compression rate: {len(set(trainable_tokens)) / len(unique_tokens):.2f}X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Decoding tokens into strings\n",
    "\n",
    "UTF-8 follows a specific schema that bytes can take, which is used to encode and decode strings. Per this schema, a multi-byte character must follow certain rules as to how each byte is structured (see section 1 above). In order to avoid running into errors, the binary decode function can take a `errors` argument, which can be set to `replace`, which replaces any byte that cannot be decoded to a Unicode character with a question mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocabulary:  276\n"
     ]
    }
   ],
   "source": [
    "vocab = {i: bytes([i]) for i in range(max_tokens_per_byte)}\n",
    "for (i, j), new_token in bpe_forest.items():\n",
    "    vocab[new_token] = vocab[i] + vocab[j]\n",
    "print('Number of tokens in the vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded token:  a\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens: list[int]) -> str:\n",
    "    \"\"\"Decodes the given list of tokens using the given vocabulary.\"\"\"\n",
    "    binary = b''.join(vocab[token] for token in tokens)\n",
    "    return binary.decode('utf-8', errors='replace')\n",
    "\n",
    "print('Decoded token: ', decode([97]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Encoding strings into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text:  26\n"
     ]
    }
   ],
   "source": [
    "text = 'hello software engineering'\n",
    "print('Length of the text: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens:  [104, 101, 108, 108, 111, 32, 115, 111, 102, 116, 119, 97, 114, 256, 101, 110, 103, 267, 101, 101, 114, 267, 103]\n",
      "Length of the encoded tokens:  23\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Encodes the given text using the given vocabulary.\"\"\"\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "\n",
    "    while len(tokens) > 1:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: bpe_forest.get(p, float('inf')))\n",
    "        if pair not in bpe_forest:\n",
    "            break\n",
    "          \n",
    "        new_token = bpe_forest[pair]\n",
    "        tokens = merge(tokens, pair, new_token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print('Encoded tokens: ', encode(text))\n",
    "print('Length of the encoded tokens: ', len(encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:  hello software engineering\n"
     ]
    }
   ],
   "source": [
    "print('Decoded text: ', decode(encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regex patterns to force splits across categories\n",
    "\n",
    "This section is based on the following excerpt from the GPT-2 paper: \"We observed BPE including many versions of common words like `dog` since they occur in many variations such as `dog.`, `dog!` and `dog?`. This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence\". \n",
    "\n",
    "In order to prevent BPE from merging across character categories, regex patterns are used to force splits across categories and then tokenization can be performed on the resulting splits. In the end, the results of that processing are concatenated back together. This way, byte-pair merges can only happen within the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", ' aren', \"'t\", ' they', \"'re\", ' they', \"'ve\", ' I', \"'m\", ' I', \"'ll\", ' He', \"'d\", '       ', ' Hello', '123', ' World', '!?!?']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(pattern, \"It's aren't they're they've I'm I'll He'd        Hello123 World!?!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `Tiktoken` library intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 encoding:  [220, 220, 220, 23748, 995, 10185]\n",
      "GPT-4 encoding:  [262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "text = '    hello world!!!'\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "gpt2_encoding = tiktoken.get_encoding('gpt2')\n",
    "print('GPT-2 encoding: ', gpt2_encoding.encode(text))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "gpt4_encoding = tiktoken.get_encoding('cl100k_base')\n",
    "print('GPT-4 encoding: ', gpt4_encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT-2 `encoder.py` walkthrough\n",
    "\n",
    "References: \n",
    "- Code repository: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "- Vocabulary: https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
    "- BPE merges: https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocab:  50257\n"
     ]
    }
   ],
   "source": [
    "with open('../data/encoder.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print('Number of tokens in the vocab: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BPE merges:  50000\n",
      "First 10 BPE merges:  [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e'), ('o', 'n'), ('Ġt', 'he'), ('e', 'r'), ('Ġ', 's'), ('a', 't')]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/vocab.bpe', 'r', encoding='utf-8') as f:\n",
    "    bpe_data = f.read()\n",
    "    \n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "print('Number of BPE merges: ', len(bpe_merges))\n",
    "print('First 10 BPE merges: ', bpe_merges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted code from original at https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab, bpe_merges, errors='replace'):\n",
    "        self.vocab = vocab\n",
    "        self.decoder = {v:k for k,v in self.vocab.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def get_pairs(word):\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "\n",
    "    def bpe(self, token):\n",
    "        word = tuple(token)\n",
    "        pairs = self.get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self.get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.vocab[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = Tokenizer(vocab=vocab, bpe_merges=bpe_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tokenizer' object has no attribute 'byte_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhello world\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mTokenizer.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     58\u001b[39m bpe_tokens = []\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     token = \u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbyte_encoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     bpe_tokens.extend(\u001b[38;5;28mself\u001b[39m.vocab[bpe_token] \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe(token).split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     58\u001b[39m bpe_tokens = []\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     token = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbyte_encoder\u001b[49m[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     61\u001b[39m     bpe_tokens.extend(\u001b[38;5;28mself\u001b[39m.vocab[bpe_token] \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe(token).split(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tokenizer' object has no attribute 'byte_encoder'"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer.encode('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. special tokens, tiktoken handling of, GPT-2/GPT-4 differences\n",
    "# 18. minbpe exercise time! write your own GPT-4 tokenizer\n",
    "# 19. sentencepiece library intro, used to train Llama 2 vocabulary\n",
    "# 20. how to set vocabulary set? revisiting gpt.py transformer\n",
    "# 21. training new tokens, example of prompt compression\n",
    "# 22. multimodal [image, video, audio] tokenization with vector quantization\n",
    "# 23. revisiting and explaining the quirks of LLM tokenization\n",
    "# 24. final recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Let's build the GPT Tokenizer, by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE&t=38s)\n",
    "2. [A programmer's introduction to Unicode, by Nathan Reed](https://www.reedbeta.com/blog/programmers-intro-to-unicode)\n",
    "3. [Language models are unsupervised multitask learners [GPT-2 paper], by Alec Radford; Dario Amodei; Ilya Sutskever; et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
