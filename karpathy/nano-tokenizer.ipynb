{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import regex as re\n",
    "\n",
    "import sentencepiece as spm\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"Much glory awaits someone who can delete the need for tokenization\" -- (Andrej Karpathy)**\n",
    "\n",
    "The tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which the vocabulary is trained using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequence of tokens. The LLM only ever sees the tokens and never directly deals with any text.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/tokenizer-llm-diagram.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Strings in Python\n",
    "\n",
    "According to Python's documentation, \"strings are immutable *sequences* of *Unicode code points*\". The function to access the Unicode code point of a character is `ord()`. The function to access the character of a Unicode code point is `chr()`. Also, Unicode text is processed and stored as binary data *using one of several encodings*: `UTF-8`, `UTF-16`, `UTF-32`, among others. Of these, `UTF-8` is the most widely used, in part due to its backwards-compatibility with ASCII. The function to encode a string into a binary data is `encode()`. The function to decode a binary data into a string is `decode()`.\n",
    "\n",
    "`UTF-8` means *Unicode Transformation Format - 8 bit* and supports all valid Unicode code points using a *variable-width encoding* of one to four one-byte code units. Code points with lower numerical values, which tend to occur more frequently, are encoded using fewer bytes. In the following table, the characters `u` to `z` are replaced by the bits of the code point, from the positions U+uvwxyz:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/utf8-encoding.jpg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Examples:\n",
    "- U+0041 (‘A’) → 01000001 → 01000001 (same as ASCII)\n",
    "- U+00A9 (‘©’)\t→ 1010001001 → 11010100 10010001\n",
    "\n",
    "Now, considering that `UTF-8` is represented as byte streams, it implies a maximum vocabulary length of 256 possible tokens. This means tiny embedding tables, counterweighted by very long sequences of tokens, which can be a hindrance to context length in transformer-based neural networks, where each token needs to attend to all other tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode length:  5\n",
      "Unicode representation:  [50504, 45397, 54616, 49464, 50836]\n"
     ]
    }
   ],
   "source": [
    "unicode_enc = [ord(x) for x in '안녕하세요']\n",
    "print('Unicode length: ', len(unicode_enc))\n",
    "print('Unicode representation: ', unicode_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 length:  15\n",
      "UTF-8 representation:  [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148]\n",
      "UTF-8 byte string:  b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94'\n"
     ]
    }
   ],
   "source": [
    "utf8_enc = '안녕하세요'.encode('utf-8')\n",
    "print('UTF-8 length: ', len(utf8_enc))\n",
    "print('UTF-8 representation: ', list(utf8_enc))\n",
    "print('UTF-8 byte string: ', utf8_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Byte Pair Encoding (BPE)\n",
    "\n",
    "This algorithm was first described in 1994, by Philip Gage, for encoding strings of text into smaller strings by creating and using a translation table. It builds \"tokens\" (units of recognition) that match varying amounts of source text, from single characters (including single digits or single punctuation marks) to whole words (even long compound words).\n",
    "\n",
    "Suppose the data to be encoded is:\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "The byte pair \"aa\" occurs most often, so it is merged into a single token:\n",
    "\n",
    "```\n",
    "ZabdZabac\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "The process is repeated with byte pair \"ab\", replacing it with Y:\n",
    "\n",
    "```\n",
    "ZYdZYac\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "Finally, the byte pair \"ZY\" is merged into a single token X:\n",
    "\n",
    "```\n",
    "XdXac\n",
    "X = ZY\n",
    "Y = ab\n",
    "Z = aa\n",
    "```\n",
    "\n",
    "The data cannot be compressed further because there are no pairs of bytes that occur more than once. We started with 11 bytes and 4 tokens, and ended with 5 bytes and 6 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text:  1414\n"
     ]
    }
   ],
   "source": [
    "with open('../data/unicode.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Number of characters in the text: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single tokens in the text:  2058\n"
     ]
    }
   ],
   "source": [
    "# NOTE: unicode text encoded in utf-8 has up to 4 bytes per character\n",
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print('Number of single tokens in the text: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the text:  105\n",
      "Max token:  240\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set(tokens)\n",
    "print('Number of unique tokens in the text: ', len(unique_tokens))\n",
    "print('Max token: ', max(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams:  617\n",
      "Most common bigrams:  [((101, 32), 24), ((204, 173), 18), ((205, 153), 18), ((204, 178), 18), ((115, 32), 17)]\n",
      "Most common bigram in text:  ('e', ' ')\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print('Number of unique bigrams: ', len(stats))\n",
    "print('Most common bigrams: ', sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "print('Most common bigram in text: ', (chr(101), chr(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the most common pair\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokens before merging:  [5, 6, 6, 7, 9, 1]\n",
      "Example tokens after merging:  [5, 10, 7, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "def merge(tokens: list, pair: tuple[int, int], new_token: int) -> list:\n",
    "    \"\"\"Merges the most common pair in the given list of tokens into a single token.\"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "print('Example tokens before merging: ', ex_tokens := [5, 6, 6, 7, 9, 1])\n",
    "print('Example tokens after merging: ', merge(ex_tokens, (6, 6), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens before merging:  2058\n",
      "Number of tokens after merging:  2034\n",
      "Number of unique tokens before merging:  105\n",
      "Number of unique tokens after merging:  106\n",
      "Max token before merging:  240\n",
      "Max token after merging:  241\n"
     ]
    }
   ],
   "source": [
    "merged_tokens = merge(tokens, top_pair, max(unique_tokens) + 1)\n",
    "print('Number of tokens before merging: ', len(tokens))\n",
    "print('Number of tokens after merging: ', len(merged_tokens))\n",
    "print('Number of unique tokens before merging: ', len(unique_tokens))\n",
    "print('Number of unique tokens after merging: ', len(set(merged_tokens)))\n",
    "print('Max token before merging: ', max(unique_tokens))\n",
    "print('Max token after merging: ', max(set(merged_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Training the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 276                 # desired number of unique tokens in vocabulary\n",
    "max_tokens_per_byte = 2 ** 8     # encoding string into utf-8 converts characters into bytes\n",
    "num_merges = vocab_size - max_tokens_per_byte\n",
    "trainable_tokens = copy.deepcopy(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging pair (101, 32) into new token 256\n",
      "Merging pair (204, 173) into new token 257\n",
      "Merging pair (205, 153) into new token 258\n",
      "Merging pair (204, 178) into new token 259\n",
      "Merging pair (115, 32) into new token 260\n",
      "Merging pair (204, 171) into new token 261\n",
      "Merging pair (204, 177) into new token 262\n",
      "Merging pair (240, 159) into new token 263\n",
      "Merging pair (205, 136) into new token 264\n",
      "Merging pair (204, 185) into new token 265\n",
      "Merging pair (226, 128) into new token 266\n",
      "Merging pair (105, 110) into new token 267\n",
      "Merging pair (205, 150) into new token 268\n",
      "Merging pair (204, 187) into new token 269\n",
      "Merging pair (205, 135) into new token 270\n",
      "Merging pair (204, 188) into new token 271\n",
      "Merging pair (204, 164) into new token 272\n",
      "Merging pair (204, 166) into new token 273\n",
      "Merging pair (97, 110) into new token 274\n",
      "Merging pair (204, 176) into new token 275\n"
     ]
    }
   ],
   "source": [
    "# `bpe_forest` is an inverted tree that stores merges: (int, int) -> int\n",
    "bpe_forest = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(trainable_tokens)\n",
    "    top_pair = max(stats, key=stats.get)\n",
    "    new_token = max_tokens_per_byte + i\n",
    "    print(f'Merging pair {top_pair} into new token {new_token}')\n",
    "    trainable_tokens = merge(trainable_tokens, top_pair, new_token)\n",
    "    bpe_forest[top_pair] = new_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens before BPE:  105\n",
      "Number of unique tokens after BPE:  113\n",
      "Compression rate: 1.08X\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens before BPE: ', len(unique_tokens))\n",
    "print('Number of unique tokens after BPE: ', len(set(trainable_tokens)))\n",
    "print(f'Compression rate: {len(set(trainable_tokens)) / len(unique_tokens):.2f}X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Decoding tokens into strings\n",
    "\n",
    "UTF-8 follows a specific schema that bytes can take, which is used to encode and decode strings. Per this schema, a multi-byte character must follow certain rules as to how each byte is structured (see section 1 above). In order to avoid running into errors, the binary decode function can take a `errors` argument, which can be set to `replace`, which replaces any byte that cannot be decoded to a Unicode character with a question mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocabulary:  276\n"
     ]
    }
   ],
   "source": [
    "vocab = {i: bytes([i]) for i in range(max_tokens_per_byte)}\n",
    "for (i, j), new_token in bpe_forest.items():\n",
    "    vocab[new_token] = vocab[i] + vocab[j]\n",
    "print('Number of tokens in the vocabulary: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded token:  a\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens: list[int]) -> str:\n",
    "    \"\"\"Decodes the given list of tokens using the given vocabulary.\"\"\"\n",
    "    binary = b''.join(vocab[token] for token in tokens)\n",
    "    return binary.decode('utf-8', errors='replace')\n",
    "\n",
    "print('Decoded token: ', decode([97]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Encoding strings into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text:  26\n"
     ]
    }
   ],
   "source": [
    "text = 'hello software engineering'\n",
    "print('Length of the text: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens:  [104, 101, 108, 108, 111, 32, 115, 111, 102, 116, 119, 97, 114, 256, 101, 110, 103, 267, 101, 101, 114, 267, 103]\n",
      "Length of the encoded tokens:  23\n"
     ]
    }
   ],
   "source": [
    "def encode(text: str) -> list[int]:\n",
    "    \"\"\"Encodes the given text using the given vocabulary.\"\"\"\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "\n",
    "    while len(tokens) > 1:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: bpe_forest.get(p, float('inf')))\n",
    "        if pair not in bpe_forest:\n",
    "            break\n",
    "          \n",
    "        new_token = bpe_forest[pair]\n",
    "        tokens = merge(tokens, pair, new_token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print('Encoded tokens: ', encode(text))\n",
    "print('Length of the encoded tokens: ', len(encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:  hello software engineering\n"
     ]
    }
   ],
   "source": [
    "print('Decoded text: ', decode(encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regex patterns to force splits across categories\n",
    "\n",
    "This section is based on the following excerpt from the GPT-2 paper: \"We observed BPE including many versions of common words like `dog` since they occur in many variations such as `dog.`, `dog!` and `dog?`. This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence\". \n",
    "\n",
    "In order to prevent BPE from merging across character categories, regex patterns are used to force splits across categories and then tokenization can be performed on the resulting splits. In the end, the results of that processing are concatenated back together. This way, byte-pair merges can only happen within the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', \"'s\", ' aren', \"'t\", ' they', \"'re\", ' they', \"'ve\", ' I', \"'m\", ' I', \"'ll\", ' He', \"'d\", '       ', ' Hello', '123', ' World', '!?!?']\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(pattern, \"It's aren't they're they've I'm I'll He'd        Hello123 World!?!?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `Tiktoken` library intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 encoding:  [220, 220, 220, 23748, 995, 10185]\n",
      "GPT-4 encoding:  [262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "text = '    hello world!!!'\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "gpt2_encoding = tiktoken.get_encoding('gpt2')\n",
    "print('GPT-2 encoding: ', gpt2_encoding.encode(text))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "gpt4_encoding = tiktoken.get_encoding('cl100k_base')\n",
    "print('GPT-4 encoding: ', gpt4_encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT-2 `encoder.py` walkthrough\n",
    "\n",
    "References: \n",
    "- Code repository: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "- Vocabulary: https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
    "- BPE merges: https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocab:  50257\n",
      "First 10 tokens in the vocab:  [('!', 0), ('\"', 1), ('#', 2), ('$', 3), ('%', 4), ('&', 5), (\"'\", 6), ('(', 7), (')', 8), ('*', 9)]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/encoder.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "print('Number of tokens in the vocab: ', len(vocab))\n",
    "print('First 10 tokens in the vocab: ', list(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of BPE merges:  50000\n",
      "First 10 BPE merges:  [('Ġ', 't'), ('Ġ', 'a'), ('h', 'e'), ('i', 'n'), ('r', 'e'), ('o', 'n'), ('Ġt', 'he'), ('e', 'r'), ('Ġ', 's'), ('a', 't')]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/vocab.bpe', 'r', encoding='utf-8') as f:\n",
    "    bpe_data = f.read()\n",
    "    \n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "print('Number of BPE merges: ', len(bpe_merges))\n",
    "print('First 10 BPE merges: ', bpe_merges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 bytes to unicode mapping:  [(33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*')]\n"
     ]
    }
   ],
   "source": [
    "# Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "# The reversible bpe codes work on unicode strings.\n",
    "# This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "# When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "# This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "# To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "# And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "def bytes_to_unicode():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "print('First 10 bytes to unicode mapping: ', list(bytes_to_unicode().items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs in the word \"hello\":  {('e', 'l'), ('h', 'e'), ('l', 'o'), ('l', 'l')}\n"
     ]
    }
   ],
   "source": [
    "def get_pairs(word):\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "print('Pairs in the word \"hello\": ', get_pairs('hello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  hello world\n",
      "BPE tokens:  [31373, 995]\n",
      "Decoded text:  hello world\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = Tokenizer(encoder=vocab, bpe_merges=bpe_merges)\n",
    "bpe_tokens = gpt2_tokenizer.encode('hello world')\n",
    "print('Original text: ', 'hello world')\n",
    "print('BPE tokens: ', bpe_tokens)\n",
    "text = gpt2_tokenizer.decode(bpe_tokens)\n",
    "print('Decoded text: ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Special tokens\n",
    "\n",
    "In addition to tokens that are coming from raw bytes and BPE merges, there are also special tokens that are added to the vocabulary to delimit different parts of the data or to create special structures onto the token streams. For example, OpenAI GPT-2's vocabulary is composed of 50257 tokens:\n",
    "\n",
    "- 256 tokens from raw bytes\n",
    "- 50000 tokens from BPE merges\n",
    "- 1 special token: `<|endoftext|>`\n",
    "\n",
    "This special token is used to delimit documents in the training set to signal to the language model that the document has ended and what follows is unrelated to the previous document.\n",
    "\n",
    "Special tokens can be arbitrarily registered in the vocabulary and they are used pervasively in the fine-tuning stage, for instance, to delimit the conversation turns in a dialogue dataset. In order to do that, some model *surgery* is required in the parameters of the transformer, to make sure that the special tokens are properly handled, ie., the embedding matrix has to be extended to add the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OpenAI GPT-2's BPE merges:  50000\n",
      "Number of tokens in OpenAI GPT-2's vocab:  50257\n",
      "Special token <|endoftext|> in the vocab:  50256\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of OpenAI GPT-2's BPE merges: \", len(bpe_merges))\n",
    "print(\"Number of tokens in OpenAI GPT-2's vocab: \", len(vocab))\n",
    "print('Special token <|endoftext|> in the vocab: ', vocab['<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. `Sentencepiece` library intro\n",
    "\n",
    "Commonly used because (unlike **Tiktoken**) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "**Sentencepiece** runs BPE on the Unicode code points directly. It then has as optional `character_coverage` for what to do with rare codepoints, either mapping them onto an `UNK` token, or if `byte_fallback` is turned on, encoding them with UTF-8 raw bytes. It also has *normalization* configurations, such as all lowercase, remove double-whitespaces, etc. It used to be very commom before LLMs in natural language processing, but now are less used because the LLMs can learn to normalize text by themselves.\n",
    "\n",
    "Below is a list of configurations, since it implements a large diversity of tokenization algorithms:\n",
    "\n",
    "- [Training options](https://github.com/google/sentencepiece/blob/master/doc/options.md)\n",
    "- [Protocol bufffer used to represent TrainerSpec](https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/unicode.txt\n",
      "  input_format: text\n",
      "  model_prefix: nano-tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 1000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 4\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../data/unicode.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 2 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1412\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=132\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 113\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=1\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: nano-tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: nano-tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# settings are (best effort) those used for training Llama 2\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input='../data/unicode.txt',\n",
    "  input_format='text',\n",
    "  # output spec\n",
    "  model_prefix='nano-tokenizer', # output filename prefix\n",
    "  # algorithm spec -- BPE\n",
    "  model_type='bpe',\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name='identity', # no normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=1e6,\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1e6,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare codepoint treatment\n",
    "  character_coverage=0.99995, # if token appears only once every 20k words, it's treated as rare and replaced with UNK\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True, # 'world' and 'hello world' are both mapped to '_world'\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special_tokens\n",
    "  unk_id=0,  # UNK token must exist\n",
    "  bos_id=1,  # optional BOS token - set to -1 if not needed\n",
    "  eos_id=2,  # optional EOS token - set to -1 if not needed\n",
    "  pad_id=-1, # optional PAD token - set to -1 if not needed\n",
    "  # system spec\n",
    "  num_threads=os.cpu_count() / 2, # use half of the available cores\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocab:  400\n",
      "First 5 tokens in the vocab:  [['<unk>', 0], ['<s>', 1], ['</s>', 2], ['<0x00>', 3], ['<0x01>', 4]]\n",
      "Last 5 tokens in the vocab:  [['🇮', 395], ['🇳', 396], ['🇴', 397], ['🇺', 398], ['😄', 399]]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('nano-tokenizer.model')\n",
    "vocab = [[sp.id_to_piece(i), i] for i in range(sp.get_piece_size())]\n",
    "print('Number of tokens in the vocab: ', len(vocab))\n",
    "print('First 5 tokens in the vocab: ', vocab[:5])\n",
    "print('Last 5 tokens in the vocab: ', vocab[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 length:  15\n",
      "UTF-8 representation:  [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148]\n",
      "UTF-8 byte string:  b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94'\n"
     ]
    }
   ],
   "source": [
    "# NOTE: SentencePiece has not seen 안녕하세요 codepoints during training\n",
    "#       Because `byte_fallback=True`, it will fallback to bytes\n",
    "utf8_enc = '안녕하세요'.encode('utf-8')\n",
    "print('UTF-8 length: ', len(utf8_enc))\n",
    "print('UTF-8 representation: ', list(utf8_enc))\n",
    "print('UTF-8 byte string: ', utf8_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded ids:  [268, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151, 268, 299, 265, 280, 277]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode('안녕하세요 world')\n",
    "print('Encoded ids: ', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148],\n",
       " [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_special_tokens_used = 3\n",
    "adjusted_encoded_ids = [x - 3 for x in ids[1:len(utf8_enc)+1]]\n",
    "list(utf8_enc), adjusted_encoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>', '▁', 'w', 'or', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "print('Tokens: ', [sp.id_to_piece(i) for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:  안녕하세요 world\n"
     ]
    }
   ],
   "source": [
    "text = sp.decode(ids)\n",
    "print('Decoded text: ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded ids for \"world\":  [268, 299, 265, 280, 277]\n",
      "Tokens for \"world\":  ['▁', 'w', 'or', 'l', 'd']\n",
      "Encoded ids for \"hello world\":  [268, 278, 269, 280, 280, 274, 268, 299, 265, 280, 277]\n",
      "Tokens for \"hello world\":  ['▁', 'h', 'e', 'l', 'l', 'o', '▁', 'w', 'or', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "# NOTE: demo of `add_dummy_prefix=True` option\n",
    "ids = sp.encode('world')\n",
    "print('Encoded ids for \"world\": ', ids)\n",
    "print('Tokens for \"world\": ', [sp.id_to_piece(i) for i in ids])\n",
    "\n",
    "ids = sp.encode('hello world')\n",
    "print('Encoded ids for \"hello world\": ', ids)\n",
    "print('Tokens for \"hello world\": ', [sp.id_to_piece(i) for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ../data/unicode.txt\n",
      "  input_format: text\n",
      "  model_prefix: nano-tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 1000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 4\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: ../data/unicode.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 2 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1412\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=132\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 2 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 2\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 113\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=20 all=1069 active=937 piece=▁f\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=40 all=1115 active=983 piece=le\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=1158 active=1026 piece=ft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=80 all=1200 active=1068 piece=̝͙\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=100 all=1257 active=1125 piece=͉̝\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=120 all=1289 active=1030 piece=yst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=140 all=1298 active=1039 piece=Za\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=160 all=1312 active=1053 piece=ę\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=180 all=1330 active=1071 piece=ḻ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=200 all=1349 active=1090 piece=ru\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=220 all=1367 active=1017 piece=̖͔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=240 all=1386 active=1036 piece=̞͔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=260 all=1404 active=1054 piece=̤̬\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: nano-tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: nano-tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# NOTE: demo of `byte_fallback=False` option\n",
    "options['byte_fallback'] = False\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded ids:  [268, 0, 132, 280, 277]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('nano-tokenizer.model')\n",
    "\n",
    "ids = sp.encode('안녕하세요 world')\n",
    "print('Encoded ids: ', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['▁', '<unk>', '▁wor', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "print('Tokens: ', [sp.id_to_piece(i) for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text:   ⁇  world\n"
     ]
    }
   ],
   "source": [
    "text = sp.decode(ids)\n",
    "print('Decoded text: ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Let's build the GPT Tokenizer, by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE&t=38s)\n",
    "2. [A programmer's introduction to Unicode, by Nathan Reed](https://www.reedbeta.com/blog/programmers-intro-to-unicode)\n",
    "3. [Language models are unsupervised multitask learners [GPT-2 paper], by Alec Radford; Dario Amodei; Ilya Sutskever; et al.](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "4. [Efficient Training of Language Models to Fill in the Middle, by Bavarian, M; et al.](https://arxiv.org/abs/2207.14255)\n",
    "5. [Learning to Compress Prompts with Gist Tokens, by Mu, J; et al.](https://arxiv.org/pdf/2304.08467)\n",
    "6. [Taming Transformers for High-Resolution Image Synthesis, by Esser, P; et al.](https://compvis.github.io/taming-transformers)\n",
    "7. [Integer tokenization is insane, by Beren Millidge](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/)\n",
    "8. [SolidGoldMagikarp (plus, prompt generation), by Jessica Rumbelow](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
