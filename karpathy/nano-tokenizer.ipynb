{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"Much glory awaits someone who can delete the need for tokenization\" -- (Andrej Karpathy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Strings in Python\n",
    "\n",
    "According to Python's documentation, \"strings are immutable *sequences* of *Unicode code points*\". The function to access the Unicode code point of a character is `ord()`. The function to access the character of a Unicode code point is `chr()`. Also, Unicode text is processed and stored as binary data *using one of several encodings*: `UTF-8`, `UTF-16`, `UTF-32`, among others. Of these, `UTF-8` is the most widely used, in part due to its backwards-compatibility with ASCII. The function to encode a string into a binary data is `encode()`. The function to decode a binary data into a string is `decode()`.\n",
    "\n",
    "`UTF-8` means *Unicode Transformation Format - 8 bit* and supports all valid Unicode code points using a *variable-width encoding* of one to four one-byte code units. Code points with lower numerical values, which tend to occur more frequently, are encoded using fewer bytes. In the following table, the characters `u` to `z` are replaced by the bits of the code point, from the positions U+uvwxyz:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/utf8-encoding.jpg\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Examples:\n",
    "- U+0041 (‘A’) → 01000001 → 01000001 (same as ASCII)\n",
    "- U+00A9 (‘©’)\t→ 1010001001 → 11010100 10010001\n",
    "\n",
    "Now, considering that `UTF-8` is represented as byte streams, it implies a maximum vocabulary length of 256 possible tokens. This means tiny embedding tables, counterweighted by very long sequences of tokens, which can be a hindrance to context length in transformer-based neural networks, where each tokens needs to attend to all other tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504, 45397, 54616, 49464, 50836]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_enc = [ord(x) for x in '안녕하세요']\n",
    "unicode_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94',\n",
       " [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_enc = '안녕하세요'.encode('utf-8')\n",
    "utf8_enc, list(utf8_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode length:  5\n",
      "UTF-8 length:  15\n"
     ]
    }
   ],
   "source": [
    "print('Unicode length: ', len(unicode_enc))\n",
    "print('UTF-8 length: ', len(utf8_enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Byte Pair Encoding (BPE)\n",
    "\n",
    "This algorithm was first described in 1994, by Philip Gage, for encoding strings of text into smaller strings by creating and using a translation table. It builds \"tokens\" (units of recognition) that match varying amounts of source text, from single characters (including single digits or single punctuation marks) to whole words (even long compound words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text:  1414\n"
     ]
    }
   ],
   "source": [
    "with open('../data/unicode.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Number of characters in the text: ', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single tokens in the text:  2058\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(int, text.encode('utf-8')))\n",
    "print('Number of single tokens in the text: ', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique bigrams:  617\n",
      "Most common bigrams:  [((101, 32), 24), ((204, 173), 18), ((205, 153), 18), ((204, 178), 18), ((115, 32), 17)]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print('Number of unique bigrams: ', len(stats))\n",
    "print('Most common bigrams: ', sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging the most common pair\n",
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. training the tokenizer: adding the while loop, compression ratio\n",
    "# 11. tokenizer/LLM diagram: it is a completely separate stage\n",
    "# 12. decoding tokens to strings\n",
    "# 13. encoding strings to tokens\n",
    "# 14. regex patterns to force splits across categories\n",
    "# 15. tiktoken library intro, differences between GPT-2/GPT-4 regex\n",
    "# 16. GPT-2 encoder.py released by OpenAI walkthrough\n",
    "# 17. special tokens, tiktoken handling of, GPT-2/GPT-4 differences\n",
    "# 18. minbpe exercise time! write your own GPT-4 tokenizer\n",
    "# 19. sentencepiece library intro, used to train Llama 2 vocabulary\n",
    "# 20. how to set vocabulary set? revisiting gpt.py transformer\n",
    "# 21. training new tokens, example of prompt compression\n",
    "# 22. multimodal [image, video, audio] tokenization with vector quantization\n",
    "# 23. revisiting and explaining the quirks of LLM tokenization\n",
    "# 24. final recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Let's build the GPT Tokenizer, by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE&t=38s)\n",
    "2. [A programmer's introduction to Unicode, by Nathan Reed](https://www.reedbeta.com/blog/programmers-intro-to-unicode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
