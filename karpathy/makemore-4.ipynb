{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "names = json.loads(open(\"../data/names.txt\", \"r\").read())\n",
    "names = names[\"payload\"][\"blob\"][\"rawLines\"]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  32033\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset: \", len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chtoi:  {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "itoch:  {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "vocab_size:  27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "chtoi = {ch:i+1 for i, ch in enumerate(chars)}\n",
    "chtoi[\".\"] = 0\n",
    "itoch = {i:ch for ch, i in chtoi.items()}\n",
    "vocab_size = len(chtoi)\n",
    "print(\"chtoi: \", chtoi)\n",
    "print(\"itoch: \", itoch)\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr.shape: torch.Size([182413, 3]), ytr.shape: torch.Size([182413])\n",
      "Xval.shape: torch.Size([22822, 3]), yval.shape: torch.Size([22822])\n",
      "Xte.shape: torch.Size([22911, 3]), yte.shape: torch.Size([22911])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_dataset(data, mode=\"tr\", context_len=3):\n",
    "  \"\"\"\n",
    "  Builds the dataset from the given list of words.\n",
    "  \n",
    "  Args:\n",
    "    data (list): list of words to use\n",
    "    mode (str): mode of the dataset (default: \"tr\" for training)\n",
    "    context_len (int): length of the context window (default: 3)\n",
    "  \"\"\"\n",
    "  \n",
    "  X, y = [], []\n",
    "  \n",
    "  for word in data:\n",
    "    context = [0] * context_len\n",
    "    for ch in word + \".\":\n",
    "      ix = chtoi[ch]\n",
    "      X.append(context)\n",
    "      y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "  \n",
    "  X = torch.tensor(X)\n",
    "  y = torch.tensor(y)\n",
    "  \n",
    "  print(f\"X{mode}.shape: {X.shape}, y{mode}.shape: {y.shape}\")\n",
    "  return X, y\n",
    "\n",
    "\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "Xtr, ytr = build_dataset(names[:n1])\n",
    "Xval, yval = build_dataset(names[n1:n2], mode=\"val\")\n",
    "Xte, yte = build_dataset(names[n2:], mode=\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  \"\"\"\n",
    "  Utility function to compare manual and PyTorch gradients\n",
    "\n",
    "  Args:\n",
    "    s (str): name of the tensor\n",
    "    dt (torch.Tensor): manual gradient\n",
    "    t (torch.Tensor): PyTorch gradient\n",
    "  \"\"\"\n",
    "  \n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  max_diff = (dt - t.grad).abs().max().item()\n",
    "  print(f\"{s:15s} | exact: {str(ex):5s} | approx: {str(app):5s} | max_diff: {max_diff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4137"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_embd = 10         # dimensionality of character embeddings vector\n",
    "n_hidden = 64       # number of neurons in the hidden layer\n",
    "context_len = 3     # length of the context window\n",
    "gain = 5.0 / 3.0    # gain for the Kaiming initialization\n",
    "\n",
    "\"\"\"\n",
    "Parameters are initialized in non-standard ways because initializing them with\n",
    "all zeroes could mask an incorrect implementation of the backpropagation.\n",
    "\n",
    "Using b1 despite batch normalization because it's easier to compare the gradients.\n",
    "\"\"\"\n",
    "\n",
    "C =  torch.randn(vocab_size, n_embd,             requires_grad=True)\n",
    "# Layer 1\n",
    "W1 = torch.randn(n_embd * context_len, n_hidden, requires_grad=True) * gain / math.sqrt(n_embd * context_len)\n",
    "b1 = torch.randn(n_hidden,                       requires_grad=True) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn(n_hidden, vocab_size,           requires_grad=True) * 0.1\n",
    "b2 = torch.randn(vocab_size,                     requires_grad=True) * 0.1\n",
    "# BatchNorm params\n",
    "gamma = torch.randn(1, n_hidden,                 requires_grad=True) * 0.1 + 1.0\n",
    "beta =  torch.randn(1, n_hidden,                 requires_grad=True) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, gamma, beta]\n",
    "sum(p.nelement() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# construct a mini-batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "Xb, yb = Xtr[ix], ytr[ix]     # batch X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5030, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manual implementation of forward pass\n",
    "emb = C[Xb]                                                  # embedding characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)                          # concatenate embeddings\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1                                    # pre-batchnorm hidden layer\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = hprebn.mean(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiffsq = bndiff ** 2\n",
    "bnvar = bndiffsq.sum(0, keepdim=True) / (batch_size - 1)     # Bessel's correction (N-1)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = gamma * bnraw + beta\n",
    "\n",
    "# non-linearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# Linear layer 2 (explicit implementation of cross entropy loss)\n",
    "# NOTE check source @ 21:01, 36:20\n",
    "logits = h @ W2 + b2                                         # output layer\n",
    "logits_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logits_maxes                          # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1                            # (1.0 / counts_sum) cannot get bit-exact backpropagation\n",
    "probs = counts * counts_sum_inv\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(batch_size), yb].mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5030, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch backward pass\n",
    "training_tensors = [log_probs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logits_maxes, logits, h, hpreact, bnraw, bnvar_inv, bnvar, bndiffsq, bndiff, bnmeani, hprebn, embcat, emb]\n",
    "\n",
    "for t in training_tensors:\n",
    "  t.retain_grad()\n",
    "\n",
    "for p in params:\n",
    "  p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic compute graph backpropagation\n",
    "\n",
    "Implement backpropagation manually for the entire forward pass compute graph, using atomic operations as building blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogprobs | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of log_probs\n",
    "\n",
    "derivation of log_probs in relation to loss: loss = -log_probs[range(batch_size), yb].mean()\n",
    "loss = - (a + b + c) / n\n",
    "loss = -a/n + -b/n + -c/n\n",
    "dloss_da = -1/n\n",
    "dloss_db = -1/n\n",
    "dloss_dc = -1/n\n",
    "\"\"\"\n",
    "dloss_dlogprobs = torch.zeros_like(log_probs)\n",
    "dloss_dlogprobs[range(batch_size), yb] = -1.0 / batch_size\n",
    "cmp(\"dloss_dlogprobs\", dloss_dlogprobs, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dprobs    | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of probs\n",
    "note: intuitively, a high probability character will be simply passed through the chain rule, while a low probability will be boosted to generate a high gradient.\n",
    "\n",
    "derivation of probs in relation to log_probs: log_probs = probs.log()\n",
    "log_probs = log(a), log(b), log(c)\n",
    "dlogprobs_da = 1/a\n",
    "dlogprobs_db = 1/b\n",
    "dlogprobs_dc = 1/c\n",
    "\n",
    "derivation of probs in relation to loss: dlogprobs_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dprobs = 1.0 / probs * dloss_dlogprobs\n",
    "cmp(\"dloss_dprobs\", dloss_dprobs, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcountssuminv | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts_sum_inv\n",
    "\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum_inv.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "a[3,2] * b[3,1] ---> c[3,2]\n",
    "a11 * b1, a12 * b1\n",
    "a21 * b2, a22 * b2\n",
    "a31 * b3, a32 * b3\n",
    "\n",
    "derivation of counts_sum_inv in relation to probs: probs = counts * counts_sum_inv\n",
    "probs = a * b\n",
    "dprobs_da = b\n",
    "dprobs_db = a\n",
    "\n",
    "derivation of counts_sum_inv in relation to loss: SUM of dprobs_dn * chain_rule backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dcountssuminv = (counts * dloss_dprobs).sum(1, keepdim=True)\n",
    "cmp(\"dloss_dcountssuminv\", dloss_dcountssuminv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcountssum | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts_sum\n",
    "\n",
    "derivation of counts_sum in relation to counts_sum_inv: counts_sum_inv = counts_sum ** -1\n",
    "counts_sum_inv = a ** -1\n",
    "dcountssum_da = - a ** -2\n",
    "\n",
    "derivation of counts_sum in relation to loss: dcountssum_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dcountssum = -counts_sum ** -2 * dloss_dcountssuminv\n",
    "cmp(\"dloss_dcountssum\", dloss_dcountssum, counts_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcounts   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts\n",
    "note: both probs and counts_sum depend on counts, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of counts in relation to probs: probs = counts * counts_sum_inv\n",
    "probs = a * b\n",
    "dprobs_da = b\n",
    "dprobs_db = a\n",
    "\n",
    "(2nd branch) derivation of counts in relation to counts_sum: counts_sum = counts.sum(1, keepdim=True)\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "sum(a[3,2]) ---> b[3,1]\n",
    "a11 + a12 ---> b1\n",
    "a21 + a22 ---> b2\n",
    "a31 + a32 ---> b3\n",
    "\n",
    "counts_sum = a + b + c\n",
    "dcountssum_da = 1\n",
    "dcountssum_db = 1\n",
    "dcountssum_dc = 1\n",
    "\n",
    "derivation of counts in relation to loss: dprobs_dn * chain_rule + dcountssum_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dcounts = counts_sum_inv * dloss_dprobs + torch.ones_like(counts) * dloss_dcountssum\n",
    "cmp(\"dloss_dcounts\", dloss_dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dnormlogits | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of norm_logits\n",
    "\n",
    "derivation of norm_logits in relation to counts: counts = norm_logits.exp()\n",
    "counts = e ** a\n",
    "dcounts_da = e ** a\n",
    "\n",
    "derivation of norm_logits in relation to loss: dcounts_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dnormlogits = counts * dloss_dcounts\n",
    "cmp(\"dloss_dnormlogits\", dloss_dnormlogits, norm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogitsmaxes | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of logits_maxes\n",
    "note: logits_maxes are calculated only for numerical stability of softmax function and do not affect the loss. Therefore, the gradient is zero (or close to it, due to floating point precision).\n",
    "\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum_inv.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "a[3,2] - b[3,1] ---> c[3,2]\n",
    "a11 - b1, a12 - b1\n",
    "a21 - b2, a22 - b2\n",
    "a31 - b3, a32 - b3\n",
    "\n",
    "derivation of logits_maxes in relation to norm_logits: norm_logits = logits - logits_maxes\n",
    "norm_logits = a + -b\n",
    "dnormlogits_da = 1\n",
    "dnormlogits_db = -1\n",
    "\n",
    "derivation of logits_maxes in relation to loss: SUM of dnormlogits_dn * chain_rule backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dlogitsmaxes = -dloss_dnormlogits.sum(1, keepdim=True)\n",
    "cmp(\"dloss_dlogitsmaxes\", dloss_dlogitsmaxes, logits_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogits   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of logits\n",
    "note: both norm_logits and logits_maxes depend on logits, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of logits in relation to norm_logits: check dloss_dlogitsmaxes\n",
    "\n",
    "(2nd branch) derivation of logits in relation to logits_maxes: logits_maxes = logits.max(1, keepdim=True).values\n",
    "logits_maxes = a\n",
    "dlogitsmaxes_da = 1\n",
    "\n",
    "derivation of logits in relation to loss: dnormlogits_dn * chain_rule + dlogitsmaxes_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dlogits = dloss_dnormlogits.clone() + F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dloss_dlogitsmaxes\n",
    "cmp(\"dloss_dlogits\", dloss_dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'One-hot encoding of the maximum logits')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))\n",
    "plt.xlabel(\"logits\")\n",
    "plt.ylabel(\"batch_size\")\n",
    "plt.title(\"One-hot encoding of the maximum logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Building makemore Part 4: Becoming a Backprop Ninja, By Andrej Karpathy](https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6s)\n",
    "2. [Yes, you should understand backprop, By Andrej Karpathy](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "3. [Bessel's Correction](https://math.oxford.emory.edu/site/math117/besselCorrection/)\n",
    "4. [Reducing the Dimensionality of Data with Neural Networks, By G. Hinton](https://www.cs.toronto.edu/~hinton/absps/science.pdf)\n",
    "5. Karpathy, Andrej; Li, Fei-Fei. Deep Fragment Embeddings for Bidirectional Image Sentence Mapping, 2014. URL: https://arxiv.org/pdf/1406.5679.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
