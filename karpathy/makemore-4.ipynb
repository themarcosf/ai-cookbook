{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "names = json.loads(open(\"../data/names.txt\", \"r\").read())\n",
    "names = names[\"payload\"][\"blob\"][\"rawLines\"]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset:  32033\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset: \", len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chtoi:  {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "itoch:  {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "vocab_size:  27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(\"\".join(names))))\n",
    "chtoi = {ch:i+1 for i, ch in enumerate(chars)}\n",
    "chtoi[\".\"] = 0\n",
    "itoch = {i:ch for ch, i in chtoi.items()}\n",
    "vocab_size = len(chtoi)\n",
    "print(\"chtoi: \", chtoi)\n",
    "print(\"itoch: \", itoch)\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtr.shape: torch.Size([182384, 3]), ytr.shape: torch.Size([182384])\n",
      "Xval.shape: torch.Size([22883, 3]), yval.shape: torch.Size([22883])\n",
      "Xte.shape: torch.Size([22879, 3]), yte.shape: torch.Size([22879])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def build_dataset(data, mode=\"tr\", context_len=3):\n",
    "  \"\"\"\n",
    "  Builds the dataset from the given list of words.\n",
    "  \n",
    "  Args:\n",
    "    data (list): list of words to use\n",
    "    mode (str): mode of the dataset (default: \"tr\" for training)\n",
    "    context_len (int): length of the context window (default: 3)\n",
    "  \"\"\"\n",
    "  \n",
    "  X, y = [], []\n",
    "  \n",
    "  for word in data:\n",
    "    context = [0] * context_len\n",
    "    for ch in word + \".\":\n",
    "      ix = chtoi[ch]\n",
    "      X.append(context)\n",
    "      y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "  \n",
    "  X = torch.tensor(X)\n",
    "  y = torch.tensor(y)\n",
    "  \n",
    "  print(f\"X{mode}.shape: {X.shape}, y{mode}.shape: {y.shape}\")\n",
    "  return X, y\n",
    "\n",
    "\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "Xtr, ytr = build_dataset(names[:n1])\n",
    "Xval, yval = build_dataset(names[n1:n2], mode=\"val\")\n",
    "Xte, yte = build_dataset(names[n2:], mode=\"te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  \"\"\"\n",
    "  Utility function to compare manual and PyTorch gradients\n",
    "\n",
    "  Args:\n",
    "    s (str): name of the tensor\n",
    "    dt (torch.Tensor): manual gradient\n",
    "    t (torch.Tensor): PyTorch gradient\n",
    "  \"\"\"\n",
    "  \n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  max_diff = (dt - t.grad).abs().max().item()\n",
    "  print(f\"{s:15s} | exact: {str(ex):5s} | approx: {str(app):5s} | max_diff: {max_diff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4137"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_embd = 10         # dimensionality of character embeddings vector\n",
    "n_hidden = 64       # number of neurons in the hidden layer\n",
    "context_len = 3     # length of the context window\n",
    "gain = 5.0 / 3.0    # gain for the Kaiming initialization\n",
    "\n",
    "\"\"\"\n",
    "Parameters are initialized in non-standard ways because initializing them with\n",
    "all zeroes could mask an incorrect implementation of the backpropagation.\n",
    "\n",
    "Using b1 despite batch normalization because it's easier to compare the gradients.\n",
    "\"\"\"\n",
    "\n",
    "C =  torch.randn(vocab_size, n_embd,             requires_grad=True)\n",
    "# Layer 1\n",
    "W1 = torch.randn(n_embd * context_len, n_hidden, requires_grad=True) * gain / math.sqrt(n_embd * context_len)\n",
    "b1 = torch.randn(n_hidden,                       requires_grad=True) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn(n_hidden, vocab_size,           requires_grad=True) * 0.1\n",
    "b2 = torch.randn(vocab_size,                     requires_grad=True) * 0.1\n",
    "# BatchNorm params\n",
    "gamma = torch.randn(1, n_hidden,                 requires_grad=True) * 0.1 + 1.0\n",
    "beta =  torch.randn(1, n_hidden,                 requires_grad=True) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, gamma, beta]\n",
    "sum(p.nelement() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# construct a mini-batch\n",
    "ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "Xb, yb = Xtr[ix], ytr[ix]     # batch X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation of forward pass\n",
    "emb = C[Xb]                                                      # embedding characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1)                              # concatenate embeddings\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1                                        # pre-batchnorm hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation of batch normalization\n",
    "hpreact_fast = gamma * (hprebn - hprebn.mean(dim=0, keepdim=True)) / torch.sqrt(hprebn.var(dim=0, keepdim=True, unbiased=True) + 1e-5) + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff:  0.0\n"
     ]
    }
   ],
   "source": [
    "# explicit implementation of batch normalization\n",
    "bnmeani = hprebn.mean(dim=0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiffsq = bndiff ** 2\n",
    "bnvar = bndiffsq.sum(dim=0, keepdim=True) / (batch_size - 1)     # Bessel's correction (N-1)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5                               # equivalent to 1.0 / torch.sqrt(bnvar + 1e-5)\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = gamma * bnraw + beta\n",
    "print(\"max diff: \", round((hpreact - hpreact_fast).abs().max().item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-linearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2                                             # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4687, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch implementation of cross-entropy loss\n",
    "loss_fast = F.cross_entropy(logits, yb)\n",
    "loss_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4687 max diff:  -0.0\n"
     ]
    }
   ],
   "source": [
    "# explicit implementation of cross-entropy loss\n",
    "logits_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logits_maxes                              # subtract max for numerical stability\n",
    "counts = norm_logits.exp()                                       # transform logits into positive values\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum ** -1                                # (1.0 / counts_sum) cannot get bit-exact backpropagation\n",
    "probs = counts * counts_sum_inv                                  # transform normalized counts into probabilities\n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(batch_size), yb].mean()\n",
    "print(round(loss.item(), 4), \"max diff: \", round(loss_fast.item() - loss.item(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4687, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pytorch gradient computation\n",
    "training_tensors = [log_probs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logits_maxes, logits, h, W2, b2, hpreact, gamma, beta, bnraw, bnvar_inv, bnvar, bndiffsq, bndiff, bnmeani, hprebn, embcat, emb, W1, b1]\n",
    "\n",
    "for t in training_tensors:\n",
    "  t.retain_grad()\n",
    "\n",
    "for p in params:\n",
    "  p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Atomic compute graph backpropagation\n",
    "\n",
    "Implement backpropagation manually for the entire forward pass compute graph, using atomic operations as building blocks.\n",
    "\n",
    "### Step: Explicit implementation of cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogprobs | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of log_probs\n",
    "\n",
    "derivation of log_probs in relation to loss: loss = -log_probs[range(batch_size), yb].mean()\n",
    "loss = - (a + b + c) / n\n",
    "loss = -a/n + -b/n + -c/n\n",
    "dloss_da = -1/n\n",
    "dloss_db = -1/n\n",
    "dloss_dc = -1/n\n",
    "\"\"\"\n",
    "dloss_dlogprobs = torch.zeros_like(log_probs)\n",
    "dloss_dlogprobs[range(batch_size), yb] = -1.0 / batch_size\n",
    "cmp(\"dloss_dlogprobs\", dloss_dlogprobs, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dprobs    | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of probs\n",
    "note: intuitively, a high probability character will be simply passed through the chain rule, while a low probability will be boosted to generate a high local gradient.\n",
    "\n",
    "derivation of probs in relation to log_probs: log_probs = probs.log()\n",
    "log_probs = log(a), log(b), log(c)\n",
    "dlogprobs_da = 1/a\n",
    "dlogprobs_db = 1/b\n",
    "dlogprobs_dc = 1/c\n",
    "\n",
    "derivation of probs in relation to loss: dlogprobs_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dprobs = 1.0 / probs * dloss_dlogprobs\n",
    "cmp(\"dloss_dprobs\", dloss_dprobs, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcountssuminv | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts_sum_inv\n",
    "\n",
    "derivation of counts_sum_inv in relation to probs: probs = counts * counts_sum_inv\n",
    "probs = a * b\n",
    "dprobs_da = b\n",
    "dprobs_db = a\n",
    "\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum_inv.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "a[3,2] * b[3,1] ---> c[3,2]\n",
    "a11 * b1, a12 * b1\n",
    "a21 * b2, a22 * b2\n",
    "a31 * b3, a32 * b3\n",
    "\n",
    "derivation of counts_sum_inv in relation to loss: SUM(dprobs_dn * chain_rule) backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dcountssuminv = (counts * dloss_dprobs).sum(1, keepdim=True)\n",
    "cmp(\"dloss_dcountssuminv\", dloss_dcountssuminv, counts_sum_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcountssum | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts_sum\n",
    "\n",
    "derivation of counts_sum in relation to counts_sum_inv: counts_sum_inv = counts_sum ** -1\n",
    "counts_sum_inv = a^-1\n",
    "dcountssum_da = - a^-2\n",
    "\n",
    "derivation of counts_sum in relation to loss: dcountssum_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dcountssum = -counts_sum ** -2 * dloss_dcountssuminv\n",
    "cmp(\"dloss_dcountssum\", dloss_dcountssum, counts_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dcounts   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of counts\n",
    "note: both probs and counts_sum depend on counts, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of counts in relation to probs: probs = counts * counts_sum_inv\n",
    "probs = a * b\n",
    "dprobs_da = b\n",
    "dprobs_db = a\n",
    "\n",
    "(2nd branch) derivation of counts in relation to counts_sum: counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum = a + b + c\n",
    "dcountssum_da = 1\n",
    "dcountssum_db = 1\n",
    "dcountssum_dc = 1\n",
    "\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "sum(a[3,2]) ---> b[3,1]\n",
    "a11 + a12 ---> b1\n",
    "a21 + a22 ---> b2\n",
    "a31 + a32 ---> b3\n",
    "\n",
    "derivation of counts in relation to loss: dprobs_dn * chain_rule + dcountssum_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dcounts = counts_sum_inv * dloss_dprobs + torch.ones_like(counts) * dloss_dcountssum\n",
    "cmp(\"dloss_dcounts\", dloss_dcounts, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dnormlogits | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of norm_logits\n",
    "\n",
    "derivation of norm_logits in relation to counts: counts = norm_logits.exp()\n",
    "counts = e^a\n",
    "dcounts_da = e^a\n",
    "\n",
    "derivation of norm_logits in relation to loss: dcounts_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dnormlogits = counts * dloss_dcounts\n",
    "cmp(\"dloss_dnormlogits\", dloss_dnormlogits, norm_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogitsmaxes | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of logits_maxes\n",
    "note: logits_maxes are calculated only for numerical stability of softmax function and do not affect the loss. Therefore, the gradient is zero (or close to it, due to floating point precision).\n",
    "\n",
    "derivation of logits_maxes in relation to norm_logits: norm_logits = logits - logits_maxes\n",
    "norm_logits = a + -b\n",
    "dnormlogits_da = 1\n",
    "dnormlogits_db = -1\n",
    "\n",
    "counts.shape: (batch_size, vocab_size)\n",
    "counts_sum_inv.shape: (batch_size, 1)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "a[3,2] - b[3,1] ---> c[3,2]\n",
    "a11 - b1, a12 - b1\n",
    "a21 - b2, a22 - b2\n",
    "a31 - b3, a32 - b3\n",
    "\n",
    "derivation of logits_maxes in relation to loss: SUM(dnormlogits_dn * chain_rule) backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dlogitsmaxes = -dloss_dnormlogits.sum(1, keepdim=True)\n",
    "cmp(\"dloss_dlogitsmaxes\", dloss_dlogitsmaxes, logits_maxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogits   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of logits\n",
    "note: both norm_logits and logits_maxes depend on logits, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of logits in relation to norm_logits: check dloss_dlogitsmaxes\n",
    "\n",
    "(2nd branch) derivation of logits in relation to logits_maxes: logits_maxes = logits.max(1, keepdim=True).values\n",
    "logits_maxes = a\n",
    "dlogitsmaxes_da = 1\n",
    "\n",
    "derivation of logits in relation to loss: dnormlogits_dn * chain_rule + dlogitsmaxes_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dlogits = dloss_dnormlogits.clone() + F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]) * dloss_dlogitsmaxes\n",
    "cmp(\"dloss_dlogits\", dloss_dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'One-hot encoding of the maximum logits')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1]))\n",
    "plt.xlabel(\"logits\")\n",
    "plt.ylabel(\"batch_size\")\n",
    "plt.title(\"One-hot encoding of the maximum logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Linear layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dh        | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_dW2       | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_db2       | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each element of the linear layer: logits = h @ W2 + b2\n",
    "\n",
    "derivation of h in relation to logits: dlogits/dh = W2.T\n",
    "derivation of W2 in relation to logits: dlogits/dW2 = h.T\n",
    "derivation of b2 in relation to logits: dlogits/db2 = 1\n",
    "\n",
    "h.shape: (batch_size, n_hidden)\n",
    "W2.shape: (n_hidden, vocab_size)\n",
    "b2.shape: (vocab_size,)\n",
    "\n",
    "matrix broadcasting with tensors:\n",
    "d[2,2] ---> a[2,2] @ b[2,2] + c[2]\n",
    "d11 = a11 * b11 + a12 * b21 + c1\n",
    "d12 = a11 * b12 + a12 * b22 + c2\n",
    "d21 = a21 * b11 + a22 * b21 + c1\n",
    "d22 = a21 * b12 + a22 * b22 + c2\n",
    "\n",
    "derivation of x in relation to a: dx/da[i,j] = dx/dd[i,j] @ b[i,j].T\n",
    "dx/da11 = dx/dd11 * b11 + dx/dd12 * b12\n",
    "dx/da12 = dx/dd11 * b21 + dx/dd12 * b22\n",
    "dx/da21 = dx/dd21 * b11 + dx/dd22 * b12\n",
    "dx/da22 = dx/dd21 * b21 + dx/dd22 * b22\n",
    "\n",
    "derivation of x in relation to b: dx/db[i, j] = a[i, j].T @ dx/dd[i, j]\n",
    "\n",
    "derivation of x in relation to c: dx/dc[i] = dx/dd[i].sum(dim=0)\n",
    "dx/dc1 = dx/dd11 * 1 + dx/dd21 * 1\n",
    "dx/dc2 = dx/dd12 * 1 + dx/dd22 * 1\n",
    "\n",
    "derivation of h in relation to loss: chain rule @ W2.T\n",
    "derivation of W2 in relation to loss: h.T @ chain rule\n",
    "derivation of b2 in relation to loss: SUM(chain rule) @ 1\n",
    "\"\"\"\n",
    "dloss_dh = dloss_dlogits @ W2.T\n",
    "dloss_dW2 = h.T @ dloss_dlogits\n",
    "dloss_db2 = dloss_dlogits.sum(dim=0)\n",
    "cmp(\"dloss_dh\", dloss_dh, h)\n",
    "cmp(\"dloss_dW2\", dloss_dW2, W2)\n",
    "cmp(\"dloss_db2\", dloss_db2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dhpreact  | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each output of the first layer before activation (hpreact)\n",
    "\n",
    "derivation of hpreact in relation to h: h = torch.tanh(hpreact)\n",
    "h = tanh(a)\n",
    "dh_da = 1 - h^2\n",
    "\n",
    "derivation of hpreact in relation to loss: dh_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dhpreact = (1.0 - h ** 2) * dloss_dh\n",
    "cmp(\"dloss_dhpreact\", dloss_dhpreact, hpreact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dgamma    | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_dbnraw    | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_dbeta     | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each element of the batch normalization output: hpreact = gamma * bnraw + beta\n",
    "\n",
    "derivation of gamma in relation to hpreact:\n",
    "hpreact = a * b + c\n",
    "dpreact_da = b\n",
    "dpreact_db = a\n",
    "\n",
    "derivation of bnraw in relation to hpreact: see above\n",
    "\n",
    "derivation of beta in relation to hpreact:\n",
    "dpreact_dc = 1\n",
    "\n",
    "gamma.shape: (1, n_hidden)\n",
    "bnraw.shape: (batch_size, n_hidden)\n",
    "beta.shape: (1, n_hidden)\n",
    "\n",
    "derivation of gamma in relation to loss: SUM(dpreact_dn * chain_rule) backpropagating through matrix broadcasting\n",
    "derivation of bnraw in relation to loss: dpreact_dn * chain_rule\n",
    "derivation of beta in relation to loss: SUM(dpreact_dn * chain_rule) backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dgamma = (bnraw * dloss_dhpreact).sum(dim=0, keepdim=True)\n",
    "dloss_dbnraw = (gamma * dloss_dhpreact)\n",
    "dloss_dbeta = dloss_dhpreact.sum(dim=0, keepdim=True)\n",
    "cmp(\"dloss_dgamma\", dloss_dgamma, gamma)\n",
    "cmp(\"dloss_dbnraw\", dloss_dbnraw, bnraw)\n",
    "cmp(\"dloss_dbeta\", dloss_dbeta, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dbnvarinv | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of bnvar_inv\n",
    "\n",
    "derivation of bnvar_inv in relation to bnraw: bnraw = bndiff * bnvar_inv\n",
    "bnraw = a * b\n",
    "dbnraw_da = b\n",
    "dbnraw_db = a\n",
    "\n",
    "derivation of bnvar_inv in relation to loss: SUM(dbnraw_dn * chain_rule) backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dbnvarinv = (bndiff * dloss_dbnraw).sum(dim=0, keepdim=True)\n",
    "cmp(\"dloss_dbnvarinv\", dloss_dbnvarinv, bnvar_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dbnvar    | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of bnvar\n",
    "\n",
    "derivation of bnvar in relation to bnvar_inv: bnvar_inv = (bnvar + 1e-5)^-0.5\n",
    "bnvar_inv = a^b\n",
    "dbnvarinv_da = b * a^(b-1)\n",
    "\n",
    "derivation of bnvar in relation to loss: dbnvarinv_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dbnvar = -0.5 * (bnvar + 1e-5) ** (-0.5 - 1) * dloss_dbnvarinv\n",
    "cmp(\"dloss_dbnvar\", dloss_dbnvar, bnvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dbndiffsq | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of bndiffsq\n",
    "\n",
    "derivation of bndiffsq in relation to bnvar: bnvar = bndiffsq.sum(dim=0, keepdim=True) / (batch_size - 1)\n",
    "bnvar = (a + b + c) / (n - 1)\n",
    "dbnvar_dn = 1 / (n - 1)\n",
    "\n",
    "derivation of bndiffsq in relation to loss: dbnvar_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dbndiffsq = torch.ones_like(bndiffsq) * dloss_dbnvar / (batch_size - 1)\n",
    "cmp(\"dloss_dbndiffsq\", dloss_dbndiffsq, bndiffsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dbndiff   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of bndiff\n",
    "note: both bnraw and bndiffsq depend on counts, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of bndiff in relation to bnraw: bnraw = bndiff * bnvar_inv\n",
    "bnraw = a * b\n",
    "dbnraw_da = b\n",
    "dbnraw_db = a\n",
    "\n",
    "(2nd branch) derivation of bndiff in relation to bndiffsq: bndiffsq = bndiff^2\n",
    "bndiffsq = a^2\n",
    "dbndiffsq_da = 2a\n",
    "\n",
    "derivation of bndiff in relation to loss: dbnraw_dn * chain_rule + dbndiffsq_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dbndiff = bnvar_inv * dloss_dbnraw + 2 * bndiff * dloss_dbndiffsq\n",
    "cmp(\"dloss_dbndiff\", dloss_dbndiff, bndiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dbnmeani  | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of bnmeani\n",
    "\n",
    "derivation of bnmeani in relation to bndiff: bndiff = hprebn - bnmeani\n",
    "bndiff = a + -b\n",
    "dbndiff_da = 1\n",
    "dbndiff_db = -1\n",
    "\n",
    "derivation of bnmeani in relation to loss: SUM(-1.0 * chain_rule) backpropagating through matrix broadcasting\n",
    "\"\"\"\n",
    "dloss_dbnmeani = -dloss_dbndiff.sum(dim=0, keepdim=True)\n",
    "cmp(\"dloss_dbnmeani\", dloss_dbnmeani, bnmeani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dhprebn   | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of hprebn\n",
    "note: both bndiff and bnmeani depend on hprebn, so we need to backpropagate through both branches\n",
    "\n",
    "(1st branch) derivation of hprebn in relation to bndiff: bndiff = hprebn - bnmeani\n",
    "bndiff = a + -b\n",
    "dbndiff_da = 1\n",
    "dbndiff_db = -1\n",
    "\n",
    "(2nd branch) derivation of hprebn in relation to bnmeani: bnmeani = hprebn.mean(dim=0, keepdim=True)\n",
    "bnmeani = (a + b) / 2\n",
    "dbnmeani_da = 1 / 2\n",
    "\n",
    "derivation of hprebn in relation to loss: 1.0 * chain_rule + dbnmeani_dn * chain_rule\n",
    "\"\"\"\n",
    "dloss_dhprebn = dloss_dbndiff.clone() + torch.ones_like(hprebn) / hprebn.shape[0] * dloss_dbnmeani\n",
    "cmp(\"dloss_dhprebn\", dloss_dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Linear layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dembcat   | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_dW1       | exact: True  | approx: True  | max_diff: 0.00\n",
      "dloss_db1       | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each element of the linear layer: hprebn = embcat @ W1 + b1    \n",
    "\n",
    "derivation of embcat in relation to hprebn: dhprebn/dembcat = W1.T\n",
    "derivation of W1 in relation to hprebn: dhprebn/dW1 = embcat.T\n",
    "derivation of b1 in relation to hprebn: dhprebn/db1 = 1\n",
    "\n",
    "derivation of embcat in relation to loss: chain rule @ W1.T\n",
    "derivation of W1 in relation to loss: embcat.T @ chain rule\n",
    "derivation of b1 in relation to loss: SUM(chain rule) @ 1\n",
    "\"\"\"\n",
    "dloss_dembcat = dloss_dhprebn @ W1.T\n",
    "dloss_dW1 = embcat.T @ dloss_dhprebn\n",
    "dloss_db1 = dloss_dhprebn.sum(dim=0)\n",
    "cmp(\"dloss_dembcat\", dloss_dembcat, embcat)\n",
    "cmp(\"dloss_dW1\", dloss_dW1, W1)\n",
    "cmp(\"dloss_db1\", dloss_db1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: Setup the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_demb      | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of emb\n",
    "\n",
    "derivation of emb in relation to loss: embcat = emb.view(emb.shape[0], -1)    \n",
    "note: embcat is a reshaped version of emb and its derivative is the undo of the reshape\n",
    "\"\"\"\n",
    "dloss_demb = dloss_dembcat.view(emb.shape)\n",
    "cmp(\"dloss_demb\", dloss_demb, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dC        | exact: True  | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "interpretation: how loss changes with respect to each of C\n",
    "\n",
    "derivation of C in relation to emb: emb = C[Xb]     \n",
    "note: emb is the result of an indexing operation and its derivative is the undo of the indexing\n",
    "\n",
    "emb.shape: (batch_size, context_len, n_embd)\n",
    "C.shape: (vocab_size, n_embd)\n",
    "Xb.shape: (batch_size, context_len)\n",
    "\"\"\"\n",
    "dloss_dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k, j]\n",
    "    dloss_dC[ix] += dloss_demb[k, j]\n",
    "cmp(\"dloss_dC\", dloss_dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross entropy backpropagation in one go\n",
    "\n",
    "The atomic computation performed above is inefficient. For example, the cross entropy loss could be calculated as a single operation that can be backpropagated in one go.\n",
    "\n",
    "<img src=\"../assets/nn-strut.jpg\" width=\"700\"/>\n",
    "\n",
    "Considering the structure above, to backward propagate the loss all the way to logits in one step for a single individual example:\n",
    "\n",
    "$\\text{loss} = -\\log P(y)$, where $P$ is the softmax vector output, and $y$ is the true label position in the vector.\n",
    "\n",
    "$P(i) = \\frac{e^{l_i}}{\\sum_{j} e^{l_j}}$, where $l$ is the logits vector, and $i$ is the position in the vector.\n",
    "\n",
    "The derivative of the loss with respect to the i-th logit is: $\\frac{\\partial \\text{loss}}{\\partial l_i} = \\frac{\\partial \\text{loss}}{\\partial l_i}[-\\log \\frac{e^{l_i}}{\\sum_{j} e^{l_j}}]$. Deriving this expression, we get: $\\frac{\\partial \\text{loss}}{\\partial l_i} = P(i) - 1(y=i)$, where $1(y=i)$ is the indicator function that is 1 when $y=i$, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dlogits_fast | exact: False | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "# The match is not exact due to the floating point precision\n",
    "dloss_dlogits_fast = F.softmax(logits, dim=1)\n",
    "dloss_dlogits_fast[range(batch_size), yb] -= 1.0\n",
    "dloss_dlogits_fast /= batch_size\n",
    "cmp(\"dloss_dlogits_fast\", dloss_dlogits_fast, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of probabilities:  [0.05460495 0.01869192 0.06089635 0.01185056 0.01790318 0.04536892\n",
      " 0.02940609 0.04925381 0.03800049 0.02144133 0.01881717 0.01339234\n",
      " 0.04546446 0.09663055 0.02534464 0.03392309 0.03320727 0.03450454\n",
      " 0.07901236 0.01965421 0.05352899 0.03588085 0.02504592 0.03852617\n",
      " 0.02083166 0.03107048 0.04774765]\n",
      "First row of dloss_dlogits:  [ 0.05460495  0.01869192  0.06089636  0.01185056  0.01790318  0.04536892\n",
      "  0.02940609  0.04925381  0.03800049  0.02144133  0.01881717  0.01339234\n",
      "  0.04546446  0.09663058 -0.9746554   0.03392309  0.03320728  0.03450454\n",
      "  0.07901236  0.01965421  0.053529    0.03588085  0.02504592  0.03852617\n",
      "  0.02083166  0.03107048  0.04774765]\n",
      "Sum of first row of dloss_dlogits:  -6.984919309616089e-10\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Interpretation: the black squares are the positions of the correct character in each row, where 1.0 was subtracted from the softmax output.\n",
    "                Consider the gradients at each cell like a `force`, pulling down on the probabilities of the incorrect characters and pushing up on the probability of the correct character. The amount of force (pull and push) is exactly equalized across the row, so that the sum of the forces is zero (ie. repulsion and attraction are equal). Think of the neural net as a massive pulley system, where the forces are balanced across the row, so that the net doesn't move in any direction. This is the essence of the backpropagation algorithm.\n",
    "\n",
    "                For example, if the probability came out exactly correct, the vector would have zeroes everywhere, except for a one at the correct position, resulting in dloss_dlogits being a vector of only zeroes (no pull or push). On the other hand, a very confidently misprediction would be pulled down very heavily and the correct answer will be pushed up by the same amount, while the remaining characters will not be influenced too much.\n",
    "\"\"\"\n",
    "print(\"First row of probabilities: \", F.softmax(logits, dim=1)[0].detach().numpy())\n",
    "print(\"First row of dloss_dlogits: \", (dloss_dlogits[0] * batch_size).detach().numpy())\n",
    "print(\"Sum of first row of dloss_dlogits: \", dloss_dlogits[0].sum().item())\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(dloss_dlogits_fast.detach(), cmap=\"gray\")\n",
    "plt.title(\"dloss_dlogits\")\n",
    "plt.xlabel(\"vocab_size\")\n",
    "plt.ylabel(\"batch_size\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch normalization in one go\n",
    "\n",
    "In order to calculate the $\\frac{\\partial \\text{loss}}{\\partial x_i}$ for the batch normalization layer in a single step, consider the algorithm implementation below, from the Batch Norm paper:\n",
    "\n",
    "<img src=\"../assets/batch-norm-algo.jpg\" width=\"400\"/>\n",
    "\n",
    "In this exercise, the algorithm was implemented as shown above, except that it used Bessel's correction for the variance calculation.\n",
    "\n",
    "After all the required derivations (not trivial), the final expression for the derivative of the loss with respect to the input of the batch normalization layer is:\n",
    "\n",
    "$\\frac{\\partial \\text{loss}}{\\partial x_i} = \\frac{\\gamma(\\sigma^2 + \\epsilon)^{-1/2}}{m}[m \\frac{\\partial \\text{loss}}{\\partial y_i} - \\sum_{j} \\frac{\\partial \\text{loss}}{\\partial y_j} - \\frac{m}{m-1} \\hat{x}_i \\sum_{j} \\frac{\\partial \\text{loss}}{\\partial y_i} \\hat{x}_j]$, where $j$ is the local iterator over the batch, representing the index of the elements in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss_dhprebn_fast | exact: False | approx: True  | max_diff: 0.00\n"
     ]
    }
   ],
   "source": [
    "# The match is not exact due to the floating point precision\n",
    "dloss_dhprebn_fast = gamma * bnvar_inv / batch_size * (batch_size * dloss_dhpreact - dloss_dhpreact.sum(dim=0) \\\n",
    "                                                       - batch_size / (batch_size - 1) * bnraw * (dloss_dhpreact * bnraw).sum(dim=0))\n",
    "cmp(\"dloss_dhprebn_fast\", dloss_dhprebn_fast, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Putting it all together\n",
    "\n",
    "Train the neural network with the self implemented backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10         # dimensionality of character embeddings vector\n",
    "n_hidden = 200      # number of neurons in the hidden layer\n",
    "\n",
    "C =  torch.randn(vocab_size, n_embd)\n",
    "# Layer 1\n",
    "W1 = torch.randn(n_embd * context_len, n_hidden) * gain / math.sqrt(n_embd * context_len)\n",
    "b1 = torch.randn(n_hidden) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn(n_hidden, vocab_size) * 0.1\n",
    "b2 = torch.randn(vocab_size) * 0.1\n",
    "# BatchNorm params\n",
    "gamma = torch.randn(1, n_hidden) * 0.1 + 1.0\n",
    "beta =  torch.randn(1, n_hidden) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, gamma, beta]\n",
    "for p in params:\n",
    "  p.requires_grad = False\n",
    "\n",
    "# optimization hyperparameters\n",
    "max_epochs = 200_000\n",
    "batch_size = 32\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      0 / 200000: loss = 3.5745\n",
      "Epoch  25000 / 200000: loss = 1.9827\n",
      "Epoch  50000 / 200000: loss = 2.0217\n",
      "Epoch  75000 / 200000: loss = 2.1881\n",
      "Epoch 100000 / 200000: loss = 2.2718\n",
      "Epoch 125000 / 200000: loss = 2.5477\n",
      "Epoch 150000 / 200000: loss = 1.8672\n",
      "Epoch 175000 / 200000: loss = 2.0027\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  for epoch in range(max_epochs):\n",
    "    # construct a mini-batch\n",
    "    ix = torch.randint(0, len(Xtr), (batch_size,))\n",
    "    Xb, yb = Xtr[ix], ytr[ix]            # batch X, y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb]                          # embedding characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1)  # concatenate embeddings\n",
    "\n",
    "    # Linear layer 1\n",
    "    hprebn = embcat @ W1 + b1            # pre-batchnorm hidden layer\n",
    "\n",
    "    # Batch normalization\n",
    "    bnmean = hprebn.mean(dim=0, keepdim=True)\n",
    "    bnvar = hprebn.var(dim=0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = gamma * bnraw + beta\n",
    "\n",
    "    # non-linearity\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2                 # output layer\n",
    "    loss = F.cross_entropy(logits, yb)   # cross-entropy loss\n",
    "\n",
    "    # backward pass\n",
    "    for p in params:\n",
    "      p.grad = None\n",
    "\n",
    "    # manual backpropagation\n",
    "    dloss_dlogits = F.softmax(logits, dim=1)\n",
    "    dloss_dlogits[range(batch_size), yb] -= 1.0\n",
    "    dloss_dlogits /= batch_size\n",
    "\n",
    "    dloss_dh = dloss_dlogits @ W2.T\n",
    "    dloss_dW2 = h.T @ dloss_dlogits\n",
    "    dloss_db2 = dloss_dlogits.sum(dim=0)\n",
    "\n",
    "    dloss_dhpreact = (1.0 - h ** 2) * dloss_dh\n",
    "\n",
    "    dloss_dgamma = (bnraw * dloss_dhpreact).sum(dim=0, keepdim=True)\n",
    "    dloss_dbeta = dloss_dhpreact.sum(dim=0, keepdim=True)\n",
    "    dloss_dhprebn = gamma * bnvar_inv / batch_size * (batch_size * dloss_dhpreact - dloss_dhpreact.sum(dim=0) \\\n",
    "                                                      - batch_size / (batch_size - 1) * bnraw * (dloss_dhpreact * bnraw).sum(dim=0))\n",
    "    \n",
    "    dloss_dembcat = dloss_dhprebn @ W1.T\n",
    "    dloss_dW1 = embcat.T @ dloss_dhprebn\n",
    "    dloss_db1 = dloss_dhprebn.sum(dim=0)\n",
    "\n",
    "    dloss_demb = dloss_dembcat.view(emb.shape)\n",
    "    dloss_dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dloss_dC[ix] += dloss_demb[k, j]\n",
    "\n",
    "    # update the parameters\n",
    "    grads = [dloss_dC, dloss_dW1, dloss_db1, dloss_dW2, dloss_db2, dloss_dgamma, dloss_dbeta]\n",
    "    lr = 0.1 if epoch < 100_000 else 0.01 \n",
    "    for p, grad in zip(params, grads):\n",
    "      p.data += -lr * grad\n",
    "\n",
    "    # track stats\n",
    "    if epoch % 25_000 == 0:\n",
    "      print(f\"Epoch {epoch:6d} / {max_epochs}: loss = {loss.item():.4f}\")\n",
    "    lossi.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate batch normalization parameters\n",
    "with torch.no_grad():\n",
    "  # pass the entire dataset through the network\n",
    "  emb = C[Xtr]                          # embedding characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1)   # concatenate embeddings\n",
    "  hprebn = embcat @ W1 + b1             # pre-batchnorm hidden layer\n",
    "  # measure the mean and variance of the hidden layer\n",
    "  bnmean = hprebn.mean(dim=0, keepdim=True)\n",
    "  bnvar = hprebn.var(dim=0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_tr: 2.0711\n",
      "loss_val: 2.1296\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and validation loss\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "  X, y = {\n",
    "    \"tr\": (Xtr, ytr),\n",
    "    \"val\": (Xval, yval),\n",
    "    \"te\": (Xte, yte)\n",
    "  }[split]\n",
    "  emb = C[X]                         \n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hprebn = embcat @ W1 + b1          \n",
    "  hpreact = gamma * (hprebn - bnmean) / torch.sqrt(bnvar + 1e-5) + beta\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(f\"loss_{split}: {loss.item():.4f}\")\n",
    "\n",
    "split_loss(\"tr\")\n",
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Building makemore Part 4: Becoming a Backprop Ninja, By Andrej Karpathy](https://www.youtube.com/watch?v=q8SA3rM6ckI&t=6s)\n",
    "2. [Yes, you should understand backprop, By Andrej Karpathy](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "3. [Bessel's Correction](https://math.oxford.emory.edu/site/math117/besselCorrection/)\n",
    "4. [Reducing the Dimensionality of Data with Neural Networks, By G. Hinton](https://www.cs.toronto.edu/~hinton/absps/science.pdf)\n",
    "5. Karpathy, Andrej; Li, Fei-Fei. Deep Fragment Embeddings for Bidirectional Image Sentence Mapping, 2014. URL: https://arxiv.org/pdf/1406.5679.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
