{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline language modeling and Code setup\n",
    "\n",
    "### 1.1. Reading and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tiny_shakespear.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "print('length of dataset in characters: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tokenization\n",
    "\n",
    "Trade-off: very long vocabulary size and very short sequences or very short vocabulary size and very long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  65\n",
      "vocab:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab size: ', vocab_size)\n",
    "print('vocab: ', ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [46, 43, 50, 50, 53]\n",
      "decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [stoi[ch] for ch in string]\n",
    "decode = lambda tokens: ''.join([itos[t] for t in tokens])\n",
    "\n",
    "print('encoded: ', encode('hello'))\n",
    "print('decoded: ', decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1. Production-grade example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  50257\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print('vocab size: ', enc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [31373]\n",
      "decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "print('encoded: ', enc.encode('hello'))\n",
    "print('decoded: ', enc.decode(enc.encode('hello')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Tokenize `tiny shakespear` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokenized text:  torch.Size([1115393])\n",
      "Dtype of tokenized text:  torch.int64\n",
      "First 10 characters:  First Citi\n",
      "First 10 tokens:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = torch.tensor(encode(text), dtype=torch.long)\n",
    "print('Shape of tokenized text: ', tokenized_text.shape)\n",
    "print('Dtype of tokenized text: ', tokenized_text.dtype)\n",
    "print('First 10 characters: ', text[:10])\n",
    "print('First 10 tokens: ', tokenized_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3. `Train` and `Validation` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(tokenized_text) * 0.85)\n",
    "train_dataset = tokenized_text[:n]\n",
    "val_dataset = tokenized_text[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data loader: batches of chunks of data\n",
    "\n",
    "**Note**: when a batch is created, each token dimension is an INFORMATION POINT in relation to the next token. Thus, each batch packs multiple examples in relation to the next token.\n",
    "\n",
    "Example:\n",
    "\n",
    "Block: [18, 47, 56, 57, 58,  1, 15, 47]\n",
    "Next token: 58\n",
    "\n",
    "1. Context: 18         -> 47 likely follows next\n",
    "2. Context: 18, 47     -> 56 likely follows next\n",
    "3. Context: 18, 47, 56 -> 57 likely follows next\n",
    "4. and so on...\n",
    "\n",
    "**TIME DIMENSION**: The idea behind training in this way is for the **transformer** to be able to predict the next token with as little as one token of context. Then, after `block_size`is reached, the inputs need to be truncated, because the **transformer** will never receive more than `block_size` tokens of context.\n",
    "\n",
    "**BATCH DIMENSION**: The idea behind batching is to train the model with multiple examples at the same time. This is done to speed up training and to make the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 9 tokens:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First block:  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Next token:  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "\n",
    "print('First 9 tokens: ', train_dataset[:context_length + 1])\n",
    "print('First block: ', train_dataset[:context_length])\n",
    "print('Next token: ', train_dataset[context_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  tensor([18])\n",
      "Target:  tensor(47)\n",
      "Context:  tensor([18, 47])\n",
      "Target:  tensor(56)\n",
      "Context:  tensor([18, 47, 56])\n",
      "Target:  tensor(57)\n",
      "Context:  tensor([18, 47, 56, 57])\n",
      "Target:  tensor(58)\n",
      "Context:  tensor([18, 47, 56, 57, 58])\n",
      "Target:  tensor(1)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target:  tensor(15)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target:  tensor(47)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target:  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# time dimension\n",
    "\n",
    "x = train_dataset[:context_length]\n",
    "y = train_dataset[1:context_length+1]\n",
    "\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('Context: ', context)\n",
    "    print('Target: ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  948084\n",
      "Sample of data:  tensor([18, 47, 56,  ..., 13, 57,  1])\n",
      "random_observations:  tensor([724804, 822947, 340160, 803020])\n",
      "Input batch:  tensor([[43, 57, 57,  6,  0, 27, 56,  1],\n",
      "        [ 1, 43, 63, 43, 57,  1, 43, 50],\n",
      "        [10,  0, 32, 46, 47, 52, 45, 57],\n",
      "        [43,  1, 61, 47, 50, 50,  1, 39]])\n",
      "Target batch:  tensor([[57, 57,  6,  0, 27, 56,  1, 43],\n",
      "        [43, 63, 43, 57,  1, 43, 50, 57],\n",
      "        [ 0, 32, 46, 47, 52, 45, 57,  1],\n",
      "        [ 1, 61, 47, 50, 50,  1, 39, 50]])\n",
      "Batch:  0 Time:  0\n",
      "Context:  tensor([43])\n",
      "Target:  tensor(57)\n",
      "Batch:  0 Time:  1\n",
      "Context:  tensor([43, 57])\n",
      "Target:  tensor(57)\n",
      "Batch:  0 Time:  2\n",
      "Context:  tensor([43, 57, 57])\n",
      "Target:  tensor(6)\n",
      "Batch:  0 Time:  3\n",
      "Context:  tensor([43, 57, 57,  6])\n",
      "Target:  tensor(0)\n",
      "Batch:  0 Time:  4\n",
      "Context:  tensor([43, 57, 57,  6,  0])\n",
      "Target:  tensor(27)\n",
      "Batch:  0 Time:  5\n",
      "Context:  tensor([43, 57, 57,  6,  0, 27])\n",
      "Target:  tensor(56)\n",
      "Batch:  0 Time:  6\n",
      "Context:  tensor([43, 57, 57,  6,  0, 27, 56])\n",
      "Target:  tensor(1)\n",
      "Batch:  0 Time:  7\n",
      "Context:  tensor([43, 57, 57,  6,  0, 27, 56,  1])\n",
      "Target:  tensor(43)\n"
     ]
    }
   ],
   "source": [
    "# batch dimension\n",
    "def get_batch(split, batch_size, verbose=False):\n",
    "    data = train_dataset if split == 'train' else val_dataset\n",
    "    if verbose:\n",
    "        print(\"Shape of data: \", len(data))\n",
    "        print('Sample of data: ', data)\n",
    "    \n",
    "    random_observations = torch.randint(0, len(data) - context_length, (batch_size,))\n",
    "    if verbose:\n",
    "        print(\"random_observations: \", random_observations)\n",
    "\n",
    "    input_batch = torch.stack([data[obs:obs+context_length] for obs in random_observations])\n",
    "    target_batch = torch.stack([data[obs+1:obs+context_length+1] for obs in random_observations])\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "batch_size = 4\n",
    "input_batch, target_batch = get_batch('train', batch_size, True)\n",
    "print('Input batch: ', input_batch)\n",
    "print('Target batch: ', target_batch)\n",
    "\n",
    "for batch in range(batch_size):           # batch dimension\n",
    "    for time in range(context_length):    # time dimension\n",
    "        context = input_batch[batch, :time+1]\n",
    "        target = target_batch[batch, time]\n",
    "        if batch == 0:\n",
    "            print('Batch: ', batch, 'Time: ', time)\n",
    "            print('Context: ', context)\n",
    "            print('Target: ', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Simplest baseline: bigram language model\n",
    "\n",
    "**Note**: this implementation is ridiculous by design. As a simple *character-level* bigram model, the prediction throws away all context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  65\n",
      "Input batch shape:  torch.Size([4, 8])\n",
      "Target batch shape:  torch.Size([4, 8])\n",
      "Logits shape:  torch.Size([32, 65])\n",
      "Loss:  tensor(4.7661, grad_fn=<NllLossBackward0>)\n",
      "Inputs:  tensor([[43, 57, 57,  6,  0, 27, 56,  1]])\n",
      "Predicted targets:  tensor([58, 31,  8, 33,  2, 31, 50, 11, 30, 20, 17, 12,  6,  1, 61, 30, 26,  6,\n",
      "        17, 54, 20, 30, 35, 12, 49, 46, 34, 31, 54, 49, 48, 61, 46,  4, 48,  5,\n",
      "        35, 12,  2, 10,  6, 11, 48, 35,  1, 58, 55, 14, 59, 48])\n",
      "Predicted characters:  tS.U!Sl;RHE?, wRN,EpHRW?khVSpkjwh&j'W?!:,;jW tqBuj\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        `self.embedding` is 65 x 65, because for each of the 65 tokens in the vocabulary,\n",
    "        we have a 65-dimensional vector that represents the probability of the next token\n",
    "        given the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.embedding(inputs)    # B, T, C (batch, time, channels)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, _, C = logits.shape\n",
    "            logits = logits.view(-1, C)  # Flatten to [B * T, C]\n",
    "            targets = targets.view(-1)   # Flatten to [B * T]\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, num_predictions):\n",
    "        predictions = torch.zeros(inputs.shape[0] * num_predictions, dtype=torch.long)\n",
    "\n",
    "        for i in range(num_predictions):\n",
    "            logits, _ = self(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            target = torch.multinomial(probs, num_samples=1)\n",
    "            predictions[i] = target\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(input_batch, target_batch)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print('Input batch shape: ', input_batch.shape)\n",
    "print('Target batch shape: ', target_batch.shape)\n",
    "print('Logits shape: ', logits.shape)\n",
    "print('Loss: ', loss)\n",
    "\n",
    "NUM_PREDICTIONS = 50\n",
    "INPUTS = input_batch[:1]\n",
    "\n",
    "print('Inputs: ', INPUTS)\n",
    "predicted_targets = model.generate(INPUTS, NUM_PREDICTIONS)\n",
    "print('Predicted targets: ', predicted_targets)\n",
    "print('Predicted characters: ', decode(predicted_targets.tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 4.661503314971924\n",
      "Epoch 2000 | Loss 3.0199105739593506\n",
      "Epoch 4000 | Loss 2.684917449951172\n",
      "Epoch 6000 | Loss 2.561742067337036\n",
      "Epoch 8000 | Loss 2.456726312637329\n",
      "Loss 2.36138653755188\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batch, target_batch = get_batch('train', batch_size)\n",
    "    logits, loss = model(input_batch, target_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 2000 == 0:\n",
    "        print(f'Epoch {epoch} | Loss {loss.item()}')\n",
    "\n",
    "print(f'Loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  tensor([[50, 47, 62, 43, 52, 43, 57,  0]])\n",
      "Predicted characters:  PSW!SLBIETR\n",
      "u\n",
      "RLHTI\n",
      "TMPWTWATA\n",
      "KI'DTHWFQEWOIOTNOC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_PREDICTIONS = 50\n",
    "INPUTS = input_batch[:1]\n",
    "\n",
    "print('Inputs: ', INPUTS)\n",
    "predicted_targets = model.generate(INPUTS, NUM_PREDICTIONS)\n",
    "print('Predicted characters: ', decode(predicted_targets.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the \"self-attention\" model\n",
    "\n",
    "### 2.1. `Version 1` - Weakest form of aggregation: averaging past context\n",
    "\n",
    "Each token in a batch should communicate information with other tokens in the batch, in such a way that information only flows from past tokens to the current token.\n",
    "\n",
    "Consider the fifth token in a batch of eight tokens. It should not communicate with tokens in the sixth, seventh and eighth positions, because those are FUTURE tokens in a sequence, but it should communicate with the fourth, third, second and first tokens, because those are PAST tokens in a sequence. This way, information only flows from previous context to the current timestep.\n",
    "\n",
    "Given this, the easiest way for tokens to communicate is to simply average all previous embeddings. This is the weakest form of aggregation and is extremely lossy, because all information about spatial arrangement of tokens is lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits:  torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "logits = torch.randn(B, T, C)\n",
    "print(\"Shape of logits: \", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want logits[b, t] = mean of logits[b, i] for i<=t\n",
    "logits_bow = torch.zeros((B, T, C))    # bow: bag of words\n",
    "\n",
    "for batch in range(B):\n",
    "    for time in range(T):\n",
    "        logits_prev = logits[batch, :time+1]\n",
    "        logits_bow[batch, time] = torch.mean(logits_prev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0372, -0.8731],\n",
       "         [ 0.8561,  1.5495],\n",
       "         [-0.7364, -0.5302],\n",
       "         [-0.7129,  1.1072],\n",
       "         [-0.2394, -0.3602],\n",
       "         [ 0.8442,  0.8462],\n",
       "         [ 1.2206,  1.9347],\n",
       "         [-1.4610, -0.5564]]),\n",
       " tensor([[ 0.0372, -0.8731],\n",
       "         [ 0.4466,  0.3382],\n",
       "         [ 0.0523,  0.0487],\n",
       "         [-0.1390,  0.3133],\n",
       "         [-0.1591,  0.1786],\n",
       "         [ 0.0081,  0.2899],\n",
       "         [ 0.1813,  0.5249],\n",
       "         [-0.0240,  0.3897]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each location of logits_bow is the vertical mean of all previous logits\n",
    "logits[0], logits_bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. The `mathematical trick` in self-attention: matrix multiplication with triangular mask\n",
    "\n",
    "Matrix multiplication is a very efficient way to calculate the dot product of each token with all other tokens. By masking the upper triangular part of the matrix, we can ensure that each token only communicates with previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b =\n",
      "tensor([[4., 9.],\n",
      "        [3., 8.],\n",
      "        [3., 0.]])\n",
      "---\n",
      "c =\n",
      "tensor([[10., 17.],\n",
      "        [10., 17.],\n",
      "        [10., 17.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the whole trick\n",
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b =\n",
      "tensor([[2., 1.],\n",
      "        [4., 3.],\n",
      "        [1., 5.]])\n",
      "---\n",
      "c =\n",
      "tensor([[2.0000, 1.0000],\n",
      "        [3.0000, 2.0000],\n",
      "        [2.3333, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. `Version 2` - Self-attention with matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. `Version 3`: Selft-attention adding softmax\n",
    "### 2.5. positional encoding\n",
    "### 2.6. THE CRUX OF IT ALL: version 4: self-attention\n",
    "### 2.7. note 1: attention as communication\n",
    "### 2.8. note 2: attention has no notion of space, operates over sets\n",
    "### 2.9. note 3: there is no communication across batch dimension\n",
    "### 2.10. note 4: encoder blocks vs. decoder blocks\n",
    "### 2.11. note 5: attention vs. self-attention vs. cross-attention\n",
    "### 2.12. note 6: \"scaled\" self-attention. why divide by sqrt(head_size)\n",
    "\n",
    "# 3. Building the transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cookbook-DNsoNefS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
