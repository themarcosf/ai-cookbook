{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Code setup and Baseline language modeling\n",
    "\n",
    "### 1.1. Reading and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tiny_shakespear.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "print('length of dataset in characters: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Tokenization\n",
    "\n",
    "Trade-off: very long vocabulary size and very short sequences or very short vocabulary size and very long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  65\n",
      "vocab:  \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab size: ', vocab_size)\n",
    "print('vocab: ', ''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [46, 43, 50, 50, 53]\n",
      "decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [stoi[ch] for ch in string]\n",
    "decode = lambda tokens: ''.join([itos[t] for t in tokens])\n",
    "\n",
    "print('encoded: ', encode('hello'))\n",
    "print('decoded: ', decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.1. Production-grade example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  50257\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print('vocab size: ', enc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [31373]\n",
      "decoded:  hello\n"
     ]
    }
   ],
   "source": [
    "print('encoded: ', enc.encode('hello'))\n",
    "print('decoded: ', enc.decode(enc.encode('hello')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Tokenize `tiny shakespear` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokenized text:  torch.Size([1115393])\n",
      "Dtype of tokenized text:  torch.int64\n",
      "First 10 characters:  First Citi\n",
      "First 10 tokens:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = torch.tensor(encode(text), dtype=torch.long)\n",
    "print('Shape of tokenized text: ', tokenized_text.shape)\n",
    "print('Dtype of tokenized text: ', tokenized_text.dtype)\n",
    "print('First 10 characters: ', text[:10])\n",
    "print('First 10 tokens: ', tokenized_text[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3. `Train` and `Validation` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(tokenized_text) * 0.85)\n",
    "train_dataset = tokenized_text[:n]\n",
    "val_dataset = tokenized_text[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data loader: batches of chunks of data\n",
    "\n",
    "**Note**: when a batch is created, each token dimension is an INFORMATION POINT in relation to the next token. Thus, each batch packs multiple examples in relation to the next token.\n",
    "\n",
    "Example:\n",
    "\n",
    "Block: [18, 47, 56, 57, 58,  1, 15, 47]\n",
    "Next token: 58\n",
    "\n",
    "1. Context: 18         -> 47 likely follows next\n",
    "2. Context: 18, 47     -> 56 likely follows next\n",
    "3. Context: 18, 47, 56 -> 57 likely follows next\n",
    "4. and so on...\n",
    "\n",
    "**TIME DIMENSION**: The idea behind training in this way is for the **transformer** to be able to predict the next token with as little as one token of context. Then, after `block_size`is reached, the inputs need to be truncated, because the **transformer** will never receive more than `block_size` tokens of context.\n",
    "\n",
    "**BATCH DIMENSION**: The idea behind batching is to train the model with multiple examples at the same time. This is done to speed up training and to make the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 9 tokens:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First block:  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Next token:  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "\n",
    "print('First 9 tokens: ', train_dataset[:context_length + 1])\n",
    "print('First block: ', train_dataset[:context_length])\n",
    "print('Next token: ', train_dataset[context_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  tensor([18])\n",
      "Target:  tensor(47)\n",
      "Context:  tensor([18, 47])\n",
      "Target:  tensor(56)\n",
      "Context:  tensor([18, 47, 56])\n",
      "Target:  tensor(57)\n",
      "Context:  tensor([18, 47, 56, 57])\n",
      "Target:  tensor(58)\n",
      "Context:  tensor([18, 47, 56, 57, 58])\n",
      "Target:  tensor(1)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1])\n",
      "Target:  tensor(15)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "Target:  tensor(47)\n",
      "Context:  tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "Target:  tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# time dimension\n",
    "x = train_dataset[:context_length]\n",
    "y = train_dataset[1:context_length+1]\n",
    "\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('Context: ', context)\n",
    "    print('Target: ', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  948084\n",
      "Sample of data:  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "random_observations:  tensor([561423,  15063, 784785,  87993])\n",
      "Input batch:  tensor([[53, 56, 42, 47, 39, 50,  1, 39],\n",
      "        [61,  1, 46, 53, 52, 53, 59, 56],\n",
      "        [61,  1, 47,  5,  1, 58, 46, 43],\n",
      "        [ 1, 58, 46, 53, 59,  1, 46, 39]])\n",
      "Target batch:  tensor([[56, 42, 47, 39, 50,  1, 39, 52],\n",
      "        [ 1, 46, 53, 52, 53, 59, 56,  1],\n",
      "        [ 1, 47,  5,  1, 58, 46, 43,  1],\n",
      "        [58, 46, 53, 59,  1, 46, 39, 57]])\n",
      "Batch:  0 Time:  0\n",
      "Context:  tensor([53])\n",
      "Target:  tensor(56)\n",
      "Batch:  0 Time:  1\n",
      "Context:  tensor([53, 56])\n",
      "Target:  tensor(42)\n",
      "Batch:  0 Time:  2\n",
      "Context:  tensor([53, 56, 42])\n",
      "Target:  tensor(47)\n",
      "Batch:  0 Time:  3\n",
      "Context:  tensor([53, 56, 42, 47])\n",
      "Target:  tensor(39)\n",
      "Batch:  0 Time:  4\n",
      "Context:  tensor([53, 56, 42, 47, 39])\n",
      "Target:  tensor(50)\n",
      "Batch:  0 Time:  5\n",
      "Context:  tensor([53, 56, 42, 47, 39, 50])\n",
      "Target:  tensor(1)\n",
      "Batch:  0 Time:  6\n",
      "Context:  tensor([53, 56, 42, 47, 39, 50,  1])\n",
      "Target:  tensor(39)\n",
      "Batch:  0 Time:  7\n",
      "Context:  tensor([53, 56, 42, 47, 39, 50,  1, 39])\n",
      "Target:  tensor(52)\n"
     ]
    }
   ],
   "source": [
    "# batch dimension\n",
    "def get_batch(split, batch_size, verbose=False):\n",
    "    data = train_dataset if split == 'train' else val_dataset\n",
    "    if verbose:\n",
    "        print('Shape of data: ', len(data))\n",
    "        print('Sample of data: ', data[:10])\n",
    "    \n",
    "    random_observations = torch.randint(0, len(data) - context_length, (batch_size,))\n",
    "    if verbose:\n",
    "        print(\"random_observations: \", random_observations)\n",
    "\n",
    "    input_batch = torch.stack([data[obs:obs+context_length] for obs in random_observations])\n",
    "    target_batch = torch.stack([data[obs+1:obs+context_length+1] for obs in random_observations])\n",
    "    \n",
    "    return input_batch, target_batch\n",
    "\n",
    "batch_size = 4\n",
    "input_batch, target_batch = get_batch('train', batch_size, True)\n",
    "print('Input batch: ', input_batch)\n",
    "print('Target batch: ', target_batch)\n",
    "\n",
    "for batch in range(batch_size):           # batch dimension\n",
    "    for time in range(context_length):    # time dimension\n",
    "        if batch == 0:\n",
    "            print('Batch: ', batch, 'Time: ', time)\n",
    "            context = input_batch[batch, :time+1]\n",
    "            print('Context: ', context)\n",
    "            target = target_batch[batch, time]\n",
    "            print('Target: ', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Simplest baseline: bigram language model\n",
    "\n",
    "**Note**: this implementation is ridiculous by design. As a simple *character-level* bigram model, the prediction throws away all context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        `self.embedding` is 65 x 65, because for each of the 65 tokens in the vocabulary,\n",
    "        we have a 65-dimensional vector that represents the probability of the next token\n",
    "        given the context.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        \"\"\"\n",
    "        Loss algorithm chosen was `cross_entropy`, which is equivalent to negative log likelihood loss.\n",
    "\n",
    "        `Inputs` is a tensor of shape (batch_size, context_length)\n",
    "        \"\"\"\n",
    "        logits = self.embedding(inputs)    # B, T, C (batch, time, channels)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, _, C = logits.shape\n",
    "            logits = logits.view(-1, C)  # Flatten to [B * T, C]\n",
    "            targets = targets.view(-1)   # Flatten to [B * T]\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, num_predictions):\n",
    "        \"\"\"\n",
    "        `Inputs` is a tensor of shape (batch_size, context_length)\n",
    "        \"\"\"\n",
    "        predictions = torch.zeros(inputs.shape[0] * num_predictions, dtype=torch.long)\n",
    "\n",
    "        for i in range(num_predictions):\n",
    "            logits, _ = self(inputs)\n",
    "            logits = logits[:, -1, :]  # Take the last time step (prediction) and all channels\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            target = torch.multinomial(probs, num_samples=1)\n",
    "            predictions[i] = target\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  65\n",
      "Input batch shape:  torch.Size([4, 8])\n",
      "Target batch shape:  torch.Size([4, 8])\n",
      "Logits shape:  torch.Size([32, 65])\n",
      "Baseline NLL loss:  tensor(4.1744)\n",
      "Current loss:  tensor(4.7626, grad_fn=<NllLossBackward0>)\n",
      "Inputs:  tensor([[53, 56, 42, 47, 39, 50,  1, 39]])\n",
      "Predicted targets:  tensor([64,  3, 58,  3, 39, 56, 39, 39, 20, 61, 64, 24, 46, 50, 32, 56, 39, 46,\n",
      "        24, 27, 48, 42,  1, 43, 46, 14, 25, 49, 45, 57, 55, 46,  1, 55,  6, 21,\n",
      "        57,  1, 44, 20,  7, 46,  1, 55, 43, 41,  3, 22, 32, 48])\n",
      "Predicted characters:  z$t$araaHwzLhlTrahLOjd ehBMkgsqh q,Is fH-h qec$JTj\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(input_batch, target_batch)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print('Input batch shape: ', input_batch.shape)\n",
    "print('Target batch shape: ', target_batch.shape)\n",
    "print('Logits shape: ', logits.shape)\n",
    "print('Baseline NLL loss: ', -torch.log(torch.tensor(1.0 / vocab_size)))\n",
    "print('Current loss: ', loss)\n",
    "\n",
    "NUM_PREDICTIONS = 50\n",
    "\n",
    "INPUTS = input_batch[:1]\n",
    "print('Inputs: ', INPUTS)\n",
    "\n",
    "predicted_targets = model.generate(INPUTS, NUM_PREDICTIONS)\n",
    "print('Predicted targets: ', predicted_targets)\n",
    "print('Predicted characters: ', decode(predicted_targets.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000 | Loss 3.132338523864746\n",
      "Epoch 4000 | Loss 2.700752019882202\n",
      "Epoch 6000 | Loss 2.510911703109741\n",
      "Epoch 8000 | Loss 2.4131875038146973\n",
      "Epoch 10000 | Loss 2.4606728553771973\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "    input_batch, target_batch = get_batch('train', batch_size)\n",
    "    logits, loss = model(input_batch, target_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 2000 == 0:\n",
    "        print(f'Epoch {epoch + 1} | Loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  tensor([[53, 56, 42, 47, 39, 50,  1, 39]])\n",
      "Predicted characters:  tvynpkrn nswmcrgrnlnrmtlf\n",
      "rvnin'vrtnnvtu nclnltncb\n"
     ]
    }
   ],
   "source": [
    "print('Inputs: ', INPUTS)\n",
    "\n",
    "predicted_targets = model.generate(INPUTS, NUM_PREDICTIONS)\n",
    "print('Predicted characters: ', decode(predicted_targets.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the `transformer`\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/transformer.jpg\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### 2.1. The \"self-attention\" mechanism\n",
    "\n",
    "##### 2.1.1. `Version 1` - Weakest form of aggregation: averaging past context\n",
    "\n",
    "Each token in a batch should communicate information with other tokens in the batch, in such a way that information only flows from past tokens to the current token.\n",
    "\n",
    "Consider the fifth token in a batch of eight tokens. It should not communicate with tokens in the sixth, seventh and eighth positions, because those are FUTURE tokens in a sequence, but it should communicate with the fourth, third, second and first tokens, because those are PAST tokens in a sequence. This way, information only flows from previous context to the current timestep.\n",
    "\n",
    "Given this, the easiest way for tokens to communicate is to simply average all previous embeddings. This is the weakest form of aggregation and is extremely lossy, because all information about spatial arrangement of tokens is lost. \n",
    "\n",
    "This implementation is also very low performance, because it requires computation to be linear in the number of tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of logits:  torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "logits = torch.randn(B, T, C)\n",
    "print(\"Shape of logits: \", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want bag of words be logits[b, t] = mean of logits[b, i] for i<=t\n",
    "logits_bow = torch.zeros((B, T, C))\n",
    "\n",
    "for batch in range(B):\n",
    "    for time in range(T):\n",
    "        logits_prev = logits[batch, :time+1]\n",
    "        logits_bow[batch, time] = torch.mean(logits_prev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits BOW shape:  torch.Size([4, 8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2241, -0.6516],\n",
       "         [-0.9912,  2.3514],\n",
       "         [ 0.1447, -0.4468],\n",
       "         [-1.3756, -0.4075],\n",
       "         [-1.7993, -0.1726],\n",
       "         [ 0.2994,  0.3386],\n",
       "         [-1.9777,  0.0924],\n",
       "         [-0.2113,  0.6239]]),\n",
       " tensor([[-0.2241, -0.6516],\n",
       "         [-0.6077,  0.8499],\n",
       "         [-0.3569,  0.4177],\n",
       "         [-0.6116,  0.2114],\n",
       "         [-0.8491,  0.1346],\n",
       "         [-0.6577,  0.1686],\n",
       "         [-0.8463,  0.1577],\n",
       "         [-0.7669,  0.2160]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each location of logits_bow is the vertical mean of all previous logits\n",
    "print('Logits BOW shape: ', logits_bow.shape)\n",
    "logits[0], logits_bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2. The `mathematical trick` in self-attention: Matrix multiplication with triangular mask\n",
    "\n",
    "Matrix multiplication is a very efficient way to calculate the dot product of each token with all other tokens. By masking the upper triangular part of the matrix, we can ensure that each token only communicates with previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "b =\n",
      "tensor([[6., 3.],\n",
      "        [2., 6.],\n",
      "        [1., 5.]])\n",
      "---\n",
      "c =\n",
      "tensor([[ 9., 14.],\n",
      "        [ 9., 14.],\n",
      "        [ 9., 14.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the whole trick\n",
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "b =\n",
      "tensor([[7., 0.],\n",
      "        [1., 9.],\n",
      "        [2., 0.]])\n",
      "---\n",
      "c =\n",
      "tensor([[7.0000, 0.0000],\n",
      "        [4.0000, 4.5000],\n",
      "        [3.3333, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)  # normalize\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a =')\n",
    "print(a)\n",
    "print('---')\n",
    "print('b =')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c =')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.3. `Version 2` - Averaging past context with matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / torch.sum(weights, dim=1, keepdim=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits BOW 2 shape:  torch.Size([4, 8, 2])\n",
      "Logits BOW == Logits BOW 2?  True\n"
     ]
    }
   ],
   "source": [
    "logits_bow_2 = weights @ logits\n",
    "print('Logits BOW 2 shape: ', logits_bow_2.shape)\n",
    "print('Logits BOW == Logits BOW 2? ', torch.allclose(logits_bow, logits_bow_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.4. `Version 3`: Adding softmax to self-attention\n",
    "\n",
    "The softmax function exponentiates each element and normalizes the results so that they sum up to **1** along the specified dimension. Since &minus;&infin; values correspond to zeroes when exponentiated, the softmax ensures that the attention is focused only on the elements allowed by the lower triangular mask.\n",
    "\n",
    "An important aspect to note here is that the weights are initialized as `zeroes`, giving room for future `affinities` between tokens to be data dependent. In the next section, they will start looking at each other and some tokens will find other tokens more or less interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros(T, T)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = F.softmax(weights, dim=1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits BOW 3 shape:  torch.Size([4, 8, 2])\n",
      "Logits BOW == Logits BOW 3?  True\n"
     ]
    }
   ],
   "source": [
    "logits_bow_3 = weights @ logits\n",
    "print('Logits BOW 3 shape: ', logits_bow_3.shape)\n",
    "print('Logits BOW == Logits BOW 3? ', torch.allclose(logits_bow, logits_bow_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.5. `Version 4`: THE CRUX OF IT ALL - Self-attention with `affinities`\n",
    "\n",
    "In previous versions, all past tokens are averaged in the context of the current token, resulting in uniform affinities. But self-attention is all about learning data-dependent affinities between tokens so that, for example, a *vowel* token might look for *consonants* in its past and might want to know what those consonants were and let this data to flow to the current token.\n",
    "\n",
    "In order to solve this data dependency problem, every single token in self-attention will emit two vectors:\n",
    "- **Query**: roughly speaking is \"What am I looking for?\"\n",
    "- **Key**: roughly speaking is \"What do I contain?\"\n",
    "\n",
    "These vectors will be produced in parallel and independently by two linear transformations of the token embeddings. After creation, they will communicate through a dot product, resulting in a scalar value that will be used as the **affinity** between tokens. If the `Key` and the `Query` are very similar, the dot product will be high and the value of the token will be weighted more.\n",
    "\n",
    "Finally, for the aggregations to be calculated, the **Value** vector is created by a third linear transformation of the token embeddings. This way, the logits become private information of the token and the aggregation is done by a weighted sum of the **Value** vector. Roughly speaking, \"If you find me interesting, this is what I will communicate to you.\"\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/scaled-dot-product-attention.jpg\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a single head self-attention mechanism\n",
    "head_size = 1\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "key, query, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication of key and query to get the weights\n",
    "# NOTE: previously, weights were initialized as `zeroes` and then masked\n",
    "k = key(logits)\n",
    "q = query(logits)\n",
    "v = value(logits)\n",
    "print(f'Shape of logits: {logits.shape}')\n",
    "print(f'Shape of k: {k.shape} | q: {q.shape} | v: {v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = q @ k.transpose(-2, -1)\n",
    "print(f'Shape of weights: {weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_bow_4 = weights @ v\n",
    "logits_bow_4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.6. Scaled dot-product attention\n",
    "\n",
    "As implemented in the **Attention is all your need** paper, `scaled attention` divides the weights by $\\frac{1}{\\sqrt{\\text{head\\_size}}}$. This makes it so when input Q, K are unit variance, the weights will be unit variance as well and softmax will stay diffuse and not saturate the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, head_size = 4, 8, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that variance of weights increases towards the head size\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)\n",
    "\n",
    "k.var(), q.var(), weights.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by applying the scaling factor, we can control the variance of the weights\n",
    "weights = q @ k.transpose(-2, -1) / head_size**0.5\n",
    "\n",
    "k.var(), q.var(), weights.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if weights are close together, the variance will be low\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the weights spread out, the variance increases towards one-hot vectors\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTES ABOUT SELF-ATTENTION:\n",
    "- Attention is a **communication mechanism**. It can be seen as tokens in a directed graph looking at each other and aggregating information with a weighted sum from all tokens that point to them, with data-dependent weights.\n",
    "- **There is no notion of space**. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across a batch dimension is processed completely independently and never \"talk\" to each other.\n",
    "- An attention mechanism that uses triangular masking is called a \"decoder\" and is usually used in autoregressive settings, eg. for language modeling.  In an \"encoder\" attention block, just delete the single line that does masking with `tril`, allowing all tokens to communicate, eg. for 'sentiment analysis'.\n",
    "- `self-attention` means that queries, keys and values are all produced from the same source. In `cross-attention`, the queries are produced from the logits, but the keys and values come from some other, external source, eg. an encoder module.\n",
    "\n",
    "### 2.2. Multi-head self-attention\n",
    "\n",
    "The idea behind multi-head attention is to have multiple heads and then concatenate their results over the channel dimension. In this scenario, each head represents one communication channel between tokens and each of these channels will be typically correspondingly smaller than the original embedding dimension. For example, consider a 512-dimensional embedding split into 8 heads of 64 dimensions each.\n",
    "\n",
    "Multiple independent channels of communication helps to improve the loss because the model can learn different types of relationships between tokens. For example, one head might learn to look for verbs, another for nouns, and so on.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/multi-head-attention.jpg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size, n_embeddings, context_length):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embeddings, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embeddings, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embeddings, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        _, T, _ = inputs.shape\n",
    "\n",
    "        k = self.key(inputs)\n",
    "        q = self.query(inputs)\n",
    "\n",
    "        # compute scaled attention scores, ie. affinities\n",
    "        weights = q @ k.transpose(-2, -1) / (self.head_size ** 0.5)\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(inputs)\n",
    "        logits = weights @ v\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return torch.cat([head(inputs) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Feedforward layers\n",
    "\n",
    "The feedforward layer is a simple linear transformation followed by a non-linearity applied independently to each token in the sequence. The non-linearity is usually a `GELU` or a `ReLU`.\n",
    "\n",
    "The idea here is to allow the model to *have time* to learn complex relationships between tokens. Bottomline, it is a simple way to allow the model to learn non-linear relationships between tokens.\n",
    "\n",
    "For more information, see [Attention is all you need](https://arxiv.org/pdf/1706.03762), section 3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Residual connections (Kaiming He *et al.*)\n",
    "\n",
    "Also known as `skip connections`, they help to solve the problem of degradation of deeper neural networks, caused by the curse of dimensionality, among other factors. Assuming the computation happens top to bottom, the residual pathway $\\mathbf{x} \\, \\textbf{identity}$ is added to the output of the computation $\\mathcal{F}(x)$.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"../assets/residual-connections.jpg\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "The reason this is useful is that during backpropagation, addition distributes gradients equally to both pathways. So, the gradients from the loss *hop* through every addition node, all the way back to the input, while also forking to the weights of the computation. The weights of the computation are usually initialized so they contribute very little to the output and they *come online* over time and start contributing to the output.\n",
    "\n",
    "A simple implementation of residual connections would look like this:\n",
    "\n",
    "```python\n",
    "def forward(self, inputs):\n",
    "    inputs = inputs + self.layer_1(inputs)\n",
    "    inputs = inputs + self.layer_2(inputs)\n",
    "    return inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Layer normalization\n",
    "\n",
    "It is implemented out-of-the-box in Pytorch and basically is the concept of `batch normalization`, but applied horizontally for each layer, instead of vertically for each batch inputs.\n",
    "\n",
    "Below is an implementation of normalization from `makemore-3-activations.ipynb`, adapted to perform on batches as well as on the layer dimension. Refer to that notebook for more information.\n",
    "\n",
    "**`IMPORTANT NOTE`**: In the original paper, normalization is applied after the transformation. Now, it is more commom to see it applied before the transformation, also called `pre-norm` formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "  def __init__(self, dim, norm_type='batch', eps=1e-5):\n",
    "    self.axis = 0 if norm_type == 'batch' else 1\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones((1, dim))\n",
    "    self.beta = torch.zeros((1, dim))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    xmean = x.mean(self.axis, keepdim=True)                  \n",
    "    xvar = x.var(self.axis, keepdim=True)                    \n",
    "    \n",
    "    x_hat = (x - xmean) / torch.sqrt(xvar + self.eps)  \n",
    "    self.out = self.gamma * x_hat + self.beta          \n",
    "\n",
    "    return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 100)\n",
    "print('Mean of batch dimensions (columns): ', x[:, 0].mean())\n",
    "print('Mean of layer features (rows): ', x[0, :].mean())\n",
    "print('Standard deviation of batch dimensions (columns): ', x[:, 0].std())\n",
    "print('Standard deviation of layer features (rows): ', x[0, :].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization\n",
    "batch_norm = Normalization(100, norm_type='batch')\n",
    "\n",
    "y = batch_norm(x)\n",
    "print('Mean of batch dimensions (columns): ', y[:, 0].mean())\n",
    "print('Mean of layer features (rows): ', y[0, :].mean())\n",
    "print('Standard deviation of batch dimensions (columns): ', y[:, 0].std())\n",
    "print('Standard deviation of layer features (rows): ', y[0, :].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization\n",
    "layer_norm = Normalization(100, norm_type='layer')\n",
    "\n",
    "y = layer_norm(x)\n",
    "print('Mean of batch dimensions (columns): ', y[:, 0].mean())\n",
    "print('Mean of layer features (rows): ', y[0, :].mean())\n",
    "print('Standard deviation of batch dimensions (columns): ', y[:, 0].std())\n",
    "print('Standard deviation of layer features (rows): ', y[0, :].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Let's build GPT: from scratch, in code, spelled out, By Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4496s)\n",
    "2. [Attention is all you need](https://arxiv.org/pdf/1706.03762)\n",
    "3. [Residual blocks — Building blocks of ResNet](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec)\n",
    "4. [Deep Residual Learning for Image Recognition, by Kaiming He et al.](https://arxiv.org/pdf/1512.03385)\n",
    "5. [Dropout: A simple way to prevent neural networks from overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "6. [Language Models are Few Short Learners (aka. GPT-paper)](https://arxiv.org/pdf/2005.14165)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
