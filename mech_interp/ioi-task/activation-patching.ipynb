{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320a47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c10e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import circuitsvis as cv\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML, display\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils\n",
    "\n",
    "from plotly_utils import line, imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "print('Changed working directory to parent directory')\n",
    "\n",
    "with open(os.path.expanduser('~/.huggingface/token')) as f:\n",
    "    os.environ['HF_TOKEN'] = f.read().strip()\n",
    "    print(f'Hugging Face token loaded: {os.environ['HF_TOKEN'][:3]}...')\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc380d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    'When John and Mary went to the shops, {} gave the bag to',\n",
    "    'When Tom and James went to the park, {} gave the ball to',\n",
    "    'When Dan and Sid went to the shops, {} gave an apple to',\n",
    "    'After Martin and Amy went to the park, {} gave a drink to',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pairs = [\n",
    "    (' Mary', ' John'),\n",
    "    (' Tom', ' James'),\n",
    "    (' Dan', ' Sid'),\n",
    "    (' Martin', ' Amy'),\n",
    "]\n",
    "\n",
    "prompts = [prompt.format(name) for (prompt, names) in zip(prompt_format, name_pairs) for name in names[::-1]]\n",
    "answers = [names[::i] for names in name_pairs for i in (1, -1)]\n",
    "answer_tokens = torch.concat([model.to_tokens(names, prepend_bos=False).T for names in answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits and cache of all internal activations for later analysis\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "tokens = tokens.to(device)\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens = answer_tokens,\n",
    "    per_prompt = False,\n",
    "):\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)\n",
    "    answer_logit_diff = correct_logits - incorrect_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "\n",
    "ave_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0f020",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The setup of activation patching is to take two runs of the model on two different inputs, the clean run and the corrupted run. The clean run outputs the correct answer and the corrupted run does not. The key idea is that we give the model the corrupted input, but then intervene on a specific activation and patch in the corresponding activation from the clean run (i.e., replace the corrupted activation with the clean activation), and then continue the run. And we then measure how much the output has updated towards the correct answer.\n",
    "\n",
    "We can then iterate over many possible activations and look at how much they affect the corrupted run. If patching in an activation significantly increases the probability of the correct answer, this allows us to localise which activations matter.\n",
    "\n",
    "The ability to localise is a key move in mechanistic interpretability - if the computation is diffuse and spread across the entire model, it is likely much harder to form a clean mechanistic story for what's going on. But if we can identify precisely which parts of the model matter, we can then zoom in and determine what they represent and how they connect up with each other, and ultimately reverse engineer the underlying circuit that they represent.\n",
    "\n",
    "The diagrams below demonstrate activation patching on an abstract neural network (the nodes represent activations, and the arrows between them are weight connections).\n",
    "\n",
    "A regular forward pass on the clean input looks like:\n",
    "\n",
    "<img src=\"../../assets/act-patch-regular.jpg\" width=\"300\"/>\n",
    "\n",
    "And activation patching from a corrupted input (green) into a forward pass for the clean input (black) looks like:\n",
    "\n",
    "<img src=\"../../assets/act-patch-corrupted.jpg\" width=\"400\"/>\n",
    "\n",
    "where the dotted line represents patching in a value (i.e. during the forward pass on the clean input, we replace node $D$ with the value it takes on the corrupted input). Nodes $H$, $G$ and $F$ are colored orange, to represent that they now follow a distribution which is not the same as clean or corrupted.\n",
    "\n",
    "We can patch into a transformer in many different ways, e.g. values of the residual stream, the MLP, or attention heads' output. We can also get even more granular by patching at particular sequence positions.\n",
    "\n",
    "<img src=\"../../assets/act-patch-examples.jpg\" width=\"800\"/>\n",
    "\n",
    "### 1.1. Noising vs. denoising\n",
    "\n",
    "We might call this algorithm a type of noising, since we're running the model on a clean input and adding noise by patching in from the corrupted input. We can also consider the opposite algorithm, denoising, where we run the model on a corrupted input and remove noise by patching in from the clean input.\n",
    "\n",
    "When would you use noising vs denoising? It depends on your goals. The results of denoising are much stronger, because showing that a component or set of components is sufficient for a task is a big deal. On the other hand, the complexity of transformers and interdependence of components means that noising a model can have unpredictable consequences. If loss goes up when we ablate a component, it doesn't necessarily mean that this component was necessary for the task. As an example, ablating `MLP0` in **gpt2-small** seems to make performance much worse on basically any task, but only because it acts as a kind of extended embedding. In fact, it's not doing anything important which is specfic for the IOI task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16c642",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Arena::Activation Patching](https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification#keeping-track-of-your-guesses-predictions)\n",
    "2. [Locating and Editing Factual Associations in GPT, by Meng, K, et. al.](https://arxiv.org/pdf/2202.05262)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
