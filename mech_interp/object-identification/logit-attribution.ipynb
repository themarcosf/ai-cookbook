{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320a47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c10e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77c681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to parent directory\n",
      "Hugging Face token loaded: hf_...\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "print('Changed working directory to parent directory')\n",
    "\n",
    "with open(os.path.expanduser('~/.huggingface/token')) as f:\n",
    "    os.environ['HF_TOKEN'] = f.read().strip()\n",
    "    print(f'Hugging Face token loaded: {os.environ['HF_TOKEN'][:3]}...')\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ce4487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    'gpt2-small',\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc380d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    'When John and Mary went to the shops, {} gave the bag to',\n",
    "    'When Tom and James went to the park, {} gave the ball to',\n",
    "    'When Dan and Sid went to the shops, {} gave an apple to',\n",
    "    'After Martin and Amy went to the park, {} gave a drink to',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767fdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pairs = [\n",
    "    (' Mary', ' John'),\n",
    "    (' Tom', ' James'),\n",
    "    (' Dan', ' Sid'),\n",
    "    (' Martin', ' Amy'),\n",
    "]\n",
    "\n",
    "prompts = [prompt.format(name) for (prompt, names) in zip(prompt_format, name_pairs) for name in names[::-1]]\n",
    "answers = [names[::i] for names in name_pairs for i in (1, -1)]\n",
    "answer_tokens = torch.concat([model.to_tokens(names, prepend_bos=False).T for names in answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724bdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logits and cache of all internal activations for later analysis\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "tokens = tokens.to(device)\n",
    "logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67e6d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(\n",
    "    logits,\n",
    "    answer_tokens = answer_tokens,\n",
    "    per_prompt = False,\n",
    "):\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)\n",
    "    answer_logit_diff = correct_logits - incorrect_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "\n",
    "ave_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0f020",
   "metadata": {},
   "source": [
    "# 1. Direct Logit Attribution\n",
    "\n",
    "The easiest part of the model to understand is the output - this is what the model is trained to optimize, and so it can always be directly interpreted. Often the right approach to reverse engineering a circuit is to start at the end, understand how the model produces the right answer, and to then work backwards. The main technique used to do this is called `direct logit` attribution.\n",
    "\n",
    "### 1.1. Background and motivation of the logit difference\n",
    "\n",
    "The central object of a transformer is the residual stream. This is the sum of the outputs of each layer and of the original token and positional embedding. Importantly, this means that any linear function of the residual stream can be perfectly decomposed into the contribution of each layer of the transformer. Further, each attention layer's output can be broken down into the sum of the output of each head (See `A Mathematical Framework for Transformer Circuits` for details), and each MLP layer's output can be broken down into the sum of the output of each neuron (and a bias term for each layer).\n",
    "\n",
    "In general, there are two natural ways to interpret the model's outputs: the output logits or the output log probabilities. Let $\\vec{x}$ be the logits, $\\vec{L}$ be the log probabilities, and $\\vec{p}$ be the probabilities. Then we have the following relations:\n",
    "\n",
    "$$\n",
    "p_i = \\mathrm{softmax}(\\vec{x})_i = \\frac{e^{x_i}}{\\sum_{i=1}^{n} e^{x_i}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "L_i = \\log(p_i)\n",
    "$$\n",
    "\n",
    "Combining these, we get:\n",
    "\n",
    "$$\n",
    "L_i = \\log \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} = x_i - \\log \\sum_{j=1}^{n} e^{x_j}\n",
    "$$\n",
    "\n",
    "The sum term on the right is the same for all $i$, so we get:\n",
    "\n",
    "$$\n",
    "L_i - L_j = x_i - x_j\n",
    "$$\n",
    "\n",
    "In other words, the logit difference $x_i - x_j$ is the same as the log probability difference $L_i - L_j$, motivating the use of logit differences to understand the model's outputs.\n",
    "\n",
    "### 1.2. Logit diff directions\n",
    "\n",
    "**Getting an output logit is equivalent to projecting onto a direction in the residual stream, and the same is true for getting the logit diff.**\n",
    "\n",
    "Suppose the final value in the residual stream for a single sequence and a position within that sequence is $x$ (i.e., $x$ is a vector of length $d_{\\text{model}}$). Then, we get logits by multiplying by the unembedding matrix $W_U$ (which has shape($d_{\\text{model}}$, $d_{\\text{vocab}}$)):\n",
    "\n",
    "$$\n",
    "\\text{output} = x^T W_U\n",
    "$$\n",
    "\n",
    "Now, the logit difference between two tokens $i$ and $j$ is given by:\n",
    "\n",
    "$$\n",
    "\\text{logit diff}_{ij} = x^T W_U[:, i] - x^T W_U[:, j] = x^T (W_U[:, i] - W_U[:, j])\n",
    "$$\n",
    "\n",
    "This means that the logit difference is given by the projection of the residual stream onto the vector $W_U[:, i] - W_U[:, j]$. This vector is called the **logit diff direction**, because it points in the direction of the largest logit difference between the two tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbd37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5335,  1757],\n",
       "        [ 1757,  5335],\n",
       "        [ 4186,  3700],\n",
       "        [ 3700,  4186],\n",
       "        [ 6035, 15686],\n",
       "        [15686,  6035],\n",
       "        [ 5780, 14235],\n",
       "        [14235,  5780]], device='mps:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5208bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit difference directions shape: torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "# map answer_tokens to logit diff direction\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "\n",
    "correct_residual_directions, incorrect_residual_directions = answer_residual_directions.unbind(dim=1)\n",
    "logit_diff_directions = correct_residual_directions - incorrect_residual_directions\n",
    "print('Logit difference directions shape:', logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910871a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final residual stream shape: torch.Size([8, 16, 768])\n",
      "Calculated average logit diff: 2.7098135948\n",
      "Original logit difference:     2.7098159790\n"
     ]
    }
   ],
   "source": [
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "\n",
    "# Apply LayerNorm scaling (to just the final sequence position)\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer=-1, pos_slice=-1)\n",
    "\n",
    "average_logit_diff = einops.einsum(\n",
    "    scaled_final_token_residual_stream, logit_diff_directions, \"batch d_model, batch d_model ->\"\n",
    ") / len(prompts)\n",
    "\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {ave_logit_diff:.10f}\")\n",
    "\n",
    "torch.testing.assert_close(average_logit_diff, ave_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c620b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75793726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a49f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f43358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f16c642",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. [Ground truth - Arena::Logit attribution](https://arena-chapter1-transformer-interp.streamlit.app/[1.4.1]_Indirect_Object_Identification#keeping-track-of-your-guesses-predictions)\n",
    "2. [A mathematical framework for transformer circuits, by Chris Olah, Neel Nanda, et. al.](https://transformer-circuits.pub/2021/framework/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
