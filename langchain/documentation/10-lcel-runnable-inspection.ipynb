{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["' Harrison worked at Kensho, but this information is unrelated to the context about bears and honey.'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_community.vectorstores import FAISS\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n","from langchain_community.embeddings import OllamaEmbeddings\n","from langchain_community.chat_models import ChatOllama\n","from langchain_core.output_parsers import StrOutputParser\n","\n","chat_model = ChatOllama(model=\"mistral\")\n","\n","vectorstore = FAISS.from_texts(\n","    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n","    embedding=OllamaEmbeddings(model=\"mistral\"),\n",")\n","\n","retriever = vectorstore.as_retriever()\n","\n","prompt = ChatPromptTemplate.from_messages([\n","  (\"system\", \"Answer the question based only on the following context: {context}\"),\n","  (\"human\", \"Question: {question}\")\n","])\n","\n","setup_and_retrieval = RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n","\n","chain = setup_and_retrieval | prompt | chat_model | StrOutputParser()\n","\n","chain.invoke(\"where did harrison work?\")"]},{"cell_type":"markdown","metadata":{},"source":["# Get a `graph`"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["           +---------------------------------+         \n","           | Parallel<context,question>Input |         \n","           +---------------------------------+         \n","                    **               **                \n","                 ***                   ***             \n","               **                         **           \n","+----------------------+              +-------------+  \n","| VectorStoreRetriever |              | Passthrough |  \n","+----------------------+              +-------------+  \n","                    **               **                \n","                      ***         ***                  \n","                         **     **                     \n","           +----------------------------------+        \n","           | Parallel<context,question>Output |        \n","           +----------------------------------+        \n","                             *                         \n","                             *                         \n","                             *                         \n","                  +--------------------+               \n","                  | ChatPromptTemplate |               \n","                  +--------------------+               \n","                             *                         \n","                             *                         \n","                             *                         \n","                      +------------+                   \n","                      | ChatOllama |                   \n","                      +------------+                   \n","                             *                         \n","                             *                         \n","                             *                         \n","                   +-----------------+                 \n","                   | StrOutputParser |                 \n","                   +-----------------+                 \n","                             *                         \n","                             *                         \n","                             *                         \n","                +-----------------------+              \n","                | StrOutputParserOutput |              \n","                +-----------------------+              \n"]}],"source":["chain.get_graph().print_ascii()"]},{"cell_type":"markdown","metadata":{},"source":["# `Get the prompts`"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["[ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Answer the question based only on the following context: {context}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Question: {question}'))])]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["chain.get_prompts()"]}],"metadata":{"kernelspec":{"display_name":"ai-cookbook-DNsoNefS","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":2}
